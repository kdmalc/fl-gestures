{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ef1e63-343c-4d55-ad97-cf3cabb3fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347da32c-36cc-49e0-80cd-e5b456561a30",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533b39ce-387c-4e40-b5bd-58dd97d233f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pID 101 because it doesn't exist\n",
    "# remove pID 131 because it  doesnt have enough user defined gestures\n",
    "# each participant has 100 experimenter defined files and 50 user defined files\n",
    "# 10 experimenter defined gestures and 5 user defined gestures\n",
    "\n",
    "file_types = [\"IMU_extract\", \"movavg_files\"]\n",
    "expt_types = [\"experimenter-defined\"]\n",
    "\n",
    "#remove participant 131 because they are missing gestures \n",
    "pIDs_impaired = ['P102','P103','P104','P105','P106','P107','P108','P109','P110','P111',\n",
    "       'P112','P114','P115','P116','P118','P119','P121','P122','P123','P124','P125',\n",
    "       'P126','P127','P128', 'P132']\n",
    "# remove participants P001 and P003 because they dont have duplicate or open gestures\n",
    "pIDs_unimpaired = ['P004','P005','P006','P008','P010','P011']\n",
    "\n",
    "pIDs_both = pIDs_impaired + pIDs_unimpaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647a91a5-4d27-485f-8ed0-db92218ccb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "# Kai's laptop\n",
    "data_path = \"C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\PCA_40D\\\\\"\n",
    "# BRC Desktop\n",
    "#data_path = \"D:\\\\Kai_MetaGestureClustering_24\\\\saved_datasets\\\\\"\n",
    "\n",
    "print(\"Loading\")\n",
    "\n",
    "metadata_cols = ['Participant', 'Gesture_ID', 'Gesture_Num']\n",
    "\n",
    "PCA_df = pd.read_pickle(data_path+'PCA_ms_IMUEMG_df.pkl')\n",
    "metadata_cols_df = pd.read_pickle('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\metadata_cols_df.pkl')\n",
    "\n",
    "# Dropping the metadata when we read it in!\n",
    "training_u_df = pd.read_pickle(data_path+'training_u_df.pkl').drop(metadata_cols, axis=1)\n",
    "test_users_df = pd.read_pickle(data_path+'test_users_df.pkl').drop(metadata_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0430ab-dd10-48ee-b823-b5a91d36b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327168, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027903</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>-0.019699</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>-0.031254</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>0.066484</td>\n",
       "      <td>0.108729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019453</td>\n",
       "      <td>0.062983</td>\n",
       "      <td>-0.025869</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.037645</td>\n",
       "      <td>-0.186270</td>\n",
       "      <td>-0.046251</td>\n",
       "      <td>-0.104630</td>\n",
       "      <td>-0.002939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038982</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>-0.007901</td>\n",
       "      <td>-0.027368</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>0.035053</td>\n",
       "      <td>-0.056843</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>-0.022542</td>\n",
       "      <td>-0.022563</td>\n",
       "      <td>-0.160826</td>\n",
       "      <td>-0.048161</td>\n",
       "      <td>-0.073771</td>\n",
       "      <td>0.043268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116782</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>-0.014612</td>\n",
       "      <td>-0.093325</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>-0.013155</td>\n",
       "      <td>-0.046150</td>\n",
       "      <td>0.036385</td>\n",
       "      <td>0.052746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014298</td>\n",
       "      <td>0.072109</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.018695</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>-0.041831</td>\n",
       "      <td>-0.109653</td>\n",
       "      <td>0.027043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.030245</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>-0.048905</td>\n",
       "      <td>-0.029129</td>\n",
       "      <td>0.090026</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>-0.064307</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>0.053055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010992</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>-0.097073</td>\n",
       "      <td>-0.056870</td>\n",
       "      <td>-0.001038</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>-0.165858</td>\n",
       "      <td>-0.049424</td>\n",
       "      <td>-0.108671</td>\n",
       "      <td>0.069886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.112950</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>-0.063254</td>\n",
       "      <td>-0.108892</td>\n",
       "      <td>0.198729</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>-0.124893</td>\n",
       "      <td>0.114817</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>-0.093678</td>\n",
       "      <td>-0.131263</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.157963</td>\n",
       "      <td>-0.041911</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>0.063311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \n",
       "0 -0.027903  0.001411 -0.019509  0.013428 -0.019699  0.027333 -0.031254  \\\n",
       "1 -0.038982  0.006470 -0.000111  0.010904 -0.015323  0.031336 -0.007901   \n",
       "2 -0.116782  0.003824  0.011550 -0.014612 -0.093325  0.081718 -0.013155   \n",
       "3 -0.030245 -0.017409  0.022540 -0.048905 -0.029129  0.090026 -0.024645   \n",
       "4 -0.112950  0.026262  0.004837 -0.063254 -0.108892  0.198729 -0.010583   \n",
       "\n",
       "         7         8         9   ...        30        31        32        33   \n",
       "0 -0.022910  0.066484  0.108729  ... -0.019453  0.062983 -0.025869  0.014303  \\\n",
       "1 -0.027368  0.060370  0.074712  ...  0.041438  0.035053 -0.056843 -0.008895   \n",
       "2 -0.046150  0.036385  0.052746  ... -0.014298  0.072109 -0.026536 -0.034365   \n",
       "3 -0.064307  0.074589  0.053055  ... -0.010992  0.059990 -0.097073 -0.056870   \n",
       "4 -0.124893  0.114817  0.038628  ...  0.035735  0.050880 -0.093678 -0.131263   \n",
       "\n",
       "         34        35        36        37        38        39  \n",
       "0 -0.013387 -0.037645 -0.186270 -0.046251 -0.104630 -0.002939  \n",
       "1 -0.022542 -0.022563 -0.160826 -0.048161 -0.073771  0.043268  \n",
       "2  0.018695 -0.011940 -0.160580 -0.041831 -0.109653  0.027043  \n",
       "3 -0.001038 -0.008015 -0.165858 -0.049424 -0.108671  0.069886  \n",
       "4  0.018035  0.056185 -0.157963 -0.041911 -0.145308  0.063311  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_u_df.shape)\n",
    "training_u_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a360eff5-02ba-4e94-aaa0-8b8ff6a195f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    # NOTE: I think this formulation makes it so the dataloader won't return (X, Y) as per usual (eg like TensorDataset)\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc04167-5ab8-4ac3-ae19-80b8e1988242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE TRAINING SET\n",
    "num_rows_per_gesture = 64 # From the interp\n",
    "num_gestures = len(training_u_df) // num_rows_per_gesture\n",
    "num_features = training_u_df.shape[1]\n",
    "\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(training_u_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns)\n",
    "X_3D_PCA40 = training_u_df.to_numpy().reshape(num_gestures, num_rows_per_gesture, num_features)\n",
    "#flattened_PCA = PCA_np.reshape(num_gestures, -1)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_3DTensor_PCA40 = torch.tensor(X_3D_PCA40, dtype=torch.float32)\n",
    "\n",
    "# Dummy dataset\n",
    "#data = torch.randn(num_gestures, timesteps, num_features)\n",
    "#dataset = TensorDataset(data)\n",
    "#data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the dataset\n",
    "u_training_dataset = GestureDataset(X_3DTensor_PCA40)\n",
    "\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "train_loader = DataLoader(u_training_dataset, batch_size=batch_size, shuffle=True) # Should shuffle be False? \n",
    "## It's shuffling the gesture order I think so that should be fine...\n",
    "\n",
    "# CREATE THE TEST SET\n",
    "num_test_gestures = len(test_users_df) // num_rows_per_gesture\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(test_users_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns) and convert to torch tensor\n",
    "## Theres probably an easier way to just create it as a torch tensor lol\n",
    "Xtest_3DTensor_PCA40 = torch.tensor(test_users_df.to_numpy().reshape(num_test_gestures, num_rows_per_gesture, num_features), dtype=torch.float32)\n",
    "\n",
    "# Create the dataset\n",
    "u_testing_dataset = GestureDataset(Xtest_3DTensor_PCA40)\n",
    "test_loader = DataLoader(u_testing_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b852-4054-443f-a4e9-bdb280c5cc84",
   "metadata": {},
   "source": [
    "# RNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b312cb-ba68-42d2-baee-5e0da666fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        return hidden[-1]\n",
    "\n",
    "\n",
    "class RNNDecoder_Original(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(hidden_dim, output_dim, num_layers, batch_first=True)\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNNAutoencoder_Original(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.encoder = RNNEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = RNNDecoder(hidden_dim, input_dim, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded, seq_len)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9457e642-d94d-43a7-9771-5d578dd18bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(hidden_dim, output_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "class RNNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, seq_len):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.encoder = RNNEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = RNNDecoder(hidden_dim, input_dim, num_layers)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent, self.seq_len)\n",
    "        return reconstructed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40c4759-9e9b-43ca-8486-8262c4592a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and dataset setup\n",
    "timesteps = 64\n",
    "num_features = 40\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = RNNAutoencoder(num_features, hidden_dim, num_layers, seq_len=timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b83f2b9-7412-4213-99ec-85971a591a6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fa199-a3f4-4b72-9484-3481f9bf79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in train_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #batch = batch[0]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_epoch_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcb47c-ed50-4031-8d73-87c2a9cb4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so I don't have to retrain later\n",
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Repos\\\\fl-gestures\\\\models\\\\RNNAE_Default_BothPCA40.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24d972-5cfb-4556-8e4a-fcc5e12f72f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47013c5c-d9cf-4ce4-ba5a-a9a21d0030b9",
   "metadata": {},
   "source": [
    "Loading in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4356426-2cfe-4e79-a06e-043d9072ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Repos\\\\fl-gestures\\\\models\\\\RNNAE_Default_BothPCA40.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be893f7-baf6-48ca-b3ea-b936a8dfd6c1",
   "metadata": {},
   "source": [
    "> Need to know judge whether or not the AE is any good. The loss by itself doesn't tell if the AE is \"good enough\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146c694-1227-45fa-8e28-6e33ac1d7f9d",
   "metadata": {},
   "source": [
    "Visual Inspection of Reconstructed VS Original Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f8aeb-e326-451e-a51d-d3cd06fae631",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_3DTensor_PCA40.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585945ac-a5fd-4438-8a51-3847af56d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_data = Xtest_3DTensor_PCA40[:32, :, :]\n",
    "    reconstructions = model(sample_data)\n",
    "\n",
    "print(reconstructions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17fd7e-1958-4b33-96c6-3f90cdd7d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the gestures and channels to visualize\n",
    "selected_gestures = [1, 10, 30]\n",
    "selected_channels = [1, 10, 30]\n",
    "\n",
    "fig, axes = plt.subplots(len(selected_gestures), len(selected_channels), figsize=(20, 15))\n",
    "\n",
    "for i, gesture in enumerate(selected_gestures):\n",
    "    for j, channel in enumerate(selected_channels):\n",
    "        original = sample_data[gesture, :, channel]\n",
    "        reconstructed = reconstructions[gesture, :, channel]\n",
    "        \n",
    "        axes[i, j].plot(original, label='Original', color='blue')\n",
    "        axes[i, j].plot(reconstructed, label='Reconstructed', linestyle='dashed', color='orange')\n",
    "        axes[i, j].fill_between(range(len(original)), original, reconstructed, color='red', alpha=0.3)\n",
    "        \n",
    "        axes[i, j].set_title(f'Gesture {gesture}, Channel {channel}')\n",
    "        if i == 0 and j == 0:\n",
    "            axes[i, j].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f6a59-19f0-4261-a3fd-b844369f1cf9",
   "metadata": {},
   "source": [
    "Oof this looks pretty bad lol. The red is all error... in some cases the orange line is just completely flat... literally captures no information...\n",
    "> Perhaps Gesture 30 is just particuarly bad, since all its channels are bad, whereas the first two gestures seem to capture the data much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f8610-7cf3-4a22-b52a-7c9c1e501557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e2503-e822-47a9-acef-746c679bb9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad81688-4c15-4ddd-aa61-80ff5d935d91",
   "metadata": {},
   "source": [
    "Latent Space Visualization\n",
    "- This code hasn't been refactored to work with our example yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4d5d6-5b60-47c8-bff3-b1ac2a4944b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get latent representations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    latent_representations = []\n",
    "    labels = []\n",
    "    for batch in test_loader:\n",
    "        batch = batch[0]\n",
    "        latent = model.encoder(batch)\n",
    "        latent_representations.append(latent.cpu().numpy())\n",
    "        # Assuming labels are available and appended similarly\n",
    "        labels.append(batch_labels.cpu().numpy())\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_latent = tsne.fit_transform(latent_representations)\n",
    "\n",
    "# Plot the reduced latent space\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(reduced_latent[:, 0], reduced_latent[:, 1], c=labels, cmap='viridis', s=2)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201a0e0-42c9-45a7-adb5-edeed59ae843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8eeef3-cdf0-4ab5-9c97-1a1eccff920f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c255bdc-ea0e-4bd5-9bd2-ac10f544fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "330b20a2-eb0a-492c-883f-e2603f295521",
   "metadata": {},
   "source": [
    "# Temporal Convolution Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85022701-bada-40a8-99d2-21938d17978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is different but not sure what the effect is...\n",
    "## Observed a higher starting loss, only sort of converged, loss only somewhat decreased\n",
    "#class TCNEncoder(nn.Module):\n",
    "#    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "#        super(TCNEncoder, self).__init__()\n",
    "#        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2, stride=2)  # Adjusted stride\n",
    "#        \n",
    "#    def forward(self, x):\n",
    "#        x = x.permute(0, 2, 1)\n",
    "#        x = torch.relu(self.conv1(x))\n",
    "#        return x\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, kernel_size):\n",
    "        super(TCNDecoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size, stride=2, padding=kernel_size//2, output_padding=1)  # Adjusted to maintain dimensions\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, output_dim, kernel_size=1)  # Convolution to adjust channels if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))  # Apply activation after the final convolution\n",
    "        x = x.permute(0, 2, 1)  # Permute dimensions to match the desired output size\n",
    "        return x\n",
    "\n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNAutoencoder, self).__init__()\n",
    "        self.encoder = TCNEncoder(input_dim, hidden_dim, kernel_size)\n",
    "        self.decoder = TCNDecoder(hidden_dim, input_dim, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09094d09-9f33-4f45-b28e-31795d511dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1183\n",
      "Epoch [2/10], Loss: 0.1957\n",
      "Epoch [3/10], Loss: 0.1652\n",
      "Epoch [4/10], Loss: 0.1677\n",
      "Epoch [5/10], Loss: 0.1668\n",
      "Epoch [6/10], Loss: 0.2249\n",
      "Epoch [7/10], Loss: 0.1822\n",
      "Epoch [8/10], Loss: 0.3137\n",
      "Epoch [9/10], Loss: 0.1739\n",
      "Epoch [10/10], Loss: 0.1876\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 40\n",
    "hidden_dim = 64\n",
    "kernel_size = 5\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "temp_conv_ae_model = TCNAutoencoder(input_dim, hidden_dim, kernel_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = temp_conv_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbb1d9-29ff-4661-b669-9ff300336246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc824e7-03e0-4eae-b880-6be6118a56fb",
   "metadata": {},
   "source": [
    "# Varitational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b356ebb-4b72-46d1-a1a0-8fd21c163c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, use_xavier_init=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim*2)  # mean and logvar\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        if use_xavier_init:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5138cb2d-7d58-4f83-8dcd-0074f0544c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Epoch [1/50], Loss: 42963.5078\n",
      "Epoch [2/50], Loss: 43016.8164\n",
      "Epoch [3/50], Loss: 43023.6992\n",
      "Epoch [4/50], Loss: 43148.0000\n",
      "Epoch [5/50], Loss: 42962.1797\n",
      "Epoch [6/50], Loss: 43021.8086\n",
      "Epoch [7/50], Loss: 42986.8750\n",
      "Epoch [8/50], Loss: 42930.5195\n",
      "Epoch [9/50], Loss: 43032.1250\n",
      "Epoch [10/50], Loss: 43092.1992\n",
      "Epoch [11/50], Loss: 43025.1055\n",
      "Epoch [12/50], Loss: 43046.6562\n",
      "Epoch [13/50], Loss: 43113.6094\n",
      "Epoch [14/50], Loss: 42920.6094\n",
      "Epoch [15/50], Loss: 42878.5156\n",
      "Epoch [16/50], Loss: 42949.3633\n",
      "Epoch [17/50], Loss: 42897.9844\n",
      "Epoch [18/50], Loss: 43115.6992\n",
      "Epoch [19/50], Loss: 43130.8125\n",
      "Epoch [20/50], Loss: 42959.1250\n",
      "Epoch [21/50], Loss: 42949.3047\n",
      "Epoch [22/50], Loss: 42865.2734\n",
      "Epoch [23/50], Loss: 42945.5234\n",
      "Epoch [24/50], Loss: 42970.3867\n",
      "Epoch [25/50], Loss: 43075.9258\n",
      "Epoch [26/50], Loss: 43067.1875\n",
      "Epoch [27/50], Loss: 42965.0195\n",
      "Epoch [28/50], Loss: 42903.5938\n",
      "Epoch [29/50], Loss: 43033.9961\n",
      "Epoch [30/50], Loss: 42904.8711\n",
      "Epoch [31/50], Loss: 42894.8477\n",
      "Epoch [32/50], Loss: 43003.8750\n",
      "Epoch [33/50], Loss: 42955.6406\n",
      "Epoch [34/50], Loss: 43096.3203\n",
      "Epoch [35/50], Loss: 43077.5039\n",
      "Epoch [36/50], Loss: 42985.2969\n",
      "Epoch [37/50], Loss: 42950.4180\n",
      "Epoch [38/50], Loss: 42902.5156\n",
      "Epoch [39/50], Loss: 43060.1914\n",
      "Epoch [40/50], Loss: 43019.2500\n",
      "Epoch [41/50], Loss: 42996.8477\n",
      "Epoch [42/50], Loss: 43067.2266\n",
      "Epoch [43/50], Loss: 42927.7969\n",
      "Epoch [44/50], Loss: 42923.4961\n",
      "Epoch [45/50], Loss: 42897.7344\n",
      "Epoch [46/50], Loss: 42928.8906\n",
      "Epoch [47/50], Loss: 42936.0195\n",
      "Epoch [48/50], Loss: 42941.3047\n",
      "Epoch [49/50], Loss: 42956.2500\n",
      "Epoch [50/50], Loss: 43111.6289\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting\")\n",
    "\n",
    "# Hyperparameters\n",
    "#input_dim = data.size(1) * data.size(2)\n",
    "input_dim = 64 * 40\n",
    "hidden_dim = 128\n",
    "latent_dim = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "vae_model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(batch)\n",
    "        loss = vae_model.loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68375495-db2c-4b19-83af-8d69eaa5b701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53e51c3f-89f8-41de-a46d-51eb348e3e42",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40d43eff-c569-4a7c-b3cf-ef428d97196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE LAYER SparseAE\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.relu(self.encoder(x))\n",
    "        decoded = self.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "579078d4-5d9c-45ec-b412-ad9997fba508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5839\n",
      "Epoch [2/20], Loss: 0.4633\n",
      "Epoch [3/20], Loss: 0.4926\n",
      "Epoch [4/20], Loss: 0.4715\n",
      "Epoch [5/20], Loss: 0.4261\n",
      "Epoch [6/20], Loss: 0.4545\n",
      "Epoch [7/20], Loss: 0.5200\n",
      "Epoch [8/20], Loss: 0.4748\n",
      "Epoch [9/20], Loss: 0.5622\n",
      "Epoch [10/20], Loss: 0.5075\n",
      "Epoch [11/20], Loss: 0.5505\n",
      "Epoch [12/20], Loss: 0.4733\n",
      "Epoch [13/20], Loss: 0.3731\n",
      "Epoch [14/20], Loss: 0.4040\n",
      "Epoch [15/20], Loss: 0.4385\n",
      "Epoch [16/20], Loss: 0.4478\n",
      "Epoch [17/20], Loss: 0.5168\n",
      "Epoch [18/20], Loss: 0.4492\n",
      "Epoch [19/20], Loss: 0.4661\n",
      "Epoch [20/20], Loss: 0.4727\n"
     ]
    }
   ],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 12\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "sparse_ae_model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = sparse_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "556637d9-d576-416d-bfe2-ef4a27dae1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N LAYER SparseAE\n",
    "## NOT WORKING YET\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        if type(hidden_dims) is int:\n",
    "            hidden_dims = list(hidden_dims)\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            encoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        for i in range(len(hidden_dims) - 1, 0, -1):\n",
    "            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        output = self.output_layer(decoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8137596-45af-4972-b837-65dfb791fe4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x40 and 3x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43msparse_ae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, batch)\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[57], line 26\u001b[0m, in \u001b[0;36mSparseAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     25\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 26\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(decoded)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x40 and 3x40)"
     ]
    }
   ],
   "source": [
    "input_dim = 40\n",
    "hidden_dim_lst = [3]\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "sparse_ae_model = SparseAutoencoder(input_dim, hidden_dim_lst)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = sparse_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780becf-e1c6-496a-95e7-1f9f1fbca123",
   "metadata": {},
   "source": [
    "# Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aaf4717f-35d1-4b24-8df8-e869e0f1a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE LAYER DenoisingAE\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.relu(self.encoder(x))\n",
    "        decoded = self.sigmoid(self.decoder(encoded))\n",
    "        return decoded\n",
    "\n",
    "    def add_noise(self, x, noise_level=0.1):\n",
    "        # Add Gaussian noise to the input\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94e8b68c-2401-4b99-8c23-5dea55abad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5360\n",
      "Epoch [2/20], Loss: 0.4331\n",
      "Epoch [3/20], Loss: 0.5389\n",
      "Epoch [4/20], Loss: 0.4440\n",
      "Epoch [5/20], Loss: 0.3972\n",
      "Epoch [6/20], Loss: 0.3777\n",
      "Epoch [7/20], Loss: 0.4812\n",
      "Epoch [8/20], Loss: 0.5647\n",
      "Epoch [9/20], Loss: 0.4161\n",
      "Epoch [10/20], Loss: 0.4335\n",
      "Epoch [11/20], Loss: 0.5013\n",
      "Epoch [12/20], Loss: 0.5023\n",
      "Epoch [13/20], Loss: 0.5446\n",
      "Epoch [14/20], Loss: 0.4555\n",
      "Epoch [15/20], Loss: 0.3840\n",
      "Epoch [16/20], Loss: 0.5035\n",
      "Epoch [17/20], Loss: 0.4043\n",
      "Epoch [18/20], Loss: 0.4634\n",
      "Epoch [19/20], Loss: 0.5212\n",
      "Epoch [20/20], Loss: 0.4279\n"
     ]
    }
   ],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 12\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "denoising_ae_model = DenoisingAutoencoder(input_dim, hidden_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = denoising_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd45f2-6881-490a-803f-eae28d4573e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
