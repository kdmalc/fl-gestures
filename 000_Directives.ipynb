{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44730608",
   "metadata": {},
   "source": [
    "> __Purpose:__ The purpose of this NB is to serve as a repository of notes/resources worth pursuiing, as well as to outline the overarching and ongoing goal of the project.\n",
    "\n",
    "# Current Goal\n",
    "1. Subspace clustering:\n",
    "    1. Reduce the dimensionality of the data. Ideally to remove noise and decrease the required computation time.\n",
    "        - Determine the optimal number of dimensions? Or at least sufficient... dim reduc may not really be that important, perhaps would be more important if we can show we're reducing it to a specific manifold or doing some kind of joint embedding, but basic approaches like PCA and t-SNE probably aren't as well informed\n",
    "        - How many PCs are required to explain at least 80% of the variance? \n",
    "        - Remember to normalize/scale the data first, especially when combining the EMG and IMU. Plus demean the EMG.\n",
    "    2. Cluster the latent representations: we are searching for clusters (presumably that are ability/anatomical in nature), looking to develop archetypes, or a set of hierarchical groupings such that we can direct model training and initilization on the basis of these groupings/clusters.\n",
    "        - Determine the optimal number of clusters...\n",
    "        - We don't have ground truth, so need to find some (possibly combination of) metric(s) to compare different cluster outputs\n",
    "    3. Finally, we need a way of few-shot cluster assignment. Template matching (basically just KNN), few-shot learning, meta-learning, mixture of experts, hierarchical FL?\n",
    "        - When testing, need to think about how to break up the data. Cross validation? Or at least hold out a few entire users, and a few entire gestures of some included users, and half of the gestures of some included users.\n",
    "\n",
    "\n",
    "# Subspace Clustering Resources\n",
    "## Misc Good StatsStackExchanges I haven't Read Yet\n",
    "- https://stats.stackexchange.com/questions/241381/clustering-methods-that-do-not-require-pre-specifying-the-number-of-clusters\n",
    "- https://stats.stackexchange.com/questions/95782/what-are-the-most-common-metrics-for-comparing-two-clustering-algorithms-especi\n",
    "- https://stats.stackexchange.com/questions/88550/using-the-gap-statistic-to-compare-algorithms\n",
    "- https://stats.stackexchange.com/questions/21807/evaluation-measures-of-goodness-or-validity-of-clustering-without-having-truth\n",
    "- https://stats.stackexchange.com/questions/195456/how-to-select-a-clustering-method-how-to-validate-a-cluster-solution-to-warran\n",
    "- https://stats.stackexchange.com/questions/23472/how-to-decide-on-the-correct-number-of-clusters\n",
    "\n",
    "## SciPy Dimensionality Reduction Algorithms\n",
    "- https://scikit-learn.org/stable/modules/manifold.html\n",
    "    - This one isn't code/docs, it's sort of more of a tutorial?\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\n",
    "    - Maybe there's an interesting angle about doing feature extraction on the data (effectively a form of dim reduction) and clustering based on that? Not sure if we could use feature extraction for real time tho... probably?\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster\n",
    "- https://scikit-learn.org/stable/modules/classes.html#clustering-metrics\n",
    "\n",
    "## Sklearn Clustering Algorithms\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster\n",
    "- AffinityPropagation\n",
    "- AgglomerativeClustering\n",
    "- Birch\n",
    "- DBSCAN\n",
    "    - DBSCAN on Wikipedia but also shows other approaches in the side bar: https://en.wikipedia.org/wiki/DBSCAN\n",
    "- HDBSCAN\n",
    "    - HDBSCAN Demo: https://scikit-learn.org/stable/auto_examples/cluster/plot_hdbscan.html\n",
    "- FeatureAgglomeration\n",
    "- KMeans\n",
    "- BisectingKMeans\n",
    "- MiniBatchKMeans\n",
    "- MeanShift\n",
    "- OPTICS\n",
    "- SpectralClustering\n",
    "- SpectralBiclustering\n",
    "- SpectralCoclustering\n",
    "## SciPy Clustering Algorithms\n",
    "- Lots of different hierarchical algorithms: https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\n",
    "\n",
    "## Misc Clustering Algorithms To Look In To\n",
    "- \"In addition, I recommend that you use any ensemble clustering technique that can assign to more partitions, so called consensus partitions, where objects are often better partitioned than in the initial partitions.\"\n",
    "\n",
    "## Sklearn Clustering Metrics\n",
    "- Internal Evaluation Metrics: Internal evaluation metrics assess the quality of the clustering based on the data itself, without relying on external information or the true number of clusters. Examples of internal evaluation metrics include the Silhouette coefficient, Davies-Bouldin index, Calinski-Harabasz index, and Dunn index. These metrics provide a measure of compactness, separation, or overall clustering quality.\n",
    "- External Evaluation Metrics: External evaluation metrics compare the clustering results with some external reference, such as known class labels if available. However, these metrics are not suitable when the number of clusters is not equal to the number of classes. If you have class labels, you could consider transforming the clustering results into a classification problem by assigning cluster labels to the data points and then use traditional classification evaluation metrics such as accuracy, precision, recall, or F1 score.\n",
    "- Stability-based Methods: Stability-based methods evaluate clustering stability by assessing the consistency of clustering results across multiple iterations or subsets of the data. Techniques like stability index, bootstrapping, or consensus clustering can provide insights into the robustness of the clustering algorithm and help determine the optimal number of clusters.\n",
    "- Visual Inspection: Visual inspection can be a useful approach to evaluate clustering results. Plotting the data points in a low-dimensional space using dimensionality reduction techniques (e.g., t-SNE, PCA) and coloring the points according to the clustering results can provide an intuitive visualization of the clusters. However, this method is subjective and might not provide a quantitative measure of performance.\n",
    "- The AMI or (Adjusted Mutual Information) score is rescaled such that random clustering has a score of 0. The NMI (Normalized Mutual Information) is used for cases where you have a different number of clusters and therefore often a golden standard in the clustering community. Both measures range between 0 and 1, where 0 is considered as random clustering and 1 matches the ground truth perfectly.\n",
    "- There exist also other measures such as F-measure or Purity.\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "- Adjusted Mutual Information Score: \n",
    "    - Unsupervised. It measures the agreement between two clusterings, making it an unsupervised metric.\n",
    "- Adjusted Rand Score: \n",
    "    - Unsupervised. It quantifies the similarity between two clusterings, making it an unsupervised metric.\n",
    "- Calinski-Harabasz Score: \n",
    "    - Unsupervised. It measures the ratio of between-cluster dispersion to within-cluster dispersion, making it an unsupervised metric.\n",
    "- Davies-Bouldin Score: \n",
    "    - Unsupervised. It evaluates the average similarity between each cluster and its most similar cluster, making it an unsupervised metric.\n",
    "- Completeness Score: \n",
    "    - Supervised. It measures the completeness of a clustering given the ground truth labels, making it a supervised metric.\n",
    "- Contingency Matrix: \n",
    "    - Unsupervised. It describes the relationship between labels in two clusterings, making it an unsupervised tool.\n",
    "- Pair Confusion Matrix: \n",
    "    - Unsupervised. It represents the confusion between two clusterings, making it an unsupervised tool.\n",
    "- Fowlkes-Mallows Score: \n",
    "    - Unsupervised. It computes the similarity between two clusterings, making it an unsupervised metric.\n",
    "- Homogeneity, Completeness, and V-Measure Scores: \n",
    "    - Supervised. These metrics evaluate the agreement between a clustering and the ground truth labels, making them supervised metrics.\n",
    "- Homogeneity Score: \n",
    "    - Supervised. It measures the homogeneity of a clustering given the ground truth labels, making it a supervised metric.\n",
    "- Mutual Information Score: \n",
    "    - Unsupervised. It quantifies the mutual information between two clusterings, making it an unsupervised metric.\n",
    "- Normalized Mutual Information Score: \n",
    "    - Unsupervised. It computes the normalized mutual information between two clusterings, making it an unsupervised metric.\n",
    "- Rand Score: \n",
    "    - Unsupervised. It computes the Rand index, which measures the similarity between two clusterings, making it an unsupervised metric.\n",
    "- Silhouette Score and Silhouette Samples: \n",
    "    - Unsupervised. These metrics evaluate the quality of clusters based on intra-cluster and inter-cluster distances, making them unsupervised.\n",
    "    - Silhouette analysis demo: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "- V-Measure Score: \n",
    "    - Supervised. It evaluates the harmonic mean of homogeneity and completeness, making it a supervised metric.\n",
    "> Other metrics:\n",
    "- https://scikit-learn.org/stable/modules/classes.html#pairwise-metrics\n",
    "    - Not a clustering metric, but rather pairwise/distance metrics. People frequently take the cosine similarity between matrices, that's essentially what we are looking to do between gestures. These are basically just different distance metrics we could use.\n",
    "    \n",
    "## Choosing Optimal Number of Clusters\n",
    "- Partitioning clustering methods, like k-means and Partitioning Around Medoids (PAM), require that you specify the number of clusters to be generated.\n",
    "- The Sum of Squares Method: choose the optimal number of cluster by minimizing the within-cluster sum of squares (a measure of how tight each cluster is) and maximizing the between-cluster sum of squares (a measure of how seperated each cluster is from the others).\n",
    "- Clustree (does it exist in Python or only in R) and dendograms\n",
    "- \"For example, many of the above heuristics contradicted each other for what the optimal number of clusters was. Keep in mind these were all evaluating the k-means algorithm at different numbers of k. This could potentially mean that the k-means algorithm fails and no k is good. The k-means algorithm is not a very robust algorithm that is sensitive to outliers and this data set is quit small.\"\n",
    "- Also you could use an algorithm which does not require the number of clusters as input. DBSCAN or HDBSCAN should scale fine to your dataset size.\n",
    "- https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\n",
    "- https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/\n",
    "\n",
    "## Misc Dimensionality Reduction Github repos I haven't really looked at:\n",
    "- https://github.com/heucoder/dimensionality_reduction_alo_codes\n",
    "- https://github.com/azampagl/ai-ml-clustering/tree/master\n",
    "- https://github.com/deeptime-ml/deeptime\n",
    "    - This one is a whole library, you have to build the docs to view them, probably too much work, but if we could use some of their algos / evals that may be helpful\n",
    "\n",
    "## Random subspace clustering Github repos I haven't really looked at:\n",
    "> May or may not be useful. These are probably paper implementations so are likely too specific to be useful for us\n",
    "- https://github.com/ChongYou/subspace-clustering\n",
    "- https://github.com/panji530/Deep-subspace-clustering-networks\n",
    "- https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering\n",
    "\n",
    "## Gesture Clustering Github repos I haven't really looked at:\n",
    "- https://github.com/pjyazdian/Gesture2Vec\n",
    "- https://github.com/hamzaiqbal786/FYP-Adaptive-Clustering-For-Gesture-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aefa27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
