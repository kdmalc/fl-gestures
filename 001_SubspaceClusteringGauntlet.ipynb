{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f961eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do these need to get imported still if they're imported in the other .py file? Idk\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.manifold import TSNE\n",
    "#from sklearn.decomposition import IncrementalPCA\n",
    "#from sklearn.decomposition import KernelPCA\n",
    "#from sklearn.manifold import MDS\n",
    "#from sklearn.manifold import Isomap\n",
    "\n",
    "#from umap import UMAP  # <-- THIS IS NOT PART OF SCIPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d49703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subspace_clustering_helper_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a9b09",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e073f3",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904a58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pID 101 because it doesn't exist\n",
    "# remove pID 131 because it  doesnt have enough user defined gestures\n",
    "# each participant has 100 experimenter defined files and 50 user defined files\n",
    "# 10 experimenter defined gestures and 5 user defined gestures\n",
    "\n",
    "file_types = [\"IMU_extract\", \"movavg_files\"]\n",
    "expt_types = [\"experimenter-defined\"]\n",
    "\n",
    "#remove participant 131 because they are missing gestures \n",
    "pIDs_impaired = ['P102','P103','P104','P105','P106','P107','P108','P109','P110','P111',\n",
    "       'P112','P114','P115','P116','P118','P119','P121','P122','P123','P124','P125',\n",
    "       'P126','P127','P128', 'P132']\n",
    "# remove participants P001 and P003 because they dont have duplicate or open gestures\n",
    "pIDs_unimpaired = ['P004','P005','P006','P008','P010','P011']\n",
    "\n",
    "pIDs_both = pIDs_impaired + pIDs_unimpaired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451df465",
   "metadata": {},
   "source": [
    "Version using dataframes (wasted way too much time writing this...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5c8a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pIDs, data_dir_path=\"C:\\\\Users\\\\kdmen\\\\Box Sync\\\\$M data segmented\\\\segmented_filtered_data\\\\\", file_types=[\"IMU_extract\", \"movavg_files\"], expt_types=[\"experimenter-defined\"]):\n",
    "    \n",
    "    data_dict = {}\n",
    "\n",
    "    for expt_type in expt_types:\n",
    "        for pid in pIDs:\n",
    "            print(\"pid\")\n",
    "            pid_path = os.path.join(data_dir_path, pid)\n",
    "            for file_type in file_types:\n",
    "                sub_path = os.path.join(pid_path, file_type)\n",
    "                if not os.path.exists(sub_path):\n",
    "                    print(f\"Subpath does not exist: {sub_path}\")\n",
    "                    continue\n",
    "                for file in os.listdir(sub_path):\n",
    "                    split_filename = file.split(\"_\")\n",
    "                    if len(split_filename) < 6:\n",
    "                        print(f\"Unexpected filename format: {file}\")\n",
    "                        continue\n",
    "                    gestureID = split_filename[4]\n",
    "                    gestureNum = split_filename[5]\n",
    "\n",
    "                    if file_type == \"movavg_files\":\n",
    "                        headers = ['EMG1', 'EMG2', 'EMG3', 'EMG4', 'EMG5',\n",
    "                                   'EMG6', 'EMG7', 'EMG8', 'EMG9', 'EMG10',\n",
    "                                   'EMG11', 'EMG12', 'EMG13', 'EMG14', 'EMG15',\n",
    "                                   'EMG16']\n",
    "                    else:\n",
    "                        headers = ['IMU1_ax', 'IMU1_ay', 'IMU1_az', 'IMU1_vx', 'IMU1_vy', 'IMU1_vz',\n",
    "                                   'IMU2_ax', 'IMU2_ay', 'IMU2_az', 'IMU2_vx', 'IMU2_vy', 'IMU2_vz',\n",
    "                                   'IMU3_ax', 'IMU3_ay', 'IMU3_az', 'IMU3_vx', 'IMU3_vy', 'IMU3_vz',\n",
    "                                   'IMU4_ax', 'IMU4_ay', 'IMU4_az', 'IMU4_vx', 'IMU4_vy', 'IMU4_vz',\n",
    "                                   'IMU5_ax', 'IMU5_ay', 'IMU5_az', 'IMU5_vx', 'IMU5_vy', 'IMU5_vz',\n",
    "                                   'IMU6_ax', 'IMU6_ay', 'IMU6_az', 'IMU6_vx', 'IMU6_vy', 'IMU6_vz',\n",
    "                                   'IMU7_ax', 'IMU7_ay', 'IMU7_az', 'IMU7_vx', 'IMU7_vy', 'IMU7_vz',\n",
    "                                   'IMU8_ax', 'IMU8_ay', 'IMU8_az', 'IMU8_vx', 'IMU8_vy', 'IMU8_vz',\n",
    "                                   'IMU9_ax', 'IMU9_ay', 'IMU9_az', 'IMU9_vx', 'IMU9_vy', 'IMU9_vz',\n",
    "                                   'IMU11_ax', 'IMU11_ay', 'IMU11_az', 'IMU11_vx', 'IMU11_vy', 'IMU11_vz',\n",
    "                                   'IMU13_ax', 'IMU13_ay', 'IMU13_az', 'IMU13_vx', 'IMU13_vy', 'IMU13_vz',\n",
    "                                   'IMU15_ax', 'IMU15_ay', 'IMU15_az', 'IMU15_vx', 'IMU15_vy', 'IMU15_vz']\n",
    "\n",
    "                    file_path = os.path.join(sub_path, file)\n",
    "                    if not os.path.exists(file_path):\n",
    "                        print(f\"File does not exist: {file_path}\")\n",
    "                        continue\n",
    "                    df = pd.read_csv(file_path, names=headers, header=0)\n",
    "                    if df.empty:\n",
    "                        print(f\"DataFrame is empty for file: {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    df['Participant'] = pid\n",
    "                    df['Gesture_ID'] = gestureID\n",
    "                    df['Gesture_Num'] = gestureNum\n",
    "\n",
    "                    # Interpolate the data \n",
    "                    df_interpolated = interpolate_df(df, num_rows=64, columns_to_exclude=['Participant', 'Gesture_ID', 'Gesture_Num'])\n",
    "\n",
    "                    # Create a unique key based on Participant, Gesture_ID, and Gesture_Num\n",
    "                    unique_key = (pid, gestureID, gestureNum)\n",
    "\n",
    "                    if unique_key in data_dict:\n",
    "                        # Merge the DataFrames on index to avoid duplicate columns\n",
    "                        existing_df = data_dict[unique_key]\n",
    "                        merged_df = existing_df.merge(df_interpolated, left_index=True, right_index=True, suffixes=('', '_dup'))\n",
    "                        # Drop duplicate columns if necessary\n",
    "                        for col in merged_df.columns:\n",
    "                            if col.endswith('_dup'):\n",
    "                                merged_df.drop(columns=col, inplace=True)\n",
    "                        data_dict[unique_key] = merged_df\n",
    "                    else:\n",
    "                        data_dict[unique_key] = df_interpolated\n",
    "\n",
    "    # Convert the dictionary to a list of DataFrames\n",
    "    data_lst = list(data_dict.values())\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    print(f\"Shape of first df: {data_lst[0].shape} (expected shape is (64, 91))\")\n",
    "    # ... does this work as expected... shouldn't it also be ele_df[0].shape\n",
    "    edited_data_lst = [ele_df for ele_df in data_lst if ele_df.shape == data_lst[0].shape]\n",
    "    dataframe = pd.concat(edited_data_lst, ignore_index=True)\n",
    "\n",
    "    # Check for NaN values in the resulting dataframe\n",
    "    nan_participant_rows = dataframe[dataframe['Participant'].isna()]\n",
    "    print(f\"Number of rows with NaN Participant: {nan_participant_rows.shape[0]}\")\n",
    "        \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c64b6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Participant ID: P102\n",
      "Processing Participant ID: P103\n",
      "Processing Participant ID: P104\n",
      "Processing Participant ID: P105\n",
      "Processing Participant ID: P106\n",
      "Processing Participant ID: P107\n",
      "Processing Participant ID: P108\n",
      "Processing Participant ID: P109\n",
      "Processing Participant ID: P110\n",
      "Processing Participant ID: P111\n",
      "Processing Participant ID: P112\n",
      "Processing Participant ID: P114\n",
      "Processing Participant ID: P115\n",
      "Processing Participant ID: P116\n",
      "Processing Participant ID: P118\n",
      "Processing Participant ID: P119\n",
      "Processing Participant ID: P121\n",
      "Processing Participant ID: P122\n",
      "Processing Participant ID: P123\n",
      "Processing Participant ID: P124\n",
      "Processing Participant ID: P125\n",
      "Processing Participant ID: P126\n",
      "Processing Participant ID: P127\n",
      "Processing Participant ID: P128\n",
      "Processing Participant ID: P132\n",
      "Processing Participant ID: P004\n",
      "Processing Participant ID: P005\n",
      "Processing Participant ID: P006\n",
      "Processing Participant ID: P008\n",
      "Processing Participant ID: P010\n",
      "Processing Participant ID: P011\n",
      "Shape of first df: (64, 91) (expected shape is (64, 91))\n",
      "Number of rows with NaN Participant: 0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "data_df = load_data(pIDs_both)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nCompleted in {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d57165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426752, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>IMU1_ax</th>\n",
       "      <th>IMU1_ay</th>\n",
       "      <th>IMU1_az</th>\n",
       "      <th>IMU1_vx</th>\n",
       "      <th>IMU1_vy</th>\n",
       "      <th>IMU1_vz</th>\n",
       "      <th>IMU2_ax</th>\n",
       "      <th>...</th>\n",
       "      <th>EMG7</th>\n",
       "      <th>EMG8</th>\n",
       "      <th>EMG9</th>\n",
       "      <th>EMG10</th>\n",
       "      <th>EMG11</th>\n",
       "      <th>EMG12</th>\n",
       "      <th>EMG13</th>\n",
       "      <th>EMG14</th>\n",
       "      <th>EMG15</th>\n",
       "      <th>EMG16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.939941</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>-0.192625</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>-0.380859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336178</td>\n",
       "      <td>-0.963185</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>-0.190446</td>\n",
       "      <td>-0.026116</td>\n",
       "      <td>-0.394547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353539</td>\n",
       "      <td>-0.963704</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.095966</td>\n",
       "      <td>-0.205480</td>\n",
       "      <td>-0.155563</td>\n",
       "      <td>-0.398406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352841</td>\n",
       "      <td>-0.950288</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>0.058836</td>\n",
       "      <td>-0.184871</td>\n",
       "      <td>-0.083567</td>\n",
       "      <td>-0.389230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372621</td>\n",
       "      <td>-0.991273</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.293946</td>\n",
       "      <td>-0.178756</td>\n",
       "      <td>-0.281361</td>\n",
       "      <td>-0.396043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID Gesture_Num   IMU1_ax   IMU1_ay   IMU1_az   IMU1_vx  \\\n",
       "0        P102        pan           1  0.341797 -0.939941  0.000977 -0.007450   \n",
       "1        P102        pan           1  0.336178 -0.963185  0.003898  0.009595   \n",
       "2        P102        pan           1  0.353539 -0.963704  0.011711  0.095966   \n",
       "3        P102        pan           1  0.352841 -0.950288  0.011509  0.058836   \n",
       "4        P102        pan           1  0.372621 -0.991273  0.029847  0.293946   \n",
       "\n",
       "    IMU1_vy   IMU1_vz   IMU2_ax  ...      EMG7      EMG8      EMG9     EMG10  \\\n",
       "0 -0.192625  0.005321 -0.380859  ...  0.000002  0.000002  0.000003  0.000020   \n",
       "1 -0.190446 -0.026116 -0.394547  ...  0.000003  0.000003  0.000003  0.000014   \n",
       "2 -0.205480 -0.155563 -0.398406  ...  0.000003  0.000003  0.000004  0.000007   \n",
       "3 -0.184871 -0.083567 -0.389230  ...  0.000003  0.000003  0.000006  0.000005   \n",
       "4 -0.178756 -0.281361 -0.396043  ...  0.000003  0.000002  0.000008  0.000003   \n",
       "\n",
       "      EMG11     EMG12     EMG13     EMG14     EMG15     EMG16  \n",
       "0  0.000004  0.000004  0.000002  0.000009  0.000001  0.000002  \n",
       "1  0.000007  0.000007  0.000002  0.000017  0.000001  0.000002  \n",
       "2  0.000004  0.000005  0.000003  0.000020  0.000003  0.000002  \n",
       "3  0.000004  0.000003  0.000004  0.000015  0.000003  0.000003  \n",
       "4  0.000007  0.000022  0.000004  0.000017  0.000002  0.000003  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23843601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataframe, assuming 'df' is your dataframe\n",
    "# Count NaNs per row\n",
    "nans_per_row = data_df.isna().sum(axis=1)\n",
    "\n",
    "# Count NaNs per column\n",
    "nans_per_column = data_df.isna().sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc86a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    426752.0\n",
       "mean          0.0\n",
       "std           0.0\n",
       "min           0.0\n",
       "25%           0.0\n",
       "50%           0.0\n",
       "75%           0.0\n",
       "max           0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics for NaNs per row\n",
    "nans_per_row.describe()\n",
    "\n",
    "# Summary statistics for NaNs per column\n",
    "#nans_per_column.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80b591",
   "metadata": {},
   "source": [
    "## Applying dimensionality reduction algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dee529",
   "metadata": {},
   "source": [
    "Modified version of Ben's PCA_all_participants() func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d6caab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this later, just putting it here so I don't have to restart my kernel and reload in all the data...\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "def apply_model(model_str, input_df, num_dims, hp):\n",
    "    \n",
    "    # Drop the metadata columns (eg cols that are not the actual timeseries data)\n",
    "    #input_df.drop(columns=['Participant', 'Gesture_ID', 'Gesture_Num', 'Gesture_Type', 'File_Type'], inplace=True)\n",
    "    training_df = input_df.drop(columns=['Participant', 'Gesture_ID', 'Gesture_Num'])\n",
    "    \n",
    "    if not training_df.empty:\n",
    "        if model_str.upper() == 'PCA':\n",
    "            dim_reduc_model = PCA(n_components=num_dims)\n",
    "            dim_reduc_model.fit(training_df)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.transform(training_df))\n",
    "        elif model_str.upper() == 'T-SNE':\n",
    "            dim_reduc_model = TSNE(n_components=num_dims, perplexity=hp, random_state=42)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(df))\n",
    "        elif (model_str.upper() == 'INCREMENTALPCA') or (model_str.upper() == 'IPCA'):\n",
    "            dim_reduc_model = IncrementalPCA(n_components=num_dims)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        elif (model_str.upper() == 'KERNELPCA') or (model_str.upper() == 'KPCA'):\n",
    "            dim_reduc_model = KernelPCA(n_components=num_dims)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        #elif model_str.upper() == 'UMAP':\n",
    "        #    raise ValueError(\"Need to install the umap library first...\")\n",
    "        #    dim_reduc_model = UMAP(n_components=num_dims)\n",
    "        #    reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        elif model_str.upper() == 'MDS':\n",
    "            dim_reduc_model = MDS(n_components=num_dims, random_state=42)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        elif model_str.upper() == 'ISOMAP':\n",
    "            dim_reduc_model = Isomap(n_components=num_dims)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        else:\n",
    "            raise ValueError(f\"{model_str} not implemented. Choose an implemented model.\")\n",
    "    else:\n",
    "        raise ValueError(f\"training_df is empty!\")\n",
    "    \n",
    "    return reduced_df, dim_reduc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634e36",
   "metadata": {},
   "source": [
    "Testing in non-functionalized form first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start\")\n",
    "\n",
    "#def apply_dim_reduc(\n",
    "model_str = 'PCA'\n",
    "#data_df\n",
    "num_dims=2\n",
    "hp=None\n",
    "modality=['EMG and IMU'],\n",
    "participant_inclusion=['All'] #['Impaired', 'Unimpaired']\n",
    "\n",
    "# ADD THIS TO FUNC!!!\n",
    "apply='ALL'\n",
    "\n",
    "###########################################\n",
    "\n",
    "gestures = ['pan', 'duplicate', 'zoom-out', 'zoom-in', 'move', 'rotate', 'select-single', 'delete', 'close', 'open']\n",
    "data_types = modality\n",
    "participant_types = participant_inclusion\n",
    "\n",
    "for f_type in data_types:\n",
    "    # My code assumes you are doing EMG and IMU together...\n",
    "    ## Add slicing functionality later\n",
    "    if f_type[0] == 'EMG and IMU':\n",
    "        sel_df = data_df\n",
    "    #elif f_type[0] == 'IMU':\n",
    "    #    # slice just the IMU columns (cols with IMU in name)\n",
    "    #elif f_type[0] == 'EMG':\n",
    "    #    # slice just the EMG columns (cols with EMG in name)\n",
    "    else:\n",
    "        raise ValueError(f\"f_type {f_type} not found in [EMG, IMU, EMG and IMU]\")\n",
    "\n",
    "    for p_type in participant_types:\n",
    "        if p_type == \"All\":\n",
    "            pIDs = sel_df['Participant'].unique()\n",
    "        elif p_type == \"Impaired\":\n",
    "            # Idk what this indexing by ['Participant'] the second time is doing, presumably is broken\n",
    "            pIDs = sel_df[sel_df['Participant'].isin(pIDs_impaired)]['Participant'].unique()\n",
    "        elif p_type == \"Unimpaired\":\n",
    "            # Idk what this indexing by ['Participant'] the second time is doing, presumably is broken\n",
    "            pIDs = sel_df[sel_df['Participant'].isin(pIDs_unimpaired)]['Participant'].unique()\n",
    "        else:\n",
    "            raise ValueError(f\"Participant type {p_type} not supported, check supported versions.\")\n",
    "\n",
    "        if apply.upper() == 'ALL':\n",
    "            df_t, dim_reduc_model = apply_model(model_str, sel_df, num_dims, hp)\n",
    "        elif apply.upper() == 'BY USER':\n",
    "            for pid in pIDs:\n",
    "                for file_type in file_types:\n",
    "                        user_df = sel_df[(sel_df['Participant'] == pid)]\n",
    "                        df_t, dim_reduc_model = apply_model(model_str, user_df, num_dims, hp)\n",
    "        elif apply.upper() == 'BY GESTURE':\n",
    "            for file_type in file_types:\n",
    "                for gesture in gestures:\n",
    "                    gesture_df = sel_df[(data_df['Gesture_ID'] == gesture)]\n",
    "                    df_t, dim_reduc_model = apply_model(model_str, gesture_df, num_dims, hp)\n",
    "\n",
    "    #return df_t, dim_reduc_model\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed02795",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0e9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba2345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05acac16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedcc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7fdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945766be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dim_reduc(model_str, data_df, num_dims, hp=None, modality=['EMG and IMU'], participant_inclusion=['All Participants']):\n",
    "    '''\n",
    "    model_str: what kind of model to use (eg PCA, T-SNE, ...)\n",
    "    data_df: df containing all the (training) data\n",
    "    num_dims: how many dimensions/components should be used [HYPERPARAM!]\n",
    "    hp: [hyperparams] use this to store \n",
    "    modality: ['EMG', 'IMU', 'EMG and IMU']\n",
    "    '''\n",
    "    \n",
    "    pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625f5de",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dim_reduc():\n",
    "    # AIC? ...\n",
    "    # For PCA can used explained variance\n",
    "    ## Probably easier to not do this as a function, unless I use the same criteria for each method...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4411a",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do train/test splits?\n",
    "# I have plenty of clustering metrics but not dim reduc specific ones..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc63be",
   "metadata": {},
   "source": [
    "## Choosing Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4231905",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de44fc",
   "metadata": {},
   "source": [
    "> Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da15f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# determining the maximum number of clusters \n",
    "# using the simple method\n",
    "limit = int((dataset_new.shape[0]//2)**0.5)\n",
    " \n",
    "# wcss - within cluster sum of squared distances\n",
    "wcss = {}\n",
    " \n",
    "for k in range(2,limit+1):\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(dataset_new)\n",
    "    wcss[k] = model.inertia_\n",
    "     \n",
    "# plotting the wcss values to find the elbow value\n",
    "plt.plot(wcss.keys(), wcss.values(), 'gs-')\n",
    "plt.xlabel('Values of \"k\"')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# determining the maximum number of clusters using the simple method\n",
    "limit = int((dataset_new.shape[0]//2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefab753",
   "metadata": {},
   "source": [
    "> Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "for k in range(2, limit+1):\n",
    "\tmodel = KMeans(n_clusters=k)\n",
    "\tmodel.fit(dataset_new)\n",
    "\tpred = model.predict(dataset_new)\n",
    "\tscore = silhouette_score(dataset_new, pred)\n",
    "\tprint('Silhouette Score for k = {}: {:<.3f}'.format(k, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184fc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d2a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce2abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
