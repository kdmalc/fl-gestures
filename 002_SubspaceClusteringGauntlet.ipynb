{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f961eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do these need to get imported still if they're imported in the other .py file? Idk\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.manifold import TSNE\n",
    "#from sklearn.decomposition import IncrementalPCA\n",
    "#from sklearn.decomposition import KernelPCA\n",
    "#from sklearn.manifold import MDS\n",
    "#from sklearn.manifold import Isomap\n",
    "\n",
    "#from umap import UMAP  # <-- THIS IS NOT PART OF SCIPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d49703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subspace_clustering_helper_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a9b09",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e073f3",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904a58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pID 101 because it doesn't exist\n",
    "# remove pID 131 because it  doesnt have enough user defined gestures\n",
    "# each participant has 100 experimenter defined files and 50 user defined files\n",
    "# 10 experimenter defined gestures and 5 user defined gestures\n",
    "\n",
    "file_types = [\"IMU_extract\", \"movavg_files\"]\n",
    "expt_types = [\"experimenter-defined\"]\n",
    "\n",
    "#remove participant 131 because they are missing gestures \n",
    "pIDs_impaired = ['P102','P103','P104','P105','P106','P107','P108','P109','P110','P111',\n",
    "       'P112','P114','P115','P116','P118','P119','P121','P122','P123','P124','P125',\n",
    "       'P126','P127','P128', 'P132']\n",
    "# remove participants P001 and P003 because they dont have duplicate or open gestures\n",
    "pIDs_unimpaired = ['P004','P005','P006','P008','P010','P011']\n",
    "\n",
    "pIDs_both = pIDs_impaired + pIDs_unimpaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fbe4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Completed in 1.046649694442749s\n"
     ]
    }
   ],
   "source": [
    "## Pickle is theoretically faster for Python...\n",
    "\n",
    "print(\"Loading\")\n",
    "start_time = time.time()\n",
    "data_df = pd.read_pickle('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\metadata_IMU_EMG_allgestures_allusers.pkl')\n",
    "end_time = time.time()\n",
    "print(f\"Completed in {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f2dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426752, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>IMU1_ax</th>\n",
       "      <th>IMU1_ay</th>\n",
       "      <th>IMU1_az</th>\n",
       "      <th>IMU1_vx</th>\n",
       "      <th>IMU1_vy</th>\n",
       "      <th>IMU1_vz</th>\n",
       "      <th>IMU2_ax</th>\n",
       "      <th>...</th>\n",
       "      <th>EMG7</th>\n",
       "      <th>EMG8</th>\n",
       "      <th>EMG9</th>\n",
       "      <th>EMG10</th>\n",
       "      <th>EMG11</th>\n",
       "      <th>EMG12</th>\n",
       "      <th>EMG13</th>\n",
       "      <th>EMG14</th>\n",
       "      <th>EMG15</th>\n",
       "      <th>EMG16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.939941</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>-0.192625</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>-0.380859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336178</td>\n",
       "      <td>-0.963185</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>-0.190446</td>\n",
       "      <td>-0.026116</td>\n",
       "      <td>-0.394547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353539</td>\n",
       "      <td>-0.963704</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.095966</td>\n",
       "      <td>-0.205480</td>\n",
       "      <td>-0.155563</td>\n",
       "      <td>-0.398406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352841</td>\n",
       "      <td>-0.950288</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>0.058836</td>\n",
       "      <td>-0.184871</td>\n",
       "      <td>-0.083567</td>\n",
       "      <td>-0.389230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372621</td>\n",
       "      <td>-0.991273</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>0.293946</td>\n",
       "      <td>-0.178756</td>\n",
       "      <td>-0.281361</td>\n",
       "      <td>-0.396043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID Gesture_Num   IMU1_ax   IMU1_ay   IMU1_az   IMU1_vx  \\\n",
       "0        P102        pan           1  0.341797 -0.939941  0.000977 -0.007450   \n",
       "1        P102        pan           1  0.336178 -0.963185  0.003898  0.009595   \n",
       "2        P102        pan           1  0.353539 -0.963704  0.011711  0.095966   \n",
       "3        P102        pan           1  0.352841 -0.950288  0.011509  0.058836   \n",
       "4        P102        pan           1  0.372621 -0.991273  0.029847  0.293946   \n",
       "\n",
       "    IMU1_vy   IMU1_vz   IMU2_ax  ...      EMG7      EMG8      EMG9     EMG10  \\\n",
       "0 -0.192625  0.005321 -0.380859  ...  0.000002  0.000002  0.000003  0.000020   \n",
       "1 -0.190446 -0.026116 -0.394547  ...  0.000003  0.000003  0.000003  0.000014   \n",
       "2 -0.205480 -0.155563 -0.398406  ...  0.000003  0.000003  0.000004  0.000007   \n",
       "3 -0.184871 -0.083567 -0.389230  ...  0.000003  0.000003  0.000006  0.000005   \n",
       "4 -0.178756 -0.281361 -0.396043  ...  0.000003  0.000002  0.000008  0.000003   \n",
       "\n",
       "      EMG11     EMG12     EMG13     EMG14     EMG15     EMG16  \n",
       "0  0.000004  0.000004  0.000002  0.000009  0.000001  0.000002  \n",
       "1  0.000007  0.000007  0.000002  0.000017  0.000001  0.000002  \n",
       "2  0.000004  0.000005  0.000003  0.000020  0.000003  0.000002  \n",
       "3  0.000004  0.000003  0.000004  0.000015  0.000003  0.000003  \n",
       "4  0.000007  0.000022  0.000004  0.000017  0.000002  0.000003  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80b591",
   "metadata": {},
   "source": [
    "## Applying dimensionality reduction algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dee529",
   "metadata": {},
   "source": [
    "Modified version of Ben's PCA_all_participants() func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6caab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this later, just putting it here so I don't have to restart my kernel and reload in all the data...\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "def apply_model(model_str, input_df, num_dims, hp):\n",
    "    \n",
    "    # Drop the metadata columns (eg cols that are not the actual timeseries data)\n",
    "    #input_df.drop(columns=['Participant', 'Gesture_ID', 'Gesture_Num', 'Gesture_Type', 'File_Type'], inplace=True)\n",
    "    training_df = input_df.drop(columns=['Participant', 'Gesture_ID', 'Gesture_Num'])\n",
    "    \n",
    "    if not training_df.empty:\n",
    "        if model_str.upper() == 'PCA':\n",
    "            dim_reduc_model = PCA(n_components=num_dims)\n",
    "            dim_reduc_model.fit(training_df)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.transform(training_df))\n",
    "        elif (model_str.upper() == 'T-SNE') or (model_str.upper() == 'TSNE'):\n",
    "            dim_reduc_model = TSNE(n_components=num_dims, perplexity=hp, random_state=42)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(df))\n",
    "        elif (model_str.upper() == 'INCREMENTALPCA') or (model_str.upper() == 'IPCA'):\n",
    "            dim_reduc_model = IncrementalPCA(n_components=num_dims)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        elif (model_str.upper() == 'KERNELPCA') or (model_str.upper() == 'KPCA'):\n",
    "            dim_reduc_model = KernelPCA(n_components=num_dims)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        #elif model_str.upper() == 'UMAP':\n",
    "        #    raise ValueError(\"Need to install the umap library first...\")\n",
    "        #    dim_reduc_model = UMAP(n_components=num_dims)\n",
    "        #    reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        elif model_str.upper() == 'MDS':\n",
    "            dim_reduc_model = MDS(n_components=num_dims, random_state=42)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        elif model_str.upper() == 'ISOMAP':\n",
    "            dim_reduc_model = Isomap(n_components=num_dims)\n",
    "            reduced_df = pd.DataFrame(dim_reduc_model.fit_transform(training_df))\n",
    "        else:\n",
    "            raise ValueError(f\"{model_str} not implemented. Choose an implemented model.\")\n",
    "    else:\n",
    "        raise ValueError(f\"training_df is empty!\")\n",
    "    \n",
    "    return reduced_df, dim_reduc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634e36",
   "metadata": {},
   "source": [
    "Testing in non-functionalized form first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start\")\n",
    "\n",
    "#def apply_dim_reduc(\n",
    "model_str = 'PCA'\n",
    "#data_df\n",
    "num_dims=2\n",
    "hp=None\n",
    "modality=['EMG and IMU'],\n",
    "participant_inclusion=['All'] #['Impaired', 'Unimpaired']\n",
    "\n",
    "# ADD THIS TO FUNC!!!\n",
    "apply='ALL'\n",
    "\n",
    "###########################################\n",
    "\n",
    "gestures = ['pan', 'duplicate', 'zoom-out', 'zoom-in', 'move', 'rotate', 'select-single', 'delete', 'close', 'open']\n",
    "data_types = modality\n",
    "participant_types = participant_inclusion\n",
    "\n",
    "for f_type in data_types:\n",
    "    # My code assumes you are doing EMG and IMU together...\n",
    "    ## Add slicing functionality later\n",
    "    if f_type[0] == 'EMG and IMU':\n",
    "        sel_df = data_df\n",
    "    #elif f_type[0] == 'IMU':\n",
    "    #    # slice just the IMU columns (cols with IMU in name)\n",
    "    #elif f_type[0] == 'EMG':\n",
    "    #    # slice just the EMG columns (cols with EMG in name)\n",
    "    else:\n",
    "        raise ValueError(f\"f_type {f_type} not found in [EMG, IMU, EMG and IMU]\")\n",
    "\n",
    "    for p_type in participant_types:\n",
    "        if p_type == \"All\":\n",
    "            pIDs = sel_df['Participant'].unique()\n",
    "        elif p_type == \"Impaired\":\n",
    "            # Idk what this indexing by ['Participant'] the second time is doing, presumably is broken\n",
    "            pIDs = sel_df[sel_df['Participant'].isin(pIDs_impaired)]['Participant'].unique()\n",
    "        elif p_type == \"Unimpaired\":\n",
    "            # Idk what this indexing by ['Participant'] the second time is doing, presumably is broken\n",
    "            pIDs = sel_df[sel_df['Participant'].isin(pIDs_unimpaired)]['Participant'].unique()\n",
    "        else:\n",
    "            raise ValueError(f\"Participant type {p_type} not supported, check supported versions.\")\n",
    "\n",
    "        if apply.upper() == 'ALL':\n",
    "            df_t, dim_reduc_model = apply_model(model_str, sel_df, num_dims, hp)\n",
    "        elif apply.upper() == 'BY USER':\n",
    "            for pid in pIDs:\n",
    "                for file_type in file_types:\n",
    "                        user_df = sel_df[(sel_df['Participant'] == pid)]\n",
    "                        df_t, dim_reduc_model = apply_model(model_str, user_df, num_dims, hp)\n",
    "        elif apply.upper() == 'BY GESTURE':\n",
    "            for file_type in file_types:\n",
    "                for gesture in gestures:\n",
    "                    gesture_df = sel_df[(data_df['Gesture_ID'] == gesture)]\n",
    "                    df_t, dim_reduc_model = apply_model(model_str, gesture_df, num_dims, hp)\n",
    "\n",
    "    #return df_t, dim_reduc_model\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7612b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dim_reduc(model_str, data_df, num_dims, hp=None, modality=['EMG and IMU'], participant_inclusion=['All Participants']):\n",
    "    '''\n",
    "    model_str: what kind of model to use (eg PCA, T-SNE, ...)\n",
    "    data_df: df containing all the (training) data\n",
    "    num_dims: how many dimensions/components should be used [HYPERPARAM!]\n",
    "    hp: [hyperparams] use this to store \n",
    "    modality: ['EMG', 'IMU', 'EMG and IMU']\n",
    "    '''\n",
    "    \n",
    "    pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dim_reduc_model.explained_variance_)\n",
    "print(dim_reduc_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf20cf",
   "metadata": {},
   "source": [
    "## Completely unfunctionalized, just applying on X (data_df minus metadata) directly\n",
    "\n",
    "## Evaluation\n",
    "> Eval of dim reduc... probably not that important, just figure out roughly what corresponds to at least 80% of explained variance, and then go straight to clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1009f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dim_reduc():\n",
    "    # AIC? ...\n",
    "    # For PCA can used explained variance\n",
    "    ## Probably easier to not do this as a function, unless I use the same criteria for each method...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.drop(columns=['Participant', 'Gesture_ID', 'Gesture_Num'])\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3b79f",
   "metadata": {},
   "source": [
    "# NEED TO MEAN SUBTRACT THE EMG!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc29a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c014b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8adb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:,-16:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33780caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mean subtraction to both the IMU and EMG data \n",
    "\n",
    "def mean_subtraction_blockwise(block):\n",
    "    # Don't hardcode in 72... but ig it's not changing for this dataset...\n",
    "    imu_block = block[:, :72]  # Extract IMU block\n",
    "    emg_block = block[:, 72:]  # Extract EMG block\n",
    "    \n",
    "    # Perform mean subtraction separately for IMU and EMG blocks\n",
    "    imu_block_mean_subtracted = imu_block - imu_block.mean(axis=0)\n",
    "    emg_block_mean_subtracted = emg_block - emg_block.mean(axis=0)\n",
    "    \n",
    "    # Concatenate the mean subtracted IMU and EMG blocks\n",
    "    mean_subtracted_block = np.concatenate((imu_block_mean_subtracted, emg_block_mean_subtracted), axis=1)\n",
    "    \n",
    "    return mean_subtracted_block\n",
    "\n",
    "# Convert DataFrame to NumPy array for faster computation\n",
    "data_array = grouped_df.to_numpy()\n",
    "\n",
    "# Apply mean subtraction blockwise using NumPy vectorized operations\n",
    "mean_subtracted_array = np.apply_along_axis(mean_subtraction_blockwise, axis=1, arr=data_array)\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "mean_subtracted_df = pd.DataFrame(mean_subtracted_array, columns=grouped_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2f66d",
   "metadata": {},
   "source": [
    "Update the code below to use mean_subtracted_df instead of X..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866b953",
   "metadata": {},
   "source": [
    "Once these work, combine all of these into the same for loop (eg so that you only apply PCA for all the comps once, and eval all the (working) metrics during that for loop as opposed to redoing PCA every singl time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of components\n",
    "max_comps = X.shape[1]\n",
    "\n",
    "# Store explained variances for each number of components\n",
    "explained_variances = []\n",
    "\n",
    "# Create a combined range: 1-20, and then every 5th value after 20\n",
    "components_range = list(range(1, 21)) + list(range(25, max_comps + 1, 5))\n",
    "# Ensure max_comps is included if not already in the list\n",
    "if max_comps not in components_range:\n",
    "    components_range.append(max_comps)\n",
    "\n",
    "for n in components_range:\n",
    "    #print(f\"{n}/{max_comps}\")\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X)\n",
    "    explained_variances.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.plot(components_range, explained_variances, marker='o')\n",
    "plt.axhline(y=0.95, linestyle=\"--\", color='r')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b0cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Reconstruction Error and RMSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reconstruction_errors = []\n",
    "\n",
    "for n in components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "    rmse = np.sqrt(mean_squared_error(X, X_reconstructed))\n",
    "    reconstruction_errors.append(rmse)\n",
    "\n",
    "# Plot the reconstruction error\n",
    "plt.plot(components_range, reconstruction_errors, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Reconstruction RMSE')\n",
    "plt.title('Reconstruction Error vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2597ae",
   "metadata": {},
   "source": [
    "I think this should work? I think I just had to convert from a dataframe to a npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Information-Theoretic Measures: Mutual Information and Entropy\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Mutual Information\n",
    "mutual_info_scores = []\n",
    "\n",
    "X_npy = X.to_numpy()\n",
    "\n",
    "for n in components_range:\n",
    "    print(f\"{n}/{max_comps}\")\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X_npy)\n",
    "    \n",
    "    #mutual_info = np.mean([mutual_info_regression(X[:, i].reshape(-1, 1), X_reduced[:, j])[0] for i in range(X.shape[1]) for j in range(n)])\n",
    "    # Fixed iloc version?:\n",
    "    #mutual_info = np.mean([mutual_info_regression(X.iloc[:, i].reshape(-1, 1), X_reduced[:, j])[0] for i in range(X.shape[1]) for j in range(n)])\n",
    "    #mutual_info_scores.append(mutual_info)\n",
    "    total_mutual_info = 0\n",
    "    count = 0\n",
    "    for i in range(X_npy.shape[1]):  # Loop over each original feature\n",
    "        for j in range(X_reduced.shape[1]):  # Loop over each reduced feature\n",
    "            # Calculate mutual information between original feature i and reduced feature j\n",
    "            mutual_info = mutual_info_regression(X_npy[:, i].reshape(-1, 1), X_reduced[:, j])\n",
    "            total_mutual_info += mutual_info[0]\n",
    "            count += 1\n",
    "    # Calculate the average mutual information\n",
    "    average_mutual_info = total_mutual_info / count\n",
    "    mutual_info_scores.append(average_mutual_info)\n",
    "\n",
    "# Plot the mutual information scores\n",
    "plt.plot(components_range, mutual_info_scores, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Average Mutual Information')\n",
    "plt.title('Mutual Information vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c794cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada565a",
   "metadata": {},
   "source": [
    "I don't wanna run this one yet since it is doing a train test split... I'll manually do this later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db580c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Clustering and Classification Performance\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Clustering: Silhouette Score\n",
    "silhouette_scores = []\n",
    "\n",
    "for n in components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    # So it just totally randomly chose 3 here lol\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.plot(components_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Classification: Accuracy and F1 Score\n",
    "classification_accuracies = []\n",
    "classification_f1_scores = []\n",
    "\n",
    "# Assuming `y` is your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# This actually did a train/test split... I should do this manually...\n",
    "for n in components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "    X_test_reduced = pca.transform(X_test)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train_reduced, y_train)\n",
    "    y_pred = clf.predict(X_test_reduced)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    classification_accuracies.append(accuracy)\n",
    "    classification_f1_scores.append(f1)\n",
    "\n",
    "# Plot the classification accuracies\n",
    "plt.plot(components_range, classification_accuracies, marker='o', label='Accuracy')\n",
    "plt.plot(components_range, classification_f1_scores, marker='o', label='F1 Score')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Performance vs Number of Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topological Measures: Neighborhood Preservation\n",
    "# This one also uses an arbitrary number of clusters...\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def knn_preservation_score(X_orig, X_reduced, k=5):\n",
    "    knn_orig = NearestNeighbors(n_neighbors=k).fit(X_orig)\n",
    "    knn_reduced = NearestNeighbors(n_neighbors=k).fit(X_reduced)\n",
    "    neighbors_orig = knn_orig.kneighbors(X_orig, return_distance=False)\n",
    "    neighbors_reduced = knn_reduced.kneighbors(X_reduced, return_distance=False)\n",
    "    preservation = np.mean([len(set(neighbors_orig[i]).intersection(set(neighbors_reduced[i]))) / k for i in range(X_orig.shape[0])])\n",
    "    return preservation\n",
    "\n",
    "preservation_scores = []\n",
    "\n",
    "for n in components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    score = knn_preservation_score(X, X_reduced)\n",
    "    preservation_scores.append(score)\n",
    "\n",
    "# Plot the neighborhood preservation scores\n",
    "plt.plot(components_range, preservation_scores, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Neighborhood Preservation Score')\n",
    "plt.title('Neighborhood Preservation vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7dae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Distance Metrics: Pairwise Distances\n",
    "# This distance can only be used with matrices of the same size... so this doesn't work...\n",
    "\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "procrustes_distances = []\n",
    "\n",
    "for n in components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    _, _, distance = procrustes(X, X_reduced)\n",
    "    procrustes_distances.append(distance)\n",
    "\n",
    "# Plot the Procrustes distances\n",
    "plt.plot(components_range, procrustes_distances, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Procrustes Distance')\n",
    "plt.title('Procrustes Distance vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8429b5",
   "metadata": {},
   "source": [
    "What is y... I don't have class labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c566ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Cross-Validation Techniques\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "for n in components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    scores = cross_val_score(clf, X_reduced, y, cv=5, scoring='accuracy')\n",
    "    cv_accuracies.append(np.mean(scores))\n",
    "\n",
    "# Plot the cross-validation accuracies\n",
    "plt.plot(components_range, cv_accuracies, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Cross-Validation Accuracy vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4411a",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f91892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0fc63be",
   "metadata": {},
   "source": [
    "## Choosing Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4231905",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(1==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de44fc",
   "metadata": {},
   "source": [
    "> Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da15f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# determining the maximum number of clusters \n",
    "# using the simple method\n",
    "limit = int((dataset_new.shape[0]//2)**0.5)\n",
    " \n",
    "# wcss - within cluster sum of squared distances\n",
    "wcss = {}\n",
    " \n",
    "for k in range(2,limit+1):\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(dataset_new)\n",
    "    wcss[k] = model.inertia_\n",
    "     \n",
    "# plotting the wcss values to find the elbow value\n",
    "plt.plot(wcss.keys(), wcss.values(), 'gs-')\n",
    "plt.xlabel('Values of \"k\"')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# determining the maximum number of clusters using the simple method\n",
    "limit = int((dataset_new.shape[0]//2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefab753",
   "metadata": {},
   "source": [
    "> Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "for k in range(2, limit+1):\n",
    "\tmodel = KMeans(n_clusters=k)\n",
    "\tmodel.fit(dataset_new)\n",
    "\tpred = model.predict(dataset_new)\n",
    "\tscore = silhouette_score(dataset_new, pred)\n",
    "\tprint('Silhouette Score for k = {}: {:<.3f}'.format(k, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184fc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d2a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce2abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
