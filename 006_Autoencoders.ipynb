{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ef1e63-343c-4d55-ad97-cf3cabb3fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347da32c-36cc-49e0-80cd-e5b456561a30",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533b39ce-387c-4e40-b5bd-58dd97d233f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pID 101 because it doesn't exist\n",
    "# remove pID 131 because it  doesnt have enough user defined gestures\n",
    "# each participant has 100 experimenter defined files and 50 user defined files\n",
    "# 10 experimenter defined gestures and 5 user defined gestures\n",
    "\n",
    "file_types = [\"IMU_extract\", \"movavg_files\"]\n",
    "expt_types = [\"experimenter-defined\"]\n",
    "\n",
    "#remove participant 131 because they are missing gestures \n",
    "pIDs_impaired = ['P102','P103','P104','P105','P106','P107','P108','P109','P110','P111',\n",
    "       'P112','P114','P115','P116','P118','P119','P121','P122','P123','P124','P125',\n",
    "       'P126','P127','P128', 'P132']\n",
    "# remove participants P001 and P003 because they dont have duplicate or open gestures\n",
    "pIDs_unimpaired = ['P004','P005','P006','P008','P010','P011']\n",
    "\n",
    "pIDs_both = pIDs_impaired + pIDs_unimpaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647a91a5-4d27-485f-8ed0-db92218ccb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "# Kai's laptop\n",
    "data_path = \"C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\PCA_40D\\\\\"\n",
    "# BRC Desktop\n",
    "#data_path = \"D:\\\\Kai_MetaGestureClustering_24\\\\saved_datasets\\\\\"\n",
    "\n",
    "print(\"Loading\")\n",
    "\n",
    "metadata_cols = ['Participant', 'Gesture_ID', 'Gesture_Num']\n",
    "\n",
    "PCA_df = pd.read_pickle(data_path+'PCA_ms_IMUEMG_df.pkl')\n",
    "metadata_cols_df = pd.read_pickle('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\metadata_cols_df.pkl')\n",
    "\n",
    "# Dropping the metadata when we read it in!\n",
    "training_u_df = pd.read_pickle(data_path+'training_u_df.pkl').drop(metadata_cols, axis=1)\n",
    "test_users_df = pd.read_pickle(data_path+'test_users_df.pkl').drop(metadata_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0430ab-dd10-48ee-b823-b5a91d36b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327168, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027903</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>-0.019699</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>-0.031254</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>0.066484</td>\n",
       "      <td>0.108729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019453</td>\n",
       "      <td>0.062983</td>\n",
       "      <td>-0.025869</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.037645</td>\n",
       "      <td>-0.186270</td>\n",
       "      <td>-0.046251</td>\n",
       "      <td>-0.104630</td>\n",
       "      <td>-0.002939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038982</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>-0.007901</td>\n",
       "      <td>-0.027368</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>0.035053</td>\n",
       "      <td>-0.056843</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>-0.022542</td>\n",
       "      <td>-0.022563</td>\n",
       "      <td>-0.160826</td>\n",
       "      <td>-0.048161</td>\n",
       "      <td>-0.073771</td>\n",
       "      <td>0.043268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116782</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>-0.014612</td>\n",
       "      <td>-0.093325</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>-0.013155</td>\n",
       "      <td>-0.046150</td>\n",
       "      <td>0.036385</td>\n",
       "      <td>0.052746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014298</td>\n",
       "      <td>0.072109</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.018695</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>-0.041831</td>\n",
       "      <td>-0.109653</td>\n",
       "      <td>0.027043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.030245</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>-0.048905</td>\n",
       "      <td>-0.029129</td>\n",
       "      <td>0.090026</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>-0.064307</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>0.053055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010992</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>-0.097073</td>\n",
       "      <td>-0.056870</td>\n",
       "      <td>-0.001038</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>-0.165858</td>\n",
       "      <td>-0.049424</td>\n",
       "      <td>-0.108671</td>\n",
       "      <td>0.069886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.112950</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>-0.063254</td>\n",
       "      <td>-0.108892</td>\n",
       "      <td>0.198729</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>-0.124893</td>\n",
       "      <td>0.114817</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>-0.093678</td>\n",
       "      <td>-0.131263</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.157963</td>\n",
       "      <td>-0.041911</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>0.063311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \n",
       "0 -0.027903  0.001411 -0.019509  0.013428 -0.019699  0.027333 -0.031254  \\\n",
       "1 -0.038982  0.006470 -0.000111  0.010904 -0.015323  0.031336 -0.007901   \n",
       "2 -0.116782  0.003824  0.011550 -0.014612 -0.093325  0.081718 -0.013155   \n",
       "3 -0.030245 -0.017409  0.022540 -0.048905 -0.029129  0.090026 -0.024645   \n",
       "4 -0.112950  0.026262  0.004837 -0.063254 -0.108892  0.198729 -0.010583   \n",
       "\n",
       "         7         8         9   ...        30        31        32        33   \n",
       "0 -0.022910  0.066484  0.108729  ... -0.019453  0.062983 -0.025869  0.014303  \\\n",
       "1 -0.027368  0.060370  0.074712  ...  0.041438  0.035053 -0.056843 -0.008895   \n",
       "2 -0.046150  0.036385  0.052746  ... -0.014298  0.072109 -0.026536 -0.034365   \n",
       "3 -0.064307  0.074589  0.053055  ... -0.010992  0.059990 -0.097073 -0.056870   \n",
       "4 -0.124893  0.114817  0.038628  ...  0.035735  0.050880 -0.093678 -0.131263   \n",
       "\n",
       "         34        35        36        37        38        39  \n",
       "0 -0.013387 -0.037645 -0.186270 -0.046251 -0.104630 -0.002939  \n",
       "1 -0.022542 -0.022563 -0.160826 -0.048161 -0.073771  0.043268  \n",
       "2  0.018695 -0.011940 -0.160580 -0.041831 -0.109653  0.027043  \n",
       "3 -0.001038 -0.008015 -0.165858 -0.049424 -0.108671  0.069886  \n",
       "4  0.018035  0.056185 -0.157963 -0.041911 -0.145308  0.063311  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_u_df.shape)\n",
    "training_u_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360eff5-02ba-4e94-aaa0-8b8ff6a195f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    # NOTE: I think this formulation makes it so the dataloader won't return (X, Y) as per usual (eg like TensorDataset)\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc04167-5ab8-4ac3-ae19-80b8e1988242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE TRAINING SET\n",
    "\n",
    "num_rows_per_gesture = 64 # From the interp\n",
    "num_gestures = len(training_u_df) // num_rows_per_gesture\n",
    "num_features = training_u_df.shape[1]\n",
    "\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(training_u_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns)\n",
    "X_3D_PCA40 = training_u_df.to_numpy().reshape(num_gestures, num_rows_per_gesture, num_features)\n",
    "#flattened_PCA = PCA_np.reshape(num_gestures, -1)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_3DTensor_PCA40 = torch.tensor(X_3D_PCA40, dtype=torch.float32)\n",
    "\n",
    "# Dummy dataset\n",
    "#data = torch.randn(num_gestures, timesteps, num_features)\n",
    "#dataset = TensorDataset(data)\n",
    "#data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the dataset\n",
    "u_training_dataset = GestureDataset(X_3DTensor_PCA40)\n",
    "\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "data_loader = DataLoader(u_training_dataset, batch_size=batch_size, shuffle=True) # Should shuffle be False? \n",
    "## It's shuffling the gesture order I think so that should be fine...\n",
    "\n",
    "# CREATE THE TEST SET\n",
    "\n",
    "num_test_gestures = len(test_users_df) // num_rows_per_gesture\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(test_users_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns) and convert to torch tensor\n",
    "## Theres probably an easier way to just create it as a torch tensor lol\n",
    "Xtest_3DTensor_PCA40 = torch.tensor(test_users_df.to_numpy().reshape(num_gestures, num_rows_per_gesture, num_features), dtype=torch.float32)\n",
    "\n",
    "# Create the dataset\n",
    "u_testing_dataset = GestureDataset(Xtest_3DTensor_PCA40)\n",
    "test_loader = DataLoader(u_testing_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b852-4054-443f-a4e9-bdb280c5cc84",
   "metadata": {},
   "source": [
    "# RNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b312cb-ba68-42d2-baee-5e0da666fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        return hidden[-1]\n",
    "\n",
    "\n",
    "class RNNDecoder_Original(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(hidden_dim, output_dim, num_layers, batch_first=True)\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNNAutoencoder_Original(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.encoder = RNNEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = RNNDecoder(hidden_dim, input_dim, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded, seq_len)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9457e642-d94d-43a7-9771-5d578dd18bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(hidden_dim, output_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "class RNNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, seq_len):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.encoder = RNNEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = RNNDecoder(hidden_dim, input_dim, num_layers)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent, self.seq_len)\n",
    "        return reconstructed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a37fa199-a3f4-4b72-9484-3481f9bf79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Epoch [1/10], Loss: 0.2247\n",
      "Epoch [2/10], Loss: 0.2042\n",
      "Epoch [3/10], Loss: 0.1950\n",
      "Epoch [4/10], Loss: 0.1897\n",
      "Epoch [5/10], Loss: 0.1858\n",
      "Epoch [6/10], Loss: 0.1828\n",
      "Epoch [7/10], Loss: 0.1803\n",
      "Epoch [8/10], Loss: 0.1785\n",
      "Epoch [9/10], Loss: 0.1760\n",
      "Epoch [10/10], Loss: 0.1743\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Hyperparameters and dataset setup\n",
    "timesteps = 64\n",
    "num_features = 40\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = RNNAutoencoder(num_features, hidden_dim, num_layers, seq_len=timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in data_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch[0]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_train_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be893f7-baf6-48ca-b3ea-b936a8dfd6c1",
   "metadata": {},
   "source": [
    "> Need to know judge whether or not the AE is any good. The loss by itself doesn't tell if the AE is \"good enough\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146c694-1227-45fa-8e28-6e33ac1d7f9d",
   "metadata": {},
   "source": [
    "Visual Inspection of Reconstructed VS Original Signals\n",
    "- This code hasn't been refactored to work with our example yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11497c-5ab1-45f2-9821-9947fa9c82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to display original and reconstructed images\n",
    "def display_reconstructions(model, data, num_images=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_data = data[:num_images]\n",
    "        reconstructions = model(sample_data)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n",
    "        for i in range(num_images):\n",
    "            axes[0, i].imshow(sample_data[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(reconstructions[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "            axes[1, i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Display some original and reconstructed images\n",
    "display_reconstructions(model, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad81688-4c15-4ddd-aa61-80ff5d935d91",
   "metadata": {},
   "source": [
    "Latent Space Visualization\n",
    "- This code hasn't been refactored to work with our example yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4d5d6-5b60-47c8-bff3-b1ac2a4944b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get latent representations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    latent_representations = []\n",
    "    labels = []\n",
    "    for batch in test_loader:\n",
    "        batch = batch[0]\n",
    "        latent = model.encoder(batch)\n",
    "        latent_representations.append(latent.cpu().numpy())\n",
    "        # Assuming labels are available and appended similarly\n",
    "        labels.append(batch_labels.cpu().numpy())\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_latent = tsne.fit_transform(latent_representations)\n",
    "\n",
    "# Plot the reduced latent space\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(reduced_latent[:, 0], reduced_latent[:, 1], c=labels, cmap='viridis', s=2)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b20a2-eb0a-492c-883f-e2603f295521",
   "metadata": {},
   "source": [
    "# Temporal Convolution Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85022701-bada-40a8-99d2-21938d17978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, kernel_size):\n",
    "        super(TCNDecoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(hidden_dim, output_dim, kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNAutoencoder, self).__init__()\n",
    "        self.encoder = TCNEncoder(input_dim, hidden_dim, kernel_size)\n",
    "        self.decoder = TCNDecoder(hidden_dim, input_dim, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09094d09-9f33-4f45-b28e-31795d511dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim = 64\n",
    "kernel_size = 5\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = TCNAutoencoder(input_dim, hidden_dim, kernel_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc824e7-03e0-4eae-b880-6be6118a56fb",
   "metadata": {},
   "source": [
    "# Varitational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b356ebb-4b72-46d1-a1a0-8fd21c163c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim*2)  # mean and logvar\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138cb2d-7d58-4f83-8dcd-0074f0544c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = data.size(1) * data.size(2)\n",
    "hidden_dim = 128\n",
    "latent_dim = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss = model.loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68375495-db2c-4b19-83af-8d69eaa5b701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
