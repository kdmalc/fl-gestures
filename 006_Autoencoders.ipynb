{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ef1e63-343c-4d55-ad97-cf3cabb3fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347da32c-36cc-49e0-80cd-e5b456561a30",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533b39ce-387c-4e40-b5bd-58dd97d233f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pID 101 because it doesn't exist\n",
    "# remove pID 131 because it  doesnt have enough user defined gestures\n",
    "# each participant has 100 experimenter defined files and 50 user defined files\n",
    "# 10 experimenter defined gestures and 5 user defined gestures\n",
    "\n",
    "file_types = [\"IMU_extract\", \"movavg_files\"]\n",
    "expt_types = [\"experimenter-defined\"]\n",
    "\n",
    "#remove participant 131 because they are missing gestures \n",
    "pIDs_impaired = ['P102','P103','P104','P105','P106','P107','P108','P109','P110','P111',\n",
    "       'P112','P114','P115','P116','P118','P119','P121','P122','P123','P124','P125',\n",
    "       'P126','P127','P128', 'P132']\n",
    "# remove participants P001 and P003 because they dont have duplicate or open gestures\n",
    "pIDs_unimpaired = ['P004','P005','P006','P008','P010','P011']\n",
    "\n",
    "pIDs_both = pIDs_impaired + pIDs_unimpaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647a91a5-4d27-485f-8ed0-db92218ccb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "# Kai's laptop\n",
    "data_path = \"C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\PCA_40D\\\\\"\n",
    "# BRC Desktop\n",
    "#data_path = \"D:\\\\Kai_MetaGestureClustering_24\\\\saved_datasets\\\\\"\n",
    "\n",
    "print(\"Loading\")\n",
    "\n",
    "metadata_cols = ['Participant', 'Gesture_ID', 'Gesture_Num']\n",
    "\n",
    "PCA_df = pd.read_pickle(data_path+'PCA_ms_IMUEMG_df.pkl')\n",
    "metadata_cols_df = pd.read_pickle('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\metadata_cols_df.pkl')\n",
    "\n",
    "# Dropping the metadata when we read it in!\n",
    "training_u_df = pd.read_pickle(data_path+'training_u_df.pkl').drop(metadata_cols, axis=1)\n",
    "test_users_df = pd.read_pickle(data_path+'test_users_df.pkl').drop(metadata_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0430ab-dd10-48ee-b823-b5a91d36b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327168, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027903</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>-0.019699</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>-0.031254</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>0.066484</td>\n",
       "      <td>0.108729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019453</td>\n",
       "      <td>0.062983</td>\n",
       "      <td>-0.025869</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.037645</td>\n",
       "      <td>-0.186270</td>\n",
       "      <td>-0.046251</td>\n",
       "      <td>-0.104630</td>\n",
       "      <td>-0.002939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038982</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>-0.007901</td>\n",
       "      <td>-0.027368</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>0.035053</td>\n",
       "      <td>-0.056843</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>-0.022542</td>\n",
       "      <td>-0.022563</td>\n",
       "      <td>-0.160826</td>\n",
       "      <td>-0.048161</td>\n",
       "      <td>-0.073771</td>\n",
       "      <td>0.043268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116782</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>-0.014612</td>\n",
       "      <td>-0.093325</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>-0.013155</td>\n",
       "      <td>-0.046150</td>\n",
       "      <td>0.036385</td>\n",
       "      <td>0.052746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014298</td>\n",
       "      <td>0.072109</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.018695</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>-0.041831</td>\n",
       "      <td>-0.109653</td>\n",
       "      <td>0.027043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.030245</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>-0.048905</td>\n",
       "      <td>-0.029129</td>\n",
       "      <td>0.090026</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>-0.064307</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>0.053055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010992</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>-0.097073</td>\n",
       "      <td>-0.056870</td>\n",
       "      <td>-0.001038</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>-0.165858</td>\n",
       "      <td>-0.049424</td>\n",
       "      <td>-0.108671</td>\n",
       "      <td>0.069886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.112950</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>-0.063254</td>\n",
       "      <td>-0.108892</td>\n",
       "      <td>0.198729</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>-0.124893</td>\n",
       "      <td>0.114817</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>-0.093678</td>\n",
       "      <td>-0.131263</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.157963</td>\n",
       "      <td>-0.041911</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>0.063311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \n",
       "0 -0.027903  0.001411 -0.019509  0.013428 -0.019699  0.027333 -0.031254  \\\n",
       "1 -0.038982  0.006470 -0.000111  0.010904 -0.015323  0.031336 -0.007901   \n",
       "2 -0.116782  0.003824  0.011550 -0.014612 -0.093325  0.081718 -0.013155   \n",
       "3 -0.030245 -0.017409  0.022540 -0.048905 -0.029129  0.090026 -0.024645   \n",
       "4 -0.112950  0.026262  0.004837 -0.063254 -0.108892  0.198729 -0.010583   \n",
       "\n",
       "         7         8         9   ...        30        31        32        33   \n",
       "0 -0.022910  0.066484  0.108729  ... -0.019453  0.062983 -0.025869  0.014303  \\\n",
       "1 -0.027368  0.060370  0.074712  ...  0.041438  0.035053 -0.056843 -0.008895   \n",
       "2 -0.046150  0.036385  0.052746  ... -0.014298  0.072109 -0.026536 -0.034365   \n",
       "3 -0.064307  0.074589  0.053055  ... -0.010992  0.059990 -0.097073 -0.056870   \n",
       "4 -0.124893  0.114817  0.038628  ...  0.035735  0.050880 -0.093678 -0.131263   \n",
       "\n",
       "         34        35        36        37        38        39  \n",
       "0 -0.013387 -0.037645 -0.186270 -0.046251 -0.104630 -0.002939  \n",
       "1 -0.022542 -0.022563 -0.160826 -0.048161 -0.073771  0.043268  \n",
       "2  0.018695 -0.011940 -0.160580 -0.041831 -0.109653  0.027043  \n",
       "3 -0.001038 -0.008015 -0.165858 -0.049424 -0.108671  0.069886  \n",
       "4  0.018035  0.056185 -0.157963 -0.041911 -0.145308  0.063311  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_u_df.shape)\n",
    "training_u_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cab3ccc-82b7-4db1-8ee8-706461cc31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_per_gesture = 64 # From the interp\n",
    "num_gestures = len(training_u_df) // num_rows_per_gesture\n",
    "num_features = training_u_df.shape[1]\n",
    "\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(training_u_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "data_np = training_u_df.to_numpy()\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns)\n",
    "X_3D_PCA40 = data_np.reshape(num_gestures, num_rows_per_gesture, num_features)\n",
    "#flattened_PCA = PCA_np.reshape(num_gestures, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6640d67d-fe1b-43e3-8afe-9af1a0035867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor\n",
    "X_3DTensor_PCA40 = torch.tensor(X_3D_PCA40, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eaec717-afd4-4292-8e6b-eaec1c3b0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1627b00-98d0-4f47-927e-5c711c78240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 40])\n",
      "\n",
      "NEW BATCH SHAPE! INCONSISTENT BATCH SIZES! At batch_counter 160\n",
      "torch.Size([24, 64, 40])\n",
      "\n",
      "Completed. Final batch_counter: 160\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "u_training_dataset = GestureDataset(X_3DTensor_PCA40)\n",
    "\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "data_loader = DataLoader(u_training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "batch_counter = 0\n",
    "consistent_batch_shape = 0\n",
    "for batch in data_loader:\n",
    "    batch_counter += 1\n",
    "    if batch_counter == 1:\n",
    "        # Should print (batch_size, 64, 40)\n",
    "        print(batch.shape)\n",
    "        print()\n",
    "        consistent_batch_shape = batch.shape\n",
    "    elif batch.shape != consistent_batch_shape:\n",
    "        print(f\"NEW BATCH SHAPE! INCONSISTENT BATCH SIZES! At batch_counter {batch_counter}\")\n",
    "        print(batch.shape)\n",
    "        print()\n",
    "        \n",
    "print(f\"Completed. Final batch_counter: {batch_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b852-4054-443f-a4e9-bdb280c5cc84",
   "metadata": {},
   "source": [
    "# RNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9457e642-d94d-43a7-9771-5d578dd18bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        return hidden[-1]\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.rnn = nn.LSTM(hidden_dim, output_dim, num_layers, batch_first=True)\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "class RNNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.encoder = RNNEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = RNNDecoder(hidden_dim, input_dim, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded, seq_len)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b04eea-0d32-46d2-8a7e-9ea6ad8fb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 32, got 40",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch)\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mRNNAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded, seq_len)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mRNNEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 7\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:692\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    691\u001b[0m                        ):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    694\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 32, got 40"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# Using num_gestures as the batch_dim and the input_dim...\n",
    "#input_dim = X_3D_PCA40.shape[0]  # --> 5112\n",
    "input_dim = batch_size  # --> 32\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = RNNAutoencoder(input_dim, hidden_dim, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "epoch_losses = []  # To store average loss per epoch\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in data_loader:\n",
    "        #batch = batch[0]  # DataLoader returns a tuple, get the tensor\n",
    "        print(batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    epoch_losses.append(average_epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_epoch_loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb125f05-b07d-4da6-8f41-163209a5bf5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m num_gestures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     23\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_gestures, timesteps, num_features)\n\u001b[1;32m---> 24\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m(data)\n\u001b[0;32m     25\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize the model, criterion, and optimizer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
     ]
    }
   ],
   "source": [
    "class RNNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, seq_len):\n",
    "        super(RNNAutoencoder, self).__init__()\n",
    "        self.encoder = RNNEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = RNNDecoder(hidden_dim, input_dim, num_layers)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent, self.seq_len)\n",
    "        return reconstructed\n",
    "\n",
    "# Hyperparameters and dataset setup\n",
    "timesteps = 50\n",
    "num_features = 10\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# Dummy dataset\n",
    "num_gestures = 1000\n",
    "data = torch.randn(num_gestures, timesteps, num_features)\n",
    "dataset = TensorDataset(data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = RNNAutoencoder(num_features, hidden_dim, latent_dim=16, num_layers=num_layers, seq_len=timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epoch_losses = []  # To store average loss per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in data_loader:\n",
    "        batch = batch[0]  # DataLoader returns a tuple, get the tensor\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    epoch_losses.append(average_epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_epoch_loss:.4f}')\n",
    "\n",
    "# Optionally, save the epoch_losses for later analysis\n",
    "# For example, save to a file or use for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a29247-afce-48f1-80f5-43be5b66be45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422d0b9-7070-485f-9b69-93bb9e337aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fa199-a3f4-4b72-9484-3481f9bf79da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd871aa2-0d7a-4eed-ba63-7e21e0de6eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e28e5d-606b-409b-be1a-a655f6fb1e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "330b20a2-eb0a-492c-883f-e2603f295521",
   "metadata": {},
   "source": [
    "# Temporal Convolution Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85022701-bada-40a8-99d2-21938d17978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, kernel_size):\n",
    "        super(TCNDecoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(hidden_dim, output_dim, kernel_size, padding=kernel_size//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNAutoencoder, self).__init__()\n",
    "        self.encoder = TCNEncoder(input_dim, hidden_dim, kernel_size)\n",
    "        self.decoder = TCNDecoder(hidden_dim, input_dim, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09094d09-9f33-4f45-b28e-31795d511dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim = 64\n",
    "kernel_size = 5\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = TCNAutoencoder(input_dim, hidden_dim, kernel_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc824e7-03e0-4eae-b880-6be6118a56fb",
   "metadata": {},
   "source": [
    "# Varitational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b356ebb-4b72-46d1-a1a0-8fd21c163c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim*2)  # mean and logvar\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138cb2d-7d58-4f83-8dcd-0074f0544c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = data.size(1) * data.size(2)\n",
    "hidden_dim = 128\n",
    "latent_dim = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss = model.loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68375495-db2c-4b19-83af-8d69eaa5b701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
