{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0fef36",
   "metadata": {},
   "source": [
    "> __Purpose:__ This NB tests a CNN in the agglomerative model clustering procedure. With later finetuning. Uses the previously developed PyTorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "from moments_engr import *\n",
    "from agglo_model_clust import *\n",
    "from DNN_FT_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b80c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204800, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>EMG1</th>\n",
       "      <th>EMG2</th>\n",
       "      <th>EMG3</th>\n",
       "      <th>EMG4</th>\n",
       "      <th>EMG5</th>\n",
       "      <th>EMG6</th>\n",
       "      <th>EMG7</th>\n",
       "      <th>EMG8</th>\n",
       "      <th>EMG9</th>\n",
       "      <th>EMG10</th>\n",
       "      <th>EMG11</th>\n",
       "      <th>EMG12</th>\n",
       "      <th>EMG13</th>\n",
       "      <th>EMG14</th>\n",
       "      <th>EMG15</th>\n",
       "      <th>EMG16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.362743</td>\n",
       "      <td>-0.801651</td>\n",
       "      <td>-0.383077</td>\n",
       "      <td>-0.195299</td>\n",
       "      <td>-0.203047</td>\n",
       "      <td>-0.464472</td>\n",
       "      <td>-0.276292</td>\n",
       "      <td>-0.026736</td>\n",
       "      <td>-0.873870</td>\n",
       "      <td>-1.036152</td>\n",
       "      <td>-0.580930</td>\n",
       "      <td>-0.719494</td>\n",
       "      <td>-0.502255</td>\n",
       "      <td>-1.750091</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.094192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.351553</td>\n",
       "      <td>-0.775334</td>\n",
       "      <td>-0.382545</td>\n",
       "      <td>-0.154773</td>\n",
       "      <td>-0.131977</td>\n",
       "      <td>-0.295204</td>\n",
       "      <td>-0.125822</td>\n",
       "      <td>0.089679</td>\n",
       "      <td>-0.816215</td>\n",
       "      <td>-2.082635</td>\n",
       "      <td>-0.006283</td>\n",
       "      <td>-0.139439</td>\n",
       "      <td>-0.367764</td>\n",
       "      <td>-0.208084</td>\n",
       "      <td>-0.111811</td>\n",
       "      <td>-0.039009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.380825</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.398388</td>\n",
       "      <td>-0.085411</td>\n",
       "      <td>0.017528</td>\n",
       "      <td>-0.205675</td>\n",
       "      <td>-0.068451</td>\n",
       "      <td>0.117076</td>\n",
       "      <td>-0.668221</td>\n",
       "      <td>-3.403064</td>\n",
       "      <td>-0.526030</td>\n",
       "      <td>-0.478294</td>\n",
       "      <td>-0.300443</td>\n",
       "      <td>0.203266</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.004728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.366795</td>\n",
       "      <td>-0.765464</td>\n",
       "      <td>-0.374423</td>\n",
       "      <td>-0.073225</td>\n",
       "      <td>0.183172</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>-0.058907</td>\n",
       "      <td>0.080977</td>\n",
       "      <td>-0.424416</td>\n",
       "      <td>-3.709413</td>\n",
       "      <td>-0.570894</td>\n",
       "      <td>-0.775155</td>\n",
       "      <td>-0.144710</td>\n",
       "      <td>-0.619539</td>\n",
       "      <td>0.146499</td>\n",
       "      <td>0.199975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.245578</td>\n",
       "      <td>-0.761283</td>\n",
       "      <td>-0.303976</td>\n",
       "      <td>-0.081947</td>\n",
       "      <td>0.224996</td>\n",
       "      <td>0.103319</td>\n",
       "      <td>-0.003929</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>-0.016530</td>\n",
       "      <td>-4.075150</td>\n",
       "      <td>-0.127710</td>\n",
       "      <td>2.682791</td>\n",
       "      <td>-0.141750</td>\n",
       "      <td>-0.208404</td>\n",
       "      <td>-0.035642</td>\n",
       "      <td>0.172662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID Gesture_Num      EMG1      EMG2      EMG3      EMG4   \n",
       "0        P102        pan           1 -0.362743 -0.801651 -0.383077 -0.195299  \\\n",
       "1        P102        pan           1 -0.351553 -0.775334 -0.382545 -0.154773   \n",
       "2        P102        pan           1 -0.380825 -0.762588 -0.398388 -0.085411   \n",
       "3        P102        pan           1 -0.366795 -0.765464 -0.374423 -0.073225   \n",
       "4        P102        pan           1 -0.245578 -0.761283 -0.303976 -0.081947   \n",
       "\n",
       "       EMG5      EMG6      EMG7      EMG8      EMG9     EMG10     EMG11   \n",
       "0 -0.203047 -0.464472 -0.276292 -0.026736 -0.873870 -1.036152 -0.580930  \\\n",
       "1 -0.131977 -0.295204 -0.125822  0.089679 -0.816215 -2.082635 -0.006283   \n",
       "2  0.017528 -0.205675 -0.068451  0.117076 -0.668221 -3.403064 -0.526030   \n",
       "3  0.183172  0.009277 -0.058907  0.080977 -0.424416 -3.709413 -0.570894   \n",
       "4  0.224996  0.103319 -0.003929  0.041526 -0.016530 -4.075150 -0.127710   \n",
       "\n",
       "      EMG12     EMG13     EMG14     EMG15     EMG16  \n",
       "0 -0.719494 -0.502255 -1.750091 -0.127847 -0.094192  \n",
       "1 -0.139439 -0.367764 -0.208084 -0.111811 -0.039009  \n",
       "2 -0.478294 -0.300443  0.203266  0.113300  0.004728  \n",
       "3 -0.775155 -0.144710 -0.619539  0.146499  0.199975  \n",
       "4  2.682791 -0.141750 -0.208404 -0.035642  0.172662  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Meta_Gesture_2024\\\\saved_datasets\\\\filtered_datasets\\\\$BStand_EMG_df.pkl'\n",
    "\n",
    "with open(path1, 'rb') as file:\n",
    "    raw_userdef_data_df = pickle.load(file)  # (204800, 19)\n",
    "\n",
    "print(raw_userdef_data_df.shape)\n",
    "raw_userdef_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0375f2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ade3f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>feature</th>\n",
       "      <th>Gesture_Encoded</th>\n",
       "      <th>Cluster_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>1</td>\n",
       "      <td>[[6.079045311063784], [-7.551458873254243], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>10</td>\n",
       "      <td>[[5.994789910363704], [-7.978871468164499], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>2</td>\n",
       "      <td>[[6.010193380499154], [-7.7063875553339], [-20...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>3</td>\n",
       "      <td>[[5.8212078257286874], [-7.463908156909893], [...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>4</td>\n",
       "      <td>[[5.974675085061773], [-7.945111601415482], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID Gesture_Num   \n",
       "0        P004      close           1  \\\n",
       "1        P004      close          10   \n",
       "2        P004      close           2   \n",
       "3        P004      close           3   \n",
       "4        P004      close           4   \n",
       "\n",
       "                                             feature  Gesture_Encoded   \n",
       "0  [[6.079045311063784], [-7.551458873254243], [-...                0  \\\n",
       "1  [[5.994789910363704], [-7.978871468164499], [-...                0   \n",
       "2  [[6.010193380499154], [-7.7063875553339], [-20...                0   \n",
       "3  [[5.8212078257286874], [-7.463908156909893], [...                0   \n",
       "4  [[5.974675085061773], [-7.945111601415482], [-...                0   \n",
       "\n",
       "   Cluster_ID  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Train a classification model on every single individual user\n",
    "\n",
    "userdef_df = raw_userdef_data_df.groupby(['Participant', 'Gesture_ID', 'Gesture_Num']).apply(create_feature_vectors)\n",
    "#output is df with particpant, gesture_ID, gesture_num and feature (holds 80 len vector)\n",
    "userdef_df = userdef_df.reset_index(drop=True)\n",
    "\n",
    "#convert Gesture_ID to numerical with new Gesture_Encoded column\n",
    "label_encoder = LabelEncoder()\n",
    "userdef_df['Gesture_Encoded'] = label_encoder.fit_transform(userdef_df['Gesture_ID'])\n",
    "\n",
    "label_encoder2 = LabelEncoder()\n",
    "userdef_df['Cluster_ID'] = label_encoder2.fit_transform(userdef_df['Participant'])\n",
    "\n",
    "print(userdef_df.shape)\n",
    "userdef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f2bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants = userdef_df['Participant'].unique()\n",
    "# Shuffle the participants\n",
    "np.random.shuffle(all_participants)\n",
    "# Split into two groups\n",
    "#train_participants = all_participants[:24]  # First 24 participants\n",
    "test_participants = all_participants[24:]  # Remaining 8 participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb072fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_splits = prepare_data(\n",
    "    userdef_df, 'feature', 'Gesture_Encoded', \n",
    "    all_participants, test_participants, \n",
    "    training_trials_per_gesture=8, finetuning_trials_per_gesture=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec9e45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'intra_subject_test', 'novel_trainFT', 'cross_subject_test'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7423158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(data_splits['train']['features'])\n",
    "# Create a new column 'features' that contains all 80 columns as lists\n",
    "features_df['features'] = features_df.apply(lambda row: row.tolist(), axis=1)\n",
    "# Keep only the new combined column\n",
    "features_df = features_df[['features']]\n",
    "# Combine with labels and participant_ids into a single DataFrame\n",
    "train_df = pd.concat([features_df, pd.Series(data_splits['train']['labels'], name='Gesture_Encoded'), pd.Series(data_splits['train']['participant_ids'], name='participant_ids')], axis=1)\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Cluster_ID'] = label_encoder.fit_transform(train_df['participant_ids'])\n",
    "\n",
    "features_df = pd.DataFrame(data_splits['intra_subject_test']['features'])\n",
    "# Create a new column 'features' that contains all 80 columns as lists\n",
    "features_df['features'] = features_df.apply(lambda row: row.tolist(), axis=1)\n",
    "# Keep only the new combined column\n",
    "features_df = features_df[['features']]\n",
    "# Combine with labels and participant_ids into a single DataFrame\n",
    "test_df = pd.concat([features_df, pd.Series(data_splits['intra_subject_test']['labels'], name='Gesture_Encoded'), pd.Series(data_splits['intra_subject_test']['participant_ids'], name='participant_ids')], axis=1)\n",
    "label_encoder = LabelEncoder()\n",
    "test_df['Cluster_ID'] = label_encoder.fit_transform(test_df['participant_ids'])\n",
    "\n",
    "# ENTIRELY WITHHOLDING CROSS CLUSTER DATASET (NOVEL TEST SUBJECTS) FOR NOW. \n",
    "#test_df\n",
    "#features_df = pd.DataFrame(data_splits['train']['features'])\n",
    "## Create a new column 'features' that contains all 80 columns as lists\n",
    "#features_df['features'] = features_df.apply(lambda row: row.tolist(), axis=1)\n",
    "## Keep only the new combined column\n",
    "#features_df = features_df[['features']]\n",
    "## Combine with labels and participant_ids into a single DataFrame\n",
    "#train_df = pd.concat([features_df, pd.Series(data_splits['train']['labels'], name='Gesture_Encoded'), pd.Series(data_splits['train']['participant_ids'], name='participant_ids')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d302fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs_dict = {'train':train_df, 'test':test_df}\n",
    "\n",
    "# Need to update Cluster_ID col at the end of each round, for both dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253ca21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab5959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>Gesture_Encoded</th>\n",
       "      <th>participant_ids</th>\n",
       "      <th>Cluster_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4.0728441780827485, -6.216633410388578, -15.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>P128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4.842766138855771, -7.933026853320965, -18.38...</td>\n",
       "      <td>0</td>\n",
       "      <td>P128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4.343725560839635, -6.20391996380322, -15.560...</td>\n",
       "      <td>0</td>\n",
       "      <td>P128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4.972292120731378, -6.996600730998998, -17.60...</td>\n",
       "      <td>0</td>\n",
       "      <td>P128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4.942130684716379, -6.513267267205743, -16.81...</td>\n",
       "      <td>0</td>\n",
       "      <td>P128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  Gesture_Encoded   \n",
       "0  [4.0728441780827485, -6.216633410388578, -15.0...                0  \\\n",
       "1  [4.842766138855771, -7.933026853320965, -18.38...                0   \n",
       "2  [4.343725560839635, -6.20391996380322, -15.560...                0   \n",
       "3  [4.972292120731378, -6.996600730998998, -17.60...                0   \n",
       "4  [4.942130684716379, -6.513267267205743, -16.81...                0   \n",
       "\n",
       "  participant_ids  Cluster_ID  \n",
       "0            P128          22  \n",
       "1            P128          22  \n",
       "2            P128          22  \n",
       "3            P128          22  \n",
       "4            P128          22  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93294f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22021fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48be294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_and_cv_DNN_cluster_model(train_df, model_type, cluster_ids, \n",
    "                                   cluster_column='Cluster_ID', feature_column='features', \n",
    "                                   target_column='Gesture_Encoded', n_splits=3, bs=32, \n",
    "                                   lr=0.001, criterion=nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for models trained on each cluster in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - userdef_df (DataFrame): The input dataframe with cluster, feature, and target data.\n",
    "    - model (str or sklearn model object): The model to train. If string, it must be one of:\n",
    "      ['LogisticRegression', 'SVC', 'RF', 'GradientBoosting', 'KNN'].\n",
    "    - cluster_ids (list): List of cluster IDs to process.\n",
    "    - cluster_column (str): Column name representing the cluster IDs.\n",
    "    - feature_column (str): Column name containing feature arrays.\n",
    "    - target_column (str): Column name for target labels.\n",
    "    - n_splits (int): Number of cross-validation splits.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_val_accuracy (float): The average validation accuracy across all folds and clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the model object if a string is provided\n",
    "    if isinstance(model_type, str):\n",
    "        # Select model\n",
    "        if model_type == 'CNN':\n",
    "            model = CNNModel(input_dim, num_classes).to('cpu')\n",
    "        elif model_type == 'RNN':\n",
    "            model = RNNModel(input_dim, num_classes).to('cpu')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_type}. Only CNNs and RNNs are supported.\")\n",
    "    else:\n",
    "        # Assuming a model object was passed in\n",
    "        base_model = model_type\n",
    "\n",
    "    total_val_accuracy = 0\n",
    "    num_folds_processed = 0\n",
    "    clus_model_lst = []\n",
    "    for cluster in cluster_ids:\n",
    "        \n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        \n",
    "        # Filter data for the current cluster\n",
    "        cluster_data = train_df[train_df[cluster_column] == cluster]\n",
    "        #X = np.array([x.flatten() for x in cluster_data[feature_column]])\n",
    "        X = np.array([x for x in cluster_data[feature_column]])\n",
    "        y = np.array(cluster_data[target_column])\n",
    "\n",
    "        # Stratified K-Fold for validation splits\n",
    "        ## IDK IF THIS WILL WORK WITH PYTORCH FORMATTED DATA...\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        cluster_val_accuracy = 0\n",
    "        for idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "            y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "            # Create TensorDataset\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "            # Create DataLoader\n",
    "            train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "            \n",
    "            fold_model = base_model.__class__(**base_model.init_params)\n",
    "            optimizer = torch.optim.Adam(fold_model.parameters(), lr=lr)\n",
    "            \n",
    "            # Now train your fold_model\n",
    "            fold_model.train()  # Ensure the model is in training mode\n",
    "            for epoch in range(num_epochs):\n",
    "                # Training loop with your X_train and y_train\n",
    "                for X_batch, y_batch in train_loader:  # Assuming you have a DataLoader\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = fold_model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            fold_model.eval()\n",
    "            fold_predictions = []\n",
    "            fold_true_labels = []\n",
    "            with torch.no_grad():\n",
    "                # Validation loop\n",
    "                predictions = []\n",
    "                for X_val in val_loader:  # Assuming you have a validation DataLoader\n",
    "                    outputs = fold_model(X_val)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    fold_predictions.extend(preds.cpu().numpy())\n",
    "                    fold_true_labels.extend(y_val.cpu().numpy())\n",
    "            # Predict on the validation set and calculate accuracy\n",
    "            #y_pred = fold_model.predict(X_val)\n",
    "            #cluster_val_accuracy += accuracy_score(y_val, y_pred)\n",
    "            # Calculate accuracy for the current fold\n",
    "            fold_accuracy = accuracy_score(fold_true_labels, fold_predictions)\n",
    "            cluster_val_accuracy += fold_accuracy\n",
    "\n",
    "            if idx==0:\n",
    "                # No great way to save the models... I really only want to use 1...\n",
    "                ## So for now I'll just save the first kfold's model...\n",
    "                ## Consistently biased but hopefully the val splits are all roughly equivalent\n",
    "                clus_model_lst.append(fold_model)\n",
    "\n",
    "        # REWRITE THIS!!!\n",
    "        # Average accuracy for this cluster\n",
    "        cluster_val_accuracy /= n_splits\n",
    "        # I think this really ought to append not add...\n",
    "        ## TOTAL maintains the acc of the entire process (across all clusters)\n",
    "        total_val_accuracy += cluster_val_accuracy\n",
    "        num_folds_processed += 1\n",
    "        \n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "\n",
    "    # Overall average accuracy across all clusters\n",
    "    avg_val_accuracy = total_val_accuracy / num_folds_processed\n",
    "    #print(f\"\\nOverall Average Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    \n",
    "    return clus_model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "269a8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_agglo_merge_procedure(data_dfs_dict, model_type, n_splits=2):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - model (str or sklearn model object): The model to train. If string, it must be one of:\n",
    "      ['LogisticRegression', 'SVC', 'RF', 'GradientBoosting', 'KNN', 'XGBoost'].\n",
    "    \"\"\"\n",
    "    \n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Unique gestures and number of classes\n",
    "    unique_gestures = np.unique(data_dfs_dict['train']['Gesture_Encoded'])\n",
    "    num_classes = len(unique_gestures)\n",
    "    input_dim = len(data_dfs_dict['train']['features'].iloc[0])\n",
    "    \n",
    "    # Select model\n",
    "    if model_type == 'CNN':\n",
    "        model = CNNModel(input_dim, num_classes).to('cpu')\n",
    "    elif model_type == 'RNN':\n",
    "        model = RNNModel(input_dim, num_classes).to('cpu')\n",
    "        \n",
    "    train_df = data_dfs_dict['train']\n",
    "    test_df = data_dfs_dict['test']\n",
    "        \n",
    "    # Data structures for logging cluster merging procedure\n",
    "    merge_log = []  # List of tuples: [(cluster1, cluster2, distance, new_cluster), ...]\n",
    "    unique_clusters_log = []  # List of lists: [list of unique clusters at each step]\n",
    "    # Dictionary to store self-performance over iterations\n",
    "    intra_cluster_performance = {}\n",
    "    cross_cluster_performance = {}\n",
    "    # Simulate cluster merging and model performance tracking\n",
    "    iterations = 0\n",
    "    # Main loop for cluster merging\n",
    "    while len(train_df['Cluster_ID'].unique()) > 1:\n",
    "        print(f\"{len(train_df['Cluster_ID'].unique())} Clusters Remaining\")\n",
    "        # Log the current state of clusters\n",
    "        unique_clusters_log.append(sorted(train_df['Cluster_ID'].unique()))\n",
    "\n",
    "        # userdef_df continuously changes wrt cluster ID\n",
    "        ## So... do train_df and test_df continuously change? Cluster ID changes... \n",
    "        ## but that... may or may not affect stratificiation in a meaningful way (I'm not using cluster metadata...)\n",
    "        # These 2 should be the same... it's stratified...\n",
    "        current_train_cluster_ids = sorted(train_df['Cluster_ID'].unique())\n",
    "        current_test_cluster_ids = sorted(test_df['Cluster_ID'].unique())\n",
    "        if current_train_cluster_ids == current_test_cluster_ids:\n",
    "            current_cluster_ids = current_train_cluster_ids\n",
    "        else:\n",
    "            raise ValueError(\"Train/test Cluster ID lists not the same length... Stratify failed\")\n",
    "\n",
    "        # Train models with logging for specified clusters\n",
    "        ## UPDATED TO DNN VERSION HERE!\n",
    "        clus_model_lst = train_and_cv_DNN_cluster_model(train_df, model, current_cluster_ids, n_splits=n_splits)\n",
    "        # Pairwise test models with logging for specified clusters\n",
    "        sym_acc_arr = test_models_on_clusters(test_df, clus_model_lst, current_cluster_ids)\n",
    "\n",
    "        for idx, cluster_id in enumerate(current_cluster_ids):\n",
    "            cross_acc_sum = 0\n",
    "            cross_acc_count = 0\n",
    "\n",
    "            for idx2, cluster_id2 in enumerate(current_cluster_ids):\n",
    "                if cluster_id not in intra_cluster_performance:\n",
    "                    intra_cluster_performance[cluster_id] = []  # Initialize list\n",
    "\n",
    "                if idx == idx2:  # Diagonal, so intra-cluster\n",
    "                    # Ensure the logic assumption holds\n",
    "                    if cluster_id != cluster_id2:\n",
    "                        raise ValueError(\"This code isn't working as expected...\")\n",
    "                    intra_cluster_performance[cluster_id].append((iterations, sym_acc_arr[idx, idx2]))\n",
    "                else:  # Non-diagonal, so cross-cluster\n",
    "                    cross_acc_sum += sym_acc_arr[idx, idx2]\n",
    "                    cross_acc_count += 1\n",
    "\n",
    "            # Calculate average cross-cluster accuracy\n",
    "            if cross_acc_count > 0:\n",
    "                avg_cross_acc = cross_acc_sum / cross_acc_count\n",
    "            else:\n",
    "                avg_cross_acc = None  # Handle the case where no cross-cluster pairs exist\n",
    "            # Append the average cross-cluster accuracy to all relevant clusters\n",
    "            if cluster_id not in cross_cluster_performance:\n",
    "                cross_cluster_performance[cluster_id] = []  # Initialize list\n",
    "            cross_cluster_performance[cluster_id].append((iterations, avg_cross_acc))\n",
    "\n",
    "        masked_diag_array = sym_acc_arr.copy()\n",
    "        np.fill_diagonal(masked_diag_array, 0.0)\n",
    "        similarity_score = np.max(masked_diag_array)\n",
    "        max_index = np.unravel_index(np.argmax(masked_diag_array), masked_diag_array.shape)\n",
    "        row_idx_to_merge = max_index[0]\n",
    "        col_idx_to_merge = max_index[1]\n",
    "        # Get actual cluster IDs to merge\n",
    "        row_cluster_to_merge = current_cluster_ids[row_idx_to_merge]\n",
    "        col_cluster_to_merge = current_cluster_ids[col_idx_to_merge]\n",
    "\n",
    "        # Create a new cluster ID for the merged cluster\n",
    "        new_cluster_id = max(current_cluster_ids) + 1\n",
    "        #print(f\"MERGE: {row_cluster_to_merge, col_cluster_to_merge} @ {similarity_score*100:.2f}. New cluster: {new_cluster_id}\")\n",
    "        # Log the merge\n",
    "        merge_log.append((iterations, row_cluster_to_merge, col_cluster_to_merge, similarity_score, new_cluster_id))\n",
    "        # Update the DataFrame with the new merged cluster\n",
    "        #userdef_df.loc[userdef_df['Cluster_ID'].isin([row_cluster_to_merge, col_cluster_to_merge]), 'Cluster_ID'] = new_cluster_id\n",
    "        train_df.loc[userdef_df['Cluster_ID'].isin([row_cluster_to_merge, col_cluster_to_merge]), 'Cluster_ID'] = new_cluster_id\n",
    "        test_df.loc[userdef_df['Cluster_ID'].isin([row_cluster_to_merge, col_cluster_to_merge]), 'Cluster_ID'] = new_cluster_id\n",
    "        \n",
    "        # Remove merged clusters from tracking (mark end with None)\n",
    "        intra_cluster_performance[row_cluster_to_merge].append((iterations, None))\n",
    "        intra_cluster_performance[col_cluster_to_merge].append((iterations, None))\n",
    "        cross_cluster_performance[row_cluster_to_merge].append((iterations, None))\n",
    "        cross_cluster_performance[col_cluster_to_merge].append((iterations, None))\n",
    "\n",
    "        iterations += 1\n",
    "    \n",
    "    return merge_log, intra_cluster_performance, cross_cluster_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbfe78a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Clusters Remaining\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CNNModel' object has no attribute 'init_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merge_log, intra_cluster_performance, cross_cluster_performance \u001b[38;5;241m=\u001b[39m \u001b[43mDNN_agglo_merge_procedure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dfs_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 50\u001b[0m, in \u001b[0;36mDNN_agglo_merge_procedure\u001b[1;34m(data_dfs_dict, model_type, n_splits)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain/test Cluster ID lists not the same length... Stratify failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Train models with logging for specified clusters\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m## UPDATED TO DNN VERSION HERE!\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m clus_model_lst \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_cv_DNN_cluster_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_cluster_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Pairwise test models with logging for specified clusters\u001b[39;00m\n\u001b[0;32m     52\u001b[0m sym_acc_arr \u001b[38;5;241m=\u001b[39m test_models_on_clusters(test_df, clus_model_lst, current_cluster_ids)\n",
      "Cell \u001b[1;32mIn[29], line 76\u001b[0m, in \u001b[0;36mtrain_and_cv_DNN_cluster_model\u001b[1;34m(train_df, model_type, cluster_ids, cluster_column, feature_column, target_column, n_splits, bs, lr, criterion)\u001b[0m\n\u001b[0;32m     73\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m fold_model \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_params\u001b[49m)\n\u001b[0;32m     77\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(fold_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Now train your fold_model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fl_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CNNModel' object has no attribute 'init_params'"
     ]
    }
   ],
   "source": [
    "merge_log, intra_cluster_performance, cross_cluster_performance = DNN_agglo_merge_procedure(data_dfs_dict, \"CNN\", n_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85631e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c994ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327936ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89390b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7946d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cbe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a195302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd999f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78427b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7106cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97122c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72514643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af48c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c3562",
   "metadata": {},
   "source": [
    "## INTRA CLUSTER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for cluster_id in intra_cluster_performance:\n",
    "    # Extract valid iterations and performance\n",
    "    data = intra_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    if valid_iterations[0]==0:\n",
    "        continue\n",
    "    #print(valid_iterations)\n",
    "    #print(valid_performance)\n",
    "    #print()\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Intra-Cluster Test Accuracy\", fontsize=18)\n",
    "plt.title(f\"{model_str} Intra-Cluster Acc: Merged Clusters Only\", fontsize=18)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for cluster_id in intra_cluster_performance:\n",
    "    # Extract valid iterations and performance\n",
    "    data = intra_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    if valid_iterations[0]!=0:\n",
    "        continue\n",
    "    #print(valid_iterations)\n",
    "    #print(valid_performance)\n",
    "    #print()\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Intra-Cluster Test Accuracy\", fontsize=18)\n",
    "plt.title(f\"{model_str} Intra-Cluster Acc: Original Clusters Only\", fontsize=18)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3f61a",
   "metadata": {},
   "source": [
    "## CROSS CLUSTER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for cluster_id in cross_cluster_performance:\n",
    "    # Extract valid iterations and performance\n",
    "    data = cross_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    if valid_iterations[0]==0:\n",
    "        continue\n",
    "    #print(valid_iterations)\n",
    "    #print(valid_performance)\n",
    "    #print()\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Cross-Cluster Test Accuracy\", fontsize=18)\n",
    "plt.title(f\"{model_str} Cross-Cluster Acc: Merged Clusters Only\", fontsize=18)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for cluster_id in cross_cluster_performance:\n",
    "    # Extract valid iterations and performance\n",
    "    data = cross_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    if valid_iterations[0]!=0:\n",
    "        continue\n",
    "    #print(valid_iterations)\n",
    "    #print(valid_performance)\n",
    "    #print()\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Cross-Cluster Test Accuracy\", fontsize=18)\n",
    "plt.title(f\"{model_str} Cross-Cluster Acc: Original Clusters Only\", fontsize=18)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db152ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "n = 5\n",
    "\n",
    "# Reduce number of lines (e.g., top n longest-lived clusters)\n",
    "longest_clusters = sorted(cross_cluster_performance.keys(), key=lambda k: len(cross_cluster_performance[k]), reverse=True)[:n]\n",
    "\n",
    "for cluster_id in longest_clusters:\n",
    "    # Extract valid iterations and performance\n",
    "    data = cross_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cross-Cluster Test Accuracy\")\n",
    "plt.title(f\"Cross-Cluster Test Accuracy: {n} Longest-Lasting Clusters\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "n = 5  # Number of clusters to plot\n",
    "\n",
    "# Find clusters with the highest final accuracies\n",
    "highest_final_accuracy_clusters = sorted(\n",
    "    cross_cluster_performance.keys(), \n",
    "    key=lambda k: max([perf for it, perf in cross_cluster_performance[k] if perf is not None], default=0), \n",
    "    reverse=True\n",
    ")[:n]\n",
    "\n",
    "# Plot the performance curves for these clusters\n",
    "for cluster_id in highest_final_accuracy_clusters:\n",
    "    # Extract valid iterations and performance\n",
    "    data = cross_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cross-Cluster Test Accuracy\")\n",
    "plt.title(f\"Cross-Cluster Test Accuracy: {n} Clusters with Highest Final Accuracy\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd2293",
   "metadata": {},
   "source": [
    "## Intra-Cluster Test Accuracy Merge Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with Merge Log and Connections\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Dictionary to track the last valid point for each cluster\n",
    "last_points = {}\n",
    "\n",
    "# Plot original clusters\n",
    "for cluster_id in cross_cluster_performance:\n",
    "    data = cross_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    if valid_iterations[0] != 0:\n",
    "        continue\n",
    "\n",
    "    # Plot original cluster performance\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "    last_points[cluster_id] = (valid_iterations[-1], valid_performance[-1])  # Store the last point\n",
    "\n",
    "# Handle merged clusters and connect original clusters\n",
    "for iteration, cluster1, cluster2, _, new_cluster in merge_log:\n",
    "    # Plot and connect the merged clusters\n",
    "    for cluster in [cluster1, cluster2]:\n",
    "        if cluster in cross_cluster_performance:\n",
    "            data = cross_cluster_performance[cluster]\n",
    "            merge_perf = next((perf for it, perf in data if it == iteration), None)\n",
    "            if merge_perf is not None:\n",
    "                plt.scatter(iteration, merge_perf, color='red', marker='^')#, label=f\"Merge {cluster}  {new_cluster}\")\n",
    "                \n",
    "        if cluster in last_points:  # If it's an original cluster\n",
    "            last_iteration, last_perf = last_points[cluster]\n",
    "\n",
    "            # Connect to the newly merged cluster\n",
    "            if new_cluster in cross_cluster_performance:\n",
    "                new_data = cross_cluster_performance[new_cluster]\n",
    "                valid_iterations = [it for it, perf in new_data if perf is not None and it >= iteration]\n",
    "                valid_performance = [perf for it, perf in new_data if perf is not None and it >= iteration]\n",
    "\n",
    "                if valid_iterations:\n",
    "                    # Draw a line connecting the original cluster to the new merged cluster\n",
    "                    plt.plot(\n",
    "                        [last_iteration, valid_iterations[0]],\n",
    "                        [last_perf, valid_performance[0]],\n",
    "                        linestyle='--', color='gray'\n",
    "                    )\n",
    "\n",
    "                    # Continue plotting the merged cluster's performance\n",
    "                    plt.plot(valid_iterations, valid_performance, linestyle='--')\n",
    "\n",
    "                # Update the last points for the newly merged cluster\n",
    "                if valid_iterations:\n",
    "                    last_points[new_cluster] = (valid_iterations[-1], valid_performance[-1])\n",
    "\n",
    "# Add labels, legend, and formatting\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Cross-Cluster Test Accuracy\", fontsize=18)\n",
    "plt.title(f\"{model_str} Cross-Cluster Acc with Merge Connections\", fontsize=18)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with Merge Log and Connections\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Dictionary to track the last valid point for each cluster\n",
    "last_points = {}\n",
    "\n",
    "# Plot original clusters\n",
    "for cluster_id in intra_cluster_performance:\n",
    "    data = intra_cluster_performance[cluster_id]\n",
    "    valid_iterations = [it for it, perf in data if perf is not None]\n",
    "    valid_performance = [perf for it, perf in data if perf is not None]\n",
    "    if valid_iterations[0] != 0:\n",
    "        continue\n",
    "\n",
    "    # Plot original cluster performance\n",
    "    plt.plot(valid_iterations, valid_performance, label=f\"Cluster {cluster_id}\")\n",
    "    last_points[cluster_id] = (valid_iterations[-1], valid_performance[-1])  # Store the last point\n",
    "\n",
    "# Handle merged clusters and connect original clusters\n",
    "for iteration, cluster1, cluster2, _, new_cluster in merge_log:\n",
    "    # Plot and connect the merged clusters\n",
    "    for cluster in [cluster1, cluster2]:\n",
    "        if cluster in intra_cluster_performance:\n",
    "            data = intra_cluster_performance[cluster]\n",
    "            merge_perf = next((perf for it, perf in data if it == iteration), None)\n",
    "            if merge_perf is not None:\n",
    "                plt.scatter(iteration, merge_perf, color='red', marker='^')#, label=f\"Merge {cluster}  {new_cluster}\")\n",
    "                \n",
    "        if cluster in last_points:  # If it's an original cluster\n",
    "            last_iteration, last_perf = last_points[cluster]\n",
    "\n",
    "            # Connect to the newly merged cluster\n",
    "            if new_cluster in intra_cluster_performance:\n",
    "                new_data = intra_cluster_performance[new_cluster]\n",
    "                valid_iterations = [it for it, perf in new_data if perf is not None and it >= iteration]\n",
    "                valid_performance = [perf for it, perf in new_data if perf is not None and it >= iteration]\n",
    "\n",
    "                if valid_iterations:\n",
    "                    # Draw a line connecting the original cluster to the new merged cluster\n",
    "                    plt.plot(\n",
    "                        [last_iteration, valid_iterations[0]],\n",
    "                        [last_perf, valid_performance[0]],\n",
    "                        linestyle='--', color='gray'\n",
    "                    )\n",
    "\n",
    "                    # Continue plotting the merged cluster's performance\n",
    "                    plt.plot(valid_iterations, valid_performance, linestyle='--')\n",
    "\n",
    "                # Update the last points for the newly merged cluster\n",
    "                if valid_iterations:\n",
    "                    last_points[new_cluster] = (valid_iterations[-1], valid_performance[-1])\n",
    "\n",
    "# Add labels, legend, and formatting\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Intra-Cluster Test Accuracy\", fontsize=18)\n",
    "plt.title(f\"{model_str} Intra-Cluster Acc with Merge Connections\", fontsize=18)\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_mean_lst, cross_mean_lst, ratio_lst = compute_performance_ratios(intra_cluster_performance, cross_cluster_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(intra_mean_lst[31:], label=\"Intra Mean\")\n",
    "plt.plot(cross_mean_lst[31:], label=\"Cross Mean\")\n",
    "plt.plot((np.array(ratio_lst)/10)[31:], label=\"Ratio/10\")\n",
    "plt.xlabel(\"Iteration\", fontsize=18)\n",
    "plt.ylabel(\"Mean Accuracy | Ratio/10\", fontsize=18)\n",
    "plt.title(f\"{model_str} Summary Statistic Trends\", fontsize=18)\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7f175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
