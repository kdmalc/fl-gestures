{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b80c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Meta_Gesture_2024\\\\saved_datasets\\\\filtered_datasets\\\\$BStand_EMG_df.pkl'\n",
    "path2 = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Meta_Gesture_2024\\\\saved_datasets\\\\filtered_datasets\\\\metadata_EMG_allgestures_allusers.pkl'\n",
    "path3 = 'C:\\\\Users\\\\kdmen\\\\Box\\\\Meta_Gesture_2024\\\\saved_datasets\\\\filtered_datasets\\\\EMG_PPD\\\\full_df.pkl'\n",
    "\n",
    "with open(path1, 'rb') as file:\n",
    "    df1 = pickle.load(file)  # (204800, 19)\n",
    "#with open(path2, 'rb') as file:\n",
    "#    df2 = pickle.load(file)  # (204800, 19) --> THESE VALS ARE NON-NEGATIVE!\n",
    "#with open(path3, 'rb') as file:\n",
    "#    df3 = pickle.load(file)  # (204800, 19) --> Appears to be the same as df1!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b710811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204800, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>EMG1</th>\n",
       "      <th>EMG2</th>\n",
       "      <th>EMG3</th>\n",
       "      <th>EMG4</th>\n",
       "      <th>EMG5</th>\n",
       "      <th>EMG6</th>\n",
       "      <th>EMG7</th>\n",
       "      <th>EMG8</th>\n",
       "      <th>EMG9</th>\n",
       "      <th>EMG10</th>\n",
       "      <th>EMG11</th>\n",
       "      <th>EMG12</th>\n",
       "      <th>EMG13</th>\n",
       "      <th>EMG14</th>\n",
       "      <th>EMG15</th>\n",
       "      <th>EMG16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.362743</td>\n",
       "      <td>-0.801651</td>\n",
       "      <td>-0.383077</td>\n",
       "      <td>-0.195299</td>\n",
       "      <td>-0.203047</td>\n",
       "      <td>-0.464472</td>\n",
       "      <td>-0.276292</td>\n",
       "      <td>-0.026736</td>\n",
       "      <td>-0.873870</td>\n",
       "      <td>-1.036152</td>\n",
       "      <td>-0.580930</td>\n",
       "      <td>-0.719494</td>\n",
       "      <td>-0.502255</td>\n",
       "      <td>-1.750091</td>\n",
       "      <td>-0.127847</td>\n",
       "      <td>-0.094192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.351553</td>\n",
       "      <td>-0.775334</td>\n",
       "      <td>-0.382545</td>\n",
       "      <td>-0.154773</td>\n",
       "      <td>-0.131977</td>\n",
       "      <td>-0.295204</td>\n",
       "      <td>-0.125822</td>\n",
       "      <td>0.089679</td>\n",
       "      <td>-0.816215</td>\n",
       "      <td>-2.082635</td>\n",
       "      <td>-0.006283</td>\n",
       "      <td>-0.139439</td>\n",
       "      <td>-0.367764</td>\n",
       "      <td>-0.208084</td>\n",
       "      <td>-0.111811</td>\n",
       "      <td>-0.039009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.380825</td>\n",
       "      <td>-0.762588</td>\n",
       "      <td>-0.398388</td>\n",
       "      <td>-0.085411</td>\n",
       "      <td>0.017528</td>\n",
       "      <td>-0.205675</td>\n",
       "      <td>-0.068451</td>\n",
       "      <td>0.117076</td>\n",
       "      <td>-0.668221</td>\n",
       "      <td>-3.403064</td>\n",
       "      <td>-0.526030</td>\n",
       "      <td>-0.478294</td>\n",
       "      <td>-0.300443</td>\n",
       "      <td>0.203266</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.004728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.366795</td>\n",
       "      <td>-0.765464</td>\n",
       "      <td>-0.374423</td>\n",
       "      <td>-0.073225</td>\n",
       "      <td>0.183172</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>-0.058907</td>\n",
       "      <td>0.080977</td>\n",
       "      <td>-0.424416</td>\n",
       "      <td>-3.709413</td>\n",
       "      <td>-0.570894</td>\n",
       "      <td>-0.775155</td>\n",
       "      <td>-0.144710</td>\n",
       "      <td>-0.619539</td>\n",
       "      <td>0.146499</td>\n",
       "      <td>0.199975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P102</td>\n",
       "      <td>pan</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.245578</td>\n",
       "      <td>-0.761283</td>\n",
       "      <td>-0.303976</td>\n",
       "      <td>-0.081947</td>\n",
       "      <td>0.224996</td>\n",
       "      <td>0.103319</td>\n",
       "      <td>-0.003929</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>-0.016530</td>\n",
       "      <td>-4.075150</td>\n",
       "      <td>-0.127710</td>\n",
       "      <td>2.682791</td>\n",
       "      <td>-0.141750</td>\n",
       "      <td>-0.208404</td>\n",
       "      <td>-0.035642</td>\n",
       "      <td>0.172662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID Gesture_Num      EMG1      EMG2      EMG3      EMG4   \n",
       "0        P102        pan           1 -0.362743 -0.801651 -0.383077 -0.195299  \\\n",
       "1        P102        pan           1 -0.351553 -0.775334 -0.382545 -0.154773   \n",
       "2        P102        pan           1 -0.380825 -0.762588 -0.398388 -0.085411   \n",
       "3        P102        pan           1 -0.366795 -0.765464 -0.374423 -0.073225   \n",
       "4        P102        pan           1 -0.245578 -0.761283 -0.303976 -0.081947   \n",
       "\n",
       "       EMG5      EMG6      EMG7      EMG8      EMG9     EMG10     EMG11   \n",
       "0 -0.203047 -0.464472 -0.276292 -0.026736 -0.873870 -1.036152 -0.580930  \\\n",
       "1 -0.131977 -0.295204 -0.125822  0.089679 -0.816215 -2.082635 -0.006283   \n",
       "2  0.017528 -0.205675 -0.068451  0.117076 -0.668221 -3.403064 -0.526030   \n",
       "3  0.183172  0.009277 -0.058907  0.080977 -0.424416 -3.709413 -0.570894   \n",
       "4  0.224996  0.103319 -0.003929  0.041526 -0.016530 -4.075150 -0.127710   \n",
       "\n",
       "      EMG12     EMG13     EMG14     EMG15     EMG16  \n",
       "0 -0.719494 -0.502255 -1.750091 -0.127847 -0.094192  \n",
       "1 -0.139439 -0.367764 -0.208084 -0.111811 -0.039009  \n",
       "2 -0.478294 -0.300443  0.203266  0.113300  0.004728  \n",
       "3 -0.775155 -0.144710 -0.619539  0.146499  0.199975  \n",
       "4  2.682791 -0.141750 -0.208404 -0.035642  0.172662  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0aa08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts for Participant:\n",
      "Participant\n",
      "P102    6400\n",
      "P103    6400\n",
      "P010    6400\n",
      "P008    6400\n",
      "P006    6400\n",
      "P005    6400\n",
      "P004    6400\n",
      "P132    6400\n",
      "P131    6400\n",
      "P128    6400\n",
      "P127    6400\n",
      "P126    6400\n",
      "P125    6400\n",
      "P124    6400\n",
      "P123    6400\n",
      "P122    6400\n",
      "P121    6400\n",
      "P119    6400\n",
      "P118    6400\n",
      "P116    6400\n",
      "P115    6400\n",
      "P114    6400\n",
      "P112    6400\n",
      "P111    6400\n",
      "P110    6400\n",
      "P109    6400\n",
      "P108    6400\n",
      "P107    6400\n",
      "P106    6400\n",
      "P105    6400\n",
      "P104    6400\n",
      "P011    6400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Counts for Gesture_ID:\n",
      "Gesture_ID\n",
      "pan              20480\n",
      "duplicate        20480\n",
      "zoom-out         20480\n",
      "zoom-in          20480\n",
      "move             20480\n",
      "rotate           20480\n",
      "select-single    20480\n",
      "delete           20480\n",
      "close            20480\n",
      "open             20480\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Counts for Gesture_Num:\n",
      "Gesture_Num\n",
      "1     20480\n",
      "2     20480\n",
      "3     20480\n",
      "4     20480\n",
      "5     20480\n",
      "6     20480\n",
      "7     20480\n",
      "8     20480\n",
      "9     20480\n",
      "10    20480\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in ['Participant', 'Gesture_ID', 'Gesture_Num']:\n",
    "    print(f\"\\nCounts for {column}:\")\n",
    "    counts = df1[column].value_counts()\n",
    "    print(counts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0375f2f",
   "metadata": {},
   "source": [
    " As a preliminary\n",
    " step, we can simulate each user training a gesture classification model using solely their own dataset.\n",
    " Wecan then execute a pairwise comparison, where we test each user’s model and every other user’s\n",
    " dataset, and report the score. At this point, we can begin a agglomerative clustering procedure,\n",
    " by either clustering the two users with the highest corresponding scores, or by clustering all users\n",
    " that achieved above some minimum cross-subject classification accuracy. Then, we can repeat this\n",
    " procedure on the newly formed clusters, this time training a single model over all the data in the\n",
    " given cluster. By repeating this process until only one model remains, we can generate a dendrogram\n",
    " showing which users are best clustered in order to train cluster-level models. One issue with this\n",
    " approach is that as the clusters grow, the training sets will grow in tandem. To a limited extent, this\n",
    " can be offset by limiting all dataset sizes to the size of single user’s dataset and having the cluster\n",
    " model train on a random sample of gestures. An alternative approach would be to enforce all users\n",
    " into clusters of the same size each round, so that all clusters have the same dataset size, although this\n",
    " would artificially create many clusters. However, we also expect to have the opposite problem, as in\n",
    " myprevious works I have shown that cross-subject models typically perform poorly in general for\n",
    " these types of tasks, e.g. that training over multiple users will likely only grant modest improvements\n",
    " in performance, if any (especially if fixing the training dataset size). The goal of this procedure is\n",
    " to yield clusters for which the internal cluster model performs better than any of the intra-subject\n",
    " models within the cluster. Even if the internal cluster model does not outperform all intra-subject\n",
    " models, as long as it reduces the calibration / number of samples required from a novel user, this is\n",
    " still a success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7629f5",
   "metadata": {},
   "source": [
    "A\n",
    " related approach would be doing the above but comparing the model weights more directly, either\n",
    " in Euclidean space (e.g., Frobenius norm) or in a latent space (PCA, CCA, autoencoders, etc., just\n",
    " trying to find a more structured latent space to compare model weights), and use a similar minimum\n",
    " similarity threshold for defining edges in the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9172f2c",
   "metadata": {},
   "source": [
    " A final remaining open question is to what extent can this these questions be\n",
    " answered entirely on the basis of body-part used (e.g., clustering based on whether the finger, neck,\n",
    " arm, etc. was used for the gesture). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b9d20",
   "metadata": {},
   "source": [
    "Additionally, we may be able to leverage mixture of experts related approaches [16, 17, 18], wherein\n",
    " each new sample is passed to a gating mechanism which must decide which expert (or in our case,\n",
    " cluster model) is best suited for it, and then is run through the corresponding expert model. In our\n",
    " scenario, one could imagine having expert models that are ability-based, however that is defined, or\n",
    " potentially disability-based or body-part based (e.g., all gestures performed by the fingers will be sent\n",
    " to the finger expert model/cluster, etc.). Generally, within a disability diagnosis, there is still a wide\n",
    " range of abilities, so explicit disability-based groupings typically are undesirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15174b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f457349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd325882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72921593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e25e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12d85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1699b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f5352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c448f64f",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a5e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_order(df_freq):\n",
    "    zero_order_moments_log = []\n",
    "    zero_order_moments_raw = []\n",
    "\n",
    "    for sensor in df_freq.columns:\n",
    "        # Extract the frequency domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        \n",
    "        # Step 1: Square the signal (power) at each frequency\n",
    "        signal_squared = np.abs(time_data) ** 2\n",
    "        \n",
    "        # Step 2: Integrate (sum) the squared values to get the total power\n",
    "        total_power = np.sum(signal_squared)\n",
    "        \n",
    "        # Step 3: Take the logarithm of the total power\n",
    "        log_total_power = np.log(total_power)\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        zero_order_moments_log.append(log_total_power)\n",
    "        zero_order_moments_raw.append(total_power)\n",
    "        \n",
    "    return zero_order_moments_log, zero_order_moments_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873dc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order(df_freq, zero_order_raw):\n",
    "    # Initialize lists to store the results for each sensor\n",
    "    first_order_moments_log = []\n",
    "    first_order_moments_raw = []\n",
    "    \n",
    "    for i,sensor in enumerate(df_freq.columns):\n",
    "        # Extract the frequency domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        \n",
    "        # Step 1: Take first derivative\n",
    "        first_deriv = np.gradient(time_data)\n",
    "        \n",
    "        # Step 2: Square the signal (power) at each frequency of the first deriv\n",
    "        signal_squared = np.abs(first_deriv) ** 2\n",
    "        \n",
    "        # Step 3: Integrate (sum) the squared values to get the total power\n",
    "        total_power = np.sum(signal_squared)\n",
    "        \n",
    "        # Step 4: Take the logarithm of the total power\n",
    "        log_total_power = np.log(total_power / (zero_order_raw[i]**2))\n",
    "        \n",
    "        # Store the results in the lists\n",
    "        first_order_moments_log.append(log_total_power)\n",
    "        first_order_moments_raw.append(total_power)\n",
    "\n",
    "    # Convert lists to numpy arrays for consistency\n",
    "    first_order_moments_log = np.array(first_order_moments_log)\n",
    "    first_order_moments_raw = np.array(first_order_moments_raw)\n",
    "    \n",
    "    return first_order_moments_log, first_order_moments_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b315a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order(df_freq, zero_order_raw):\n",
    "    second_order_log = []\n",
    "    second_order_raw = []\n",
    "\n",
    "    for i, sensor in enumerate(df_freq.columns):\n",
    "        # Extract the time domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        \n",
    "        # Step 1: Take first derivative\n",
    "        first_deriv = np.gradient(time_data)\n",
    "        \n",
    "        #Step 2: take second derivative\n",
    "        second_deriv = np.gradient(first_deriv)\n",
    "        \n",
    "        # Step 3: Square the signal (power) at each frequency of the first deriv\n",
    "        signal_squared = np.abs(second_deriv) ** 2\n",
    "        \n",
    "        # Step 4: Integrate (sum) the squared values to get the total power\n",
    "        total_power = np.sum(signal_squared)\n",
    "        \n",
    "        # Step 4: Take the logarithm of the total power\n",
    "        log_total_power = np.log(total_power/(zero_order_raw[i]**4))\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        second_order_log.append(log_total_power)\n",
    "        second_order_raw.append(total_power)\n",
    "    \n",
    "    return second_order_log, second_order_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed260c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working version of third_order that has precautions so no neg values inside log\n",
    "def third_order(second_order_raw, first_order_raw, zero_order_raw):\n",
    "    second_order_raw = np.array(second_order_raw)\n",
    "    first_order_raw = np.array(first_order_raw)\n",
    "    zero_order_raw = np.array(zero_order_raw)\n",
    "    \n",
    "    # Step 1: Compute the square roots (ensure no negative values before sqrt)\n",
    "    sqrt_first_diff = np.sqrt(np.maximum(zero_order_raw - first_order_raw, 1e-10))  # Handle small or negative values\n",
    "    sqrt_second_diff = np.sqrt(np.maximum(zero_order_raw - second_order_raw, 1e-10))  # Handle small or negative values\n",
    "    \n",
    "    # Step 2: Perform the dot product (ensure no division by zero by adding a small constant)\n",
    "    dot_product = np.dot(sqrt_first_diff, sqrt_second_diff)\n",
    "    dot_product = max(dot_product, 1e-10)  # Avoid division by zero or extremely small numbers\n",
    "    \n",
    "    # Step 3: Compute the sparseness ratio (ensure the result is positive)\n",
    "    sparseness = zero_order_raw / dot_product\n",
    "    \n",
    "    # Step 4: Logarithm of the sparseness (ensure no negative or zero values inside the log)\n",
    "    # We use np.maximum to ensure all values are at least 1e-10 to avoid taking log of non-positive values.\n",
    "    safe_sparseness = np.maximum(sparseness, 1e-10)\n",
    "    \n",
    "    #print(\"Safe sparseness:\", safe_sparseness)\n",
    "    \n",
    "    # Step 5: Apply the logarithm for third-order moments\n",
    "    third_order_moments_log = np.log(safe_sparseness)\n",
    "    \n",
    "    return third_order_moments_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdeeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourth_order(df_freq, zero_order_raw, first_order_raw, second_order_raw):\n",
    "    # Initialize a list to store the fourth-order moments log for each sensor\n",
    "    fourth_order_moments_logs = []\n",
    "    \n",
    "    for i,sensor in enumerate(df_freq.columns):\n",
    "        # Extract the time domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        \n",
    "        # Step 1: Take first derivative\n",
    "        first_deriv = np.gradient(time_data)\n",
    "        \n",
    "        # Step 2: Absolute value of the signal (power) at each frequency of the first derivative\n",
    "        signal_abs = np.abs(first_deriv)\n",
    "        \n",
    "        # Step 3: Integrate (sum) the absolute values to get the total power\n",
    "        total_power = np.sum(signal_abs)\n",
    "        \n",
    "        # Step 4: Compute the fourth-order moments log for this sensor\n",
    "        moment_log = np.log(np.sqrt((first_order_raw[i]**2) / \n",
    "                           (zero_order_raw[i] * second_order_raw[i])) / total_power)\n",
    "        \n",
    "        # Store the result in the list\n",
    "        fourth_order_moments_logs.append(moment_log)\n",
    "    \n",
    "    # Convert list to numpy array for consistency\n",
    "    fourth_order_moments_logs = np.array(fourth_order_moments_logs)\n",
    "    \n",
    "    return fourth_order_moments_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08c98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vectors(group):\n",
    "    result_vector = []\n",
    "    #can only run features on EMG columns\n",
    "    emg_columns = [col for col in group.columns if col.startswith('EMG')]\n",
    "    \n",
    "    for emg_col in emg_columns:\n",
    "        data = group[[emg_col]] #run on EMG columns. convert to df with single column to run with all the features\n",
    "        \n",
    "        #zero-order\n",
    "        zero_order_log, zero_order_raw = zero_order(data)\n",
    "        result_vector.append(zero_order_log)\n",
    "        \n",
    "        #first-order\n",
    "        first_order_log, first_order_raw = first_order(data, zero_order_raw)\n",
    "        result_vector.append(first_order_log)\n",
    "        \n",
    "        #second-order\n",
    "        second_order_log, second_order_raw = second_order(data, zero_order_raw)\n",
    "        result_vector.append(second_order_log)\n",
    "        \n",
    "        #third-order\n",
    "        third_order_log = third_order(second_order_raw, first_order_raw, zero_order_raw)\n",
    "        result_vector.append(third_order_log)\n",
    "        \n",
    "        #fourth-order\n",
    "        fourth_order_log = fourth_order(data, zero_order_raw, first_order_raw, second_order_raw)\n",
    "        result_vector.append(fourth_order_log)\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        'Participant': [group['Participant'].iloc[0]],\n",
    "        'Gesture_ID': [group['Gesture_ID'].iloc[0]],\n",
    "        'Gesture_Num': [group['Gesture_Num'].iloc[0]],\n",
    "        'feature': [np.array(result_vector)]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f868798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = EMG_stand.groupby(['Participant', 'Gesture_ID', 'Gesture_Num']).apply(create_feature_vectors)\n",
    "#output is df with particpant, gesture_ID, gesture_num and feature (holds 80 len vector)\n",
    "#result = result.reset_index(drop=True)\n",
    "\n",
    "#userdef = EMG_pers.groupby(['Participant', 'Gesture_ID', 'Gesture_Num']).apply(create_feature_vectors)\n",
    "#output is df with particpant, gesture_ID, gesture_num and feature (holds 80 len vector)\n",
    "#userdef = userdef.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6132f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "userdef = df1.groupby(['Participant', 'Gesture_ID', 'Gesture_Num']).apply(create_feature_vectors)\n",
    "\n",
    "#output is df with particpant, gesture_ID, gesture_num and feature (holds 80 len vector)\n",
    "userdef = userdef.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2c12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>1</td>\n",
       "      <td>[[6.079045311063784], [-7.551458873254243], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>10</td>\n",
       "      <td>[[5.994789910363704], [-7.978871468164499], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>2</td>\n",
       "      <td>[[6.010193380499154], [-7.7063875553339], [-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>3</td>\n",
       "      <td>[[5.8212078257286874], [-7.463908156909893], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>4</td>\n",
       "      <td>[[5.974675085061773], [-7.945111601415482], [-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant Gesture_ID Gesture_Num   \n",
       "0        P004      close           1  \\\n",
       "1        P004      close          10   \n",
       "2        P004      close           2   \n",
       "3        P004      close           3   \n",
       "4        P004      close           4   \n",
       "\n",
       "                                             feature  \n",
       "0  [[6.079045311063784], [-7.551458873254243], [-...  \n",
       "1  [[5.994789910363704], [-7.978871468164499], [-...  \n",
       "2  [[6.010193380499154], [-7.7063875553339], [-20...  \n",
       "3  [[5.8212078257286874], [-7.463908156909893], [...  \n",
       "4  [[5.974675085061773], [-7.945111601415482], [-...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(userdef.shape)\n",
    "userdef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f3d6a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts for Participant:\n",
      "Participant\n",
      "P004    100\n",
      "P005    100\n",
      "P131    100\n",
      "P128    100\n",
      "P127    100\n",
      "P126    100\n",
      "P125    100\n",
      "P124    100\n",
      "P123    100\n",
      "P122    100\n",
      "P121    100\n",
      "P119    100\n",
      "P118    100\n",
      "P116    100\n",
      "P115    100\n",
      "P114    100\n",
      "P112    100\n",
      "P111    100\n",
      "P110    100\n",
      "P109    100\n",
      "P108    100\n",
      "P107    100\n",
      "P106    100\n",
      "P105    100\n",
      "P104    100\n",
      "P103    100\n",
      "P102    100\n",
      "P011    100\n",
      "P010    100\n",
      "P008    100\n",
      "P006    100\n",
      "P132    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Counts for Gesture_ID:\n",
      "Gesture_ID\n",
      "close            320\n",
      "delete           320\n",
      "duplicate        320\n",
      "move             320\n",
      "open             320\n",
      "pan              320\n",
      "rotate           320\n",
      "select-single    320\n",
      "zoom-in          320\n",
      "zoom-out         320\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Counts for Gesture_Num:\n",
      "Gesture_Num\n",
      "1     320\n",
      "10    320\n",
      "2     320\n",
      "3     320\n",
      "4     320\n",
      "5     320\n",
      "6     320\n",
      "7     320\n",
      "8     320\n",
      "9     320\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in ['Participant', 'Gesture_ID', 'Gesture_Num']:\n",
    "    print(f\"\\nCounts for {column}:\")\n",
    "    counts = userdef[column].value_counts()\n",
    "    print(counts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "635ee7e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#confirmed that all vectors are the same length (80)\n",
    "for i in range(40):\n",
    "    assert(len(userdef['feature'].loc[i]) == 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9f5bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>1</td>\n",
       "      <td>[[6.079045311063784], [-7.551458873254243], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>10</td>\n",
       "      <td>[[5.994789910363704], [-7.978871468164499], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>2</td>\n",
       "      <td>[[6.010193380499154], [-7.7063875553339], [-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>3</td>\n",
       "      <td>[[5.8212078257286874], [-7.463908156909893], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>4</td>\n",
       "      <td>[[5.974675085061773], [-7.945111601415482], [-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>5</td>\n",
       "      <td>[[1.8941044559494014], [-4.088877765699611], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>6</td>\n",
       "      <td>[[2.3768635651671475], [-4.333723378142736], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>7</td>\n",
       "      <td>[[2.9880460561483497], [-4.894965494204392], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>8</td>\n",
       "      <td>[[1.9855560622439246], [-3.4775971454624637], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>9</td>\n",
       "      <td>[[2.8477571671820128], [-4.749197002681234], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant Gesture_ID Gesture_Num   \n",
       "0           P004      close           1  \\\n",
       "1           P004      close          10   \n",
       "2           P004      close           2   \n",
       "3           P004      close           3   \n",
       "4           P004      close           4   \n",
       "...          ...        ...         ...   \n",
       "3195        P132   zoom-out           5   \n",
       "3196        P132   zoom-out           6   \n",
       "3197        P132   zoom-out           7   \n",
       "3198        P132   zoom-out           8   \n",
       "3199        P132   zoom-out           9   \n",
       "\n",
       "                                                feature  \n",
       "0     [[6.079045311063784], [-7.551458873254243], [-...  \n",
       "1     [[5.994789910363704], [-7.978871468164499], [-...  \n",
       "2     [[6.010193380499154], [-7.7063875553339], [-20...  \n",
       "3     [[5.8212078257286874], [-7.463908156909893], [...  \n",
       "4     [[5.974675085061773], [-7.945111601415482], [-...  \n",
       "...                                                 ...  \n",
       "3195  [[1.8941044559494014], [-4.088877765699611], [...  \n",
       "3196  [[2.3768635651671475], [-4.333723378142736], [...  \n",
       "3197  [[2.9880460561483497], [-4.894965494204392], [...  \n",
       "3198  [[1.9855560622439246], [-3.4775971454624637], ...  \n",
       "3199  [[2.8477571671820128], [-4.749197002681234], [...  \n",
       "\n",
       "[3200 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4d3ae",
   "metadata": {},
   "source": [
    "# CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0ec8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Gesture_ID to numerical with new Gesture_Encoded column\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "#result['Gesture_Encoded'] = label_encoder.fit_transform(result['Gesture_ID'])\n",
    "userdef['Gesture_Encoded'] = label_encoder.fit_transform(userdef['Gesture_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e7dfcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Participant\n",
       "P004    100\n",
       "P005    100\n",
       "P131    100\n",
       "P128    100\n",
       "P127    100\n",
       "P126    100\n",
       "P125    100\n",
       "P124    100\n",
       "P123    100\n",
       "P122    100\n",
       "P121    100\n",
       "P119    100\n",
       "P118    100\n",
       "P116    100\n",
       "P115    100\n",
       "P114    100\n",
       "P112    100\n",
       "P111    100\n",
       "P110    100\n",
       "P109    100\n",
       "P108    100\n",
       "P107    100\n",
       "P106    100\n",
       "P105    100\n",
       "P104    100\n",
       "P103    100\n",
       "P102    100\n",
       "P011    100\n",
       "P010    100\n",
       "P008    100\n",
       "P006    100\n",
       "P132    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of rows per PID\n",
    "#result['Participant'].value_counts()\n",
    "userdef['Participant'].value_counts() #yay! all user-def are same length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82caad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep participants with 50 rows (disabled and did all gestures)\n",
    "#participant_counts = result['Participant'].value_counts()\n",
    "\n",
    "# Filter participants who have exactly 50 rows\n",
    "#with_disability = result[result['Participant'].isin(participant_counts[participant_counts == 50].index)].copy()\n",
    "\n",
    "# Display the filtered dataset\n",
    "#with_disability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438cbca",
   "metadata": {},
   "source": [
    "CCA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4895a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out one participant as the expert\n",
    "def hold_out_expert(data, participant_column):\n",
    "    participant_ids = data[participant_column].unique()\n",
    "    expert_user = np.random.choice(participant_ids)  # Randomly select one participant as expert\n",
    "    data_expert = data[data[participant_column] == expert_user]  # Expert data\n",
    "    data_remaining = data[data[participant_column] != expert_user]  # Remaining data\n",
    "    return data_expert, data_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ba2d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for remaining data\n",
    "def split_train_test(data, participant_column, test_size=0.2):\n",
    "    participant_ids = data[participant_column].unique()\n",
    "    train_ids, test_ids = train_test_split(participant_ids, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_data = data[data[participant_column].isin(train_ids)]\n",
    "    test_data = data[data[participant_column].isin(test_ids)]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f24a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_features(feature_column):\n",
    "    # Convert a column of lists of lists into a 2D NumPy array\n",
    "    return np.array([np.array(item).flatten() for item in feature_column])\n",
    "\n",
    "#X_expert = flatten_features(expert_data['feature']) #now  a 2D array with \n",
    "#X_other = flatten_features(remaining_data['feature'])\n",
    "\n",
    "#print(X_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae078f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside k_fold function\n",
    "# Apply CCA btwn expert and train/test\n",
    "def apply_cca_between_expert_and_others(expert_data, other_data, target_column, n_components=2):\n",
    "    # Separate features (X) and target (y)\n",
    "    \n",
    "    #after flattening to 2D \n",
    "    X_expert = flatten_features(expert_data['feature'])\n",
    "    X_other = flatten_features(other_data['feature'])\n",
    "\n",
    "    y_expert = expert_data[target_column]\n",
    "    y_other = other_data[target_column]\n",
    "    \n",
    "    # Apply CCA between expert and other participant\n",
    "    cca = CCA(n_components=n_components)\n",
    "    #cca.fit(X_expert, y_expert)\n",
    "    #print(X_expert.shape)\n",
    "    #print(X_other.shape)\n",
    "    cca.fit(X_expert, X_other) #might not run bcuz diff shapes\n",
    "    #need to figure out how to pass in X_others. if not clear from paper, google cca github and try to find expert user and others\n",
    "    X_other_cca = cca.transform(X_other)\n",
    "    \n",
    "    return X_other_cca, y_other#, X_expert, X_other#, X_other, X_expert, y_expert            think we're good to leave this out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "920c2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside k_fold function\n",
    "# Run KNN -- used inside cca but can also be used outside\n",
    "def run_knn(X_train, y_train, X_test, y_test, n_neighbors=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c48ebe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement k-fold cross-validation and refer to each of the other functions to run CCA between expert user and training and expert user and testing\n",
    "def k_fold_cross_validation(data, expert_data, participant_column, target_column, k=5, n_neighbors=5, n_components=2):\n",
    "\n",
    "    #kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    accuracies = []\n",
    "    \n",
    "    #80/20 train test split. 4 folds in train and 1 in test\n",
    "    for train_index, test_index in kf.split(data[participant_column].unique()):\n",
    "        # Split based on participant IDs\n",
    "        train_ids = data[participant_column].unique()[train_index]\n",
    "        test_ids = data[participant_column].unique()[test_index]\n",
    "        \n",
    "        train_data = data[data[participant_column].isin(train_ids)]\n",
    "        test_data = data[data[participant_column].isin(test_ids)]\n",
    "        \n",
    "        # CCA between expert and training users\n",
    "        X_train_cca = []\n",
    "        y_train = []\n",
    "        for pid in train_ids:\n",
    "            pid_data = train_data[train_data[participant_column] == pid]\n",
    "            #print(\"participant id\", pid)\n",
    "            #print(\"expert data\", expert_data.shape)\n",
    "            #print(\"participant data\", pid_data.shape)\n",
    "            X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "            X_train_cca.append(X_pid_cca)\n",
    "            y_train.append(y_pid)\n",
    "        \n",
    "        # Combine all CCA-transformed training data\n",
    "        X_train_cca = np.vstack(X_train_cca)\n",
    "        y_train = np.concatenate(y_train)\n",
    "        \n",
    "        # CCA between expert and testing users\n",
    "        X_test_cca = []\n",
    "        y_test = []\n",
    "        for pid in test_ids:\n",
    "            pid_data = test_data[test_data[participant_column] == pid]\n",
    "            #change to X_expert, X_other for output     X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "            X_test_cca.append(X_pid_cca)\n",
    "            y_test.append(y_pid)\n",
    "        \n",
    "        # Combine all CCA-transformed testing data\n",
    "        X_test_cca = np.vstack(X_test_cca)\n",
    "        y_test = np.concatenate(y_test)\n",
    "        \n",
    "        # Run KNN on CCA-transformed feature sets\n",
    "        accuracy = run_knn(X_train_cca, y_train, X_test_cca, y_test, n_neighbors)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4060da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expert_data, remaining_data = hold_out_expert(with_disability, 'Participant')\n",
    "#expert_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f11ed680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Def Mean accuracies per participant: {'P125': 0.36999999999999994, 'P118': 0.35, 'P132': 0.3, 'P105': 0.37, 'P123': 0.4, 'P106': 0.35, 'P111': 0.43}\n"
     ]
    }
   ],
   "source": [
    "#11/11 output with k-fold and averaging for 100 runs\n",
    "\n",
    "#train_data, test_data = split_train_test(with_disability, 'Participant', test_size=0.2)\n",
    "#In CCA, the number of components must be less than or equal to the number of features in either dataset or the number of samples minus one, whichever is smaller.\n",
    "#so changed n_components to 1\n",
    "#standard_rep = {}\n",
    "#for i in range(100):\n",
    "#    expert_data, remaining_data = hold_out_expert(with_disability, 'Participant')\n",
    "#    cca_knn_accuracy=k_fold_cross_validation(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', k=5, n_neighbors=5, n_components=2)\n",
    "#    \n",
    "#    #get mean accuracy for each expert_id\n",
    "#    participants = expert_data['Participant'].unique()\n",
    "#    for participant, accuracy in zip(participants, cca_knn_accuracy):\n",
    "#        if participant not in standard_rep:\n",
    "#            standard_rep[participant] = []\n",
    "#        standard_rep[participant].append(accuracy)\n",
    "#\n",
    "## Calculate mean accuracy per participant\n",
    "#mean_accuracies_per_participant = {participant: np.mean(accuracies) for participant, accuracies in standard_rep.items()}\n",
    "#\n",
    "#print(\"Standardized Mean accuracies per participant:\", mean_accuracies_per_participant)\n",
    "\n",
    "\n",
    "#train_data, test_data = split_train_test(userdef, 'Participant', test_size=0.2)\n",
    "#In CCA, the number of components must be less than or equal to the number of features in either dataset or the number of samples minus one, whichever is smaller.\n",
    "#so changed n_components to 1\n",
    "userdef_rep = {}\n",
    "for i in range(10):\n",
    "    expert_data, remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "    cca_knn_accuracy=k_fold_cross_validation(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', k=5, n_neighbors=5, n_components=2)\n",
    "    \n",
    "    #get mean accuracy for each expert_id\n",
    "    participants = expert_data['Participant'].unique()\n",
    "    for participant, accuracy in zip(participants, cca_knn_accuracy):\n",
    "        if participant not in userdef_rep:\n",
    "            userdef_rep[participant] = []\n",
    "        userdef_rep[participant].append(accuracy)\n",
    "\n",
    "# Calculate mean accuracy per participant\n",
    "mean_accuracies_per_participant = {participant: np.mean(accuracies) for participant, accuracies in userdef_rep.items()}\n",
    "\n",
    "print(\"User-Def Mean accuracies per participant:\", mean_accuracies_per_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "090b738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11/11 without k-fold\n",
    "# KAI: Not interested in non-kfold at the moment\n",
    "\n",
    "'''\n",
    "def apply_cca_and_knn(data, expert_data, participant_column, target_column, test_size=0.2, n_neighbors=5, n_components=2):\n",
    "    # Split the remaining data into train and test sets using the provided function\n",
    "    train_data, test_data = split_train_test(data, participant_column, test_size=test_size)\n",
    "    #print(\"train data shape\", train_data.shape)\n",
    "    #print(\"test data shape\", test_data.shape)\n",
    "\n",
    "    \n",
    "    # Apply CCA individually for each training user with the expert\n",
    "    X_train_cca = []\n",
    "    y_train = []\n",
    "    for pid in train_data[participant_column].unique():\n",
    "        pid_data = train_data[train_data[participant_column] == pid]\n",
    "        # Individual CCA model and transformation for each training participant\n",
    "        X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "        X_train_cca.append(X_pid_cca)\n",
    "        y_train.append(y_pid)\n",
    "    \n",
    "    # Combine all CCA-transformed training data\n",
    "    X_train_cca = np.vstack(X_train_cca)  # Combining individual transformations\n",
    "    y_train = np.concatenate(y_train)\n",
    "    \n",
    "    # Apply CCA individually for each testing user with the expert\n",
    "    X_test_cca = []\n",
    "    y_test = []\n",
    "    for pid in test_data[participant_column].unique():\n",
    "        pid_data = test_data[test_data[participant_column] == pid]\n",
    "        # Individual CCA model and transformation for each testing participant\n",
    "        X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "        X_test_cca.append(X_pid_cca)\n",
    "        y_test.append(y_pid)\n",
    "    \n",
    "    # Combine all CCA-transformed testing data\n",
    "    X_test_cca = np.vstack(X_test_cca)  # Combining individual transformations\n",
    "    y_test = np.concatenate(y_test)\n",
    "    \n",
    "    # Run KNN on CCA-transformed feature sets\n",
    "    accuracy = run_knn(X_train_cca, y_train, X_test_cca, y_test, n_neighbors)\n",
    "    \n",
    "    return accuracy\n",
    "#expert_data, remaining_data = hold_out_expert(with_disability, 'Participant')\n",
    "#In CCA, the number of components must be less than or equal to the number of features in either dataset or the number of samples minus one, whichever is smaller.\n",
    "#so changed n_components to 1\n",
    "#cca_knn_accuracy=apply_cca_and_knn(remaining_data, expert_data, 'Participant', 'Gesture_Encoded')\n",
    "#\n",
    "'''\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d63279ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#running cca with no k-fold and averaging for 100 runs\n",
    "# KAI: Not interested in non-kfold at the moment\n",
    "\n",
    "'''\n",
    "standard_nokfold = {}  # Dictionary to store accuracies per participant\n",
    "\n",
    "for i in range(100):\n",
    "    expert_data, remaining_data = hold_out_expert(with_disability, 'Participant')\n",
    "\n",
    "    \n",
    "    # Apply CCA and KNN, assuming 'apply_cca_and_knn' returns accuracy for this iteration\n",
    "    accuracy = apply_cca_and_knn(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', n_components=1)\n",
    "    \n",
    "    # Add the accuracy to the corresponding participant's list\n",
    "    for pid in expert_data['Participant'].unique():\n",
    "        if pid not in standard_nokfold:\n",
    "            standard_nokfold[pid] = []\n",
    "            standard_nokfold[pid].append(accuracy)\n",
    "\n",
    "# Calculate the mean accuracy for each participant across all iterations\n",
    "mean_accuracies_per_participant = {pid: np.mean(accs) for pid, accs in standard_nokfold.items()}\n",
    "\n",
    "print(\" No KNN Standardized Mean accuracies per participant:\", mean_accuracies_per_participant)\n",
    "\n",
    "\n",
    "userdef_nokfold = {}\n",
    "for i in range(100):\n",
    "    expert_data, remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "#train_data, test_data = split_train_test(remaining_data, 'Participant', test_size=0.2)\n",
    "#In CCA, the number of components must be less than or equal to the number of features in either dataset or the number of samples minus one, whichever is smaller.\n",
    "#so changed n_components to 1\n",
    "    accuracy=apply_cca_and_knn(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', n_neighbors=5, n_components=2)\n",
    "    #get mean accuracy for each expert_id\n",
    "    for pid in expert_data['Participant'].unique():\n",
    "        if pid not in userdef_nokfold:\n",
    "            userdef_nokfold[pid] = []\n",
    "            userdef_nokfold[pid].append(accuracy)\n",
    "\n",
    "mean_accuracies_per_participant = {pid: np.mean(accs) for pid, accs in userdef_nokfold.items()}\n",
    "\n",
    "print(\"No KNN User-Def Mean accuracies per participant:\", mean_accuracies_per_participant)\n",
    "\n",
    "#Lower accuracy without k-fold\n",
    "'''\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ffa41dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user-def\n",
      "PCA Explained Variance Ratio: [0.31883444 0.20147262 0.07229554 0.05143604 0.04524832 0.03273017\n",
      " 0.02969741 0.02437628 0.02217808 0.01722988]\n",
      "KNN Classification Accuracy: 0.7578125\n"
     ]
    }
   ],
   "source": [
    "#baseline PCA\n",
    "def run_pca_knn(df, n_components=8, n_neighbors=5, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Perform PCA on the 'feature' column and KNN classification on the transformed data.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing 'feature' and 'Gesture_ID' columns.\n",
    "        n_components (int): Number of PCA components.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        test_size (float): Fraction of data to use as the test set.\n",
    "        random_state (int): Random state for train-test split.\n",
    "\n",
    "    Returns:\n",
    "        explained_variance_ratio (array): Explained variance ratio of PCA components.\n",
    "        accuracy (float): Classification accuracy of KNN on the test set.\n",
    "    \"\"\"\n",
    "    # Convert the 'feature' column from nested lists to a flat list for each entry\n",
    "    df['feature_flat'] = df['feature'].apply(lambda x: np.ravel(x))\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = np.vstack(df['feature_flat'].values)\n",
    "    y = df['Gesture_ID']\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=test_size)\n",
    "\n",
    "    # Train KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return pca.explained_variance_ratio_, accuracy\n",
    "\n",
    "#print(\"standardized\")\n",
    "#explained_variance_ratio, accuracy = run_pca_knn(with_disability, n_components=10, n_neighbors=5)\n",
    "#print(\"PCA Explained Variance Ratio:\", explained_variance_ratio)\n",
    "#print(\"KNN Classification Accuracy:\", accuracy)\n",
    "#print(\"\")\n",
    "print(\"user-def\") #better at user-def than standardized\n",
    "explained_variance_ratio, accuracy = run_pca_knn(userdef, n_components=10, n_neighbors=5)\n",
    "print(\"PCA Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"KNN Classification Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1823eba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: [0.84375, 0.8234375, 0.81875, 0.775, 0.8109375]\n",
      "userdef 0.8143749999999998\n"
     ]
    }
   ],
   "source": [
    "#baseline knn\n",
    "def run_knn_baseline_with_expert(expert_data, remaining_data, target_column, n_neighbors=5, k=5):\n",
    "    # Flatten the features from both datasets\n",
    "    X_expert = flatten_features(expert_data['feature'])\n",
    "    X_remaining = flatten_features(remaining_data['feature'])\n",
    "    \n",
    "    # Combine the feature sets and their corresponding targets\n",
    "    X_combined = np.vstack((X_expert, X_remaining))\n",
    "    y_expert = expert_data[target_column].values\n",
    "    y_remaining = remaining_data[target_column].values\n",
    "    y_combined = np.concatenate((y_expert, y_remaining))\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_combined):\n",
    "        X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "        y_train, y_test = y_combined[train_index], y_combined[test_index]\n",
    "\n",
    "        # Run KNN on the combined dataset\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "#expert_data, remaining_data = hold_out_expert(with_disability, 'Participant')\n",
    "#baseline_accuracies = run_knn_baseline_with_expert(expert_data, remaining_data, 'Gesture_Encoded')\n",
    "#print(f'KNN Accuracy: {baseline_accuracies}')\n",
    "#print(\"standardized\", np.mean(baseline_accuracies))\n",
    "\n",
    "pers_expert_data, pers_remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "baseline_accuracies = run_knn_baseline_with_expert(pers_expert_data, pers_remaining_data, 'Gesture_Encoded')\n",
    "print(f'KNN Accuracy: {baseline_accuracies}')\n",
    "print(\"userdef\", np.mean(baseline_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433cc73f",
   "metadata": {},
   "source": [
    "> Wow that is high accuracy... let me double check things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dabf679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>feature</th>\n",
       "      <th>Gesture_Encoded</th>\n",
       "      <th>feature_flat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>P116</td>\n",
       "      <td>close</td>\n",
       "      <td>1</td>\n",
       "      <td>[[5.527561206184626], [-8.098585875109096], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.527561206184626, -8.098585875109096, -20.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>P116</td>\n",
       "      <td>close</td>\n",
       "      <td>10</td>\n",
       "      <td>[[5.049786408181666], [-6.691335261235313], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.049786408181666, -6.691335261235313, -17.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>P116</td>\n",
       "      <td>close</td>\n",
       "      <td>2</td>\n",
       "      <td>[[5.261857313828599], [-7.718602672988598], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.261857313828599, -7.718602672988598, -18.78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>P116</td>\n",
       "      <td>close</td>\n",
       "      <td>3</td>\n",
       "      <td>[[4.6162905039179085], [-7.227502407333501], [...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4.6162905039179085, -7.227502407333501, -17.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>P116</td>\n",
       "      <td>close</td>\n",
       "      <td>4</td>\n",
       "      <td>[[6.166422454148102], [-8.883153520789653], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6.166422454148102, -8.883153520789653, -22.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>P116</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>5</td>\n",
       "      <td>[[5.030362236669886], [-7.115086323656174], [-...</td>\n",
       "      <td>9</td>\n",
       "      <td>[5.030362236669886, -7.115086323656174, -17.78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>P116</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4.509073413170462], [-6.830646649276103], [-...</td>\n",
       "      <td>9</td>\n",
       "      <td>[4.509073413170462, -6.830646649276103, -16.54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>P116</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>7</td>\n",
       "      <td>[[4.856808873256232], [-7.65270966627144], [-1...</td>\n",
       "      <td>9</td>\n",
       "      <td>[4.856808873256232, -7.65270966627144, -18.215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>P116</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>8</td>\n",
       "      <td>[[4.591157335933044], [-7.027517336850388], [-...</td>\n",
       "      <td>9</td>\n",
       "      <td>[4.591157335933044, -7.027517336850388, -16.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>P116</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>9</td>\n",
       "      <td>[[4.7364340428342695], [-6.840054308939138], [...</td>\n",
       "      <td>9</td>\n",
       "      <td>[4.7364340428342695, -6.840054308939138, -16.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant Gesture_ID Gesture_Num   \n",
       "1900        P116      close           1  \\\n",
       "1901        P116      close          10   \n",
       "1902        P116      close           2   \n",
       "1903        P116      close           3   \n",
       "1904        P116      close           4   \n",
       "...          ...        ...         ...   \n",
       "1995        P116   zoom-out           5   \n",
       "1996        P116   zoom-out           6   \n",
       "1997        P116   zoom-out           7   \n",
       "1998        P116   zoom-out           8   \n",
       "1999        P116   zoom-out           9   \n",
       "\n",
       "                                                feature  Gesture_Encoded   \n",
       "1900  [[5.527561206184626], [-8.098585875109096], [-...                0  \\\n",
       "1901  [[5.049786408181666], [-6.691335261235313], [-...                0   \n",
       "1902  [[5.261857313828599], [-7.718602672988598], [-...                0   \n",
       "1903  [[4.6162905039179085], [-7.227502407333501], [...                0   \n",
       "1904  [[6.166422454148102], [-8.883153520789653], [-...                0   \n",
       "...                                                 ...              ...   \n",
       "1995  [[5.030362236669886], [-7.115086323656174], [-...                9   \n",
       "1996  [[4.509073413170462], [-6.830646649276103], [-...                9   \n",
       "1997  [[4.856808873256232], [-7.65270966627144], [-1...                9   \n",
       "1998  [[4.591157335933044], [-7.027517336850388], [-...                9   \n",
       "1999  [[4.7364340428342695], [-6.840054308939138], [...                9   \n",
       "\n",
       "                                           feature_flat  \n",
       "1900  [5.527561206184626, -8.098585875109096, -20.00...  \n",
       "1901  [5.049786408181666, -6.691335261235313, -17.23...  \n",
       "1902  [5.261857313828599, -7.718602672988598, -18.78...  \n",
       "1903  [4.6162905039179085, -7.227502407333501, -17.1...  \n",
       "1904  [6.166422454148102, -8.883153520789653, -22.46...  \n",
       "...                                                 ...  \n",
       "1995  [5.030362236669886, -7.115086323656174, -17.78...  \n",
       "1996  [4.509073413170462, -6.830646649276103, -16.54...  \n",
       "1997  [4.856808873256232, -7.65270966627144, -18.215...  \n",
       "1998  [4.591157335933044, -7.027517336850388, -16.81...  \n",
       "1999  [4.7364340428342695, -6.840054308939138, -16.7...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pers_expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e52a134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Gesture_ID</th>\n",
       "      <th>Gesture_Num</th>\n",
       "      <th>feature</th>\n",
       "      <th>Gesture_Encoded</th>\n",
       "      <th>feature_flat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>1</td>\n",
       "      <td>[[6.079045311063784], [-7.551458873254243], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6.079045311063784, -7.551458873254243, -20.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>10</td>\n",
       "      <td>[[5.994789910363704], [-7.978871468164499], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.994789910363704, -7.978871468164499, -20.77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>2</td>\n",
       "      <td>[[6.010193380499154], [-7.7063875553339], [-20...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6.010193380499154, -7.7063875553339, -20.4852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>3</td>\n",
       "      <td>[[5.8212078257286874], [-7.463908156909893], [...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.8212078257286874, -7.463908156909893, -19.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P004</td>\n",
       "      <td>close</td>\n",
       "      <td>4</td>\n",
       "      <td>[[5.974675085061773], [-7.945111601415482], [-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.974675085061773, -7.945111601415482, -20.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>5</td>\n",
       "      <td>[[1.8941044559494014], [-4.088877765699611], [...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1.8941044559494014, -4.088877765699611, -8.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>6</td>\n",
       "      <td>[[2.3768635651671475], [-4.333723378142736], [...</td>\n",
       "      <td>9</td>\n",
       "      <td>[2.3768635651671475, -4.333723378142736, -9.65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>7</td>\n",
       "      <td>[[2.9880460561483497], [-4.894965494204392], [...</td>\n",
       "      <td>9</td>\n",
       "      <td>[2.9880460561483497, -4.894965494204392, -11.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>8</td>\n",
       "      <td>[[1.9855560622439246], [-3.4775971454624637], ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[1.9855560622439246, -3.4775971454624637, -8.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>P132</td>\n",
       "      <td>zoom-out</td>\n",
       "      <td>9</td>\n",
       "      <td>[[2.8477571671820128], [-4.749197002681234], [...</td>\n",
       "      <td>9</td>\n",
       "      <td>[2.8477571671820128, -4.749197002681234, -11.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant Gesture_ID Gesture_Num   \n",
       "0           P004      close           1  \\\n",
       "1           P004      close          10   \n",
       "2           P004      close           2   \n",
       "3           P004      close           3   \n",
       "4           P004      close           4   \n",
       "...          ...        ...         ...   \n",
       "3195        P132   zoom-out           5   \n",
       "3196        P132   zoom-out           6   \n",
       "3197        P132   zoom-out           7   \n",
       "3198        P132   zoom-out           8   \n",
       "3199        P132   zoom-out           9   \n",
       "\n",
       "                                                feature  Gesture_Encoded   \n",
       "0     [[6.079045311063784], [-7.551458873254243], [-...                0  \\\n",
       "1     [[5.994789910363704], [-7.978871468164499], [-...                0   \n",
       "2     [[6.010193380499154], [-7.7063875553339], [-20...                0   \n",
       "3     [[5.8212078257286874], [-7.463908156909893], [...                0   \n",
       "4     [[5.974675085061773], [-7.945111601415482], [-...                0   \n",
       "...                                                 ...              ...   \n",
       "3195  [[1.8941044559494014], [-4.088877765699611], [...                9   \n",
       "3196  [[2.3768635651671475], [-4.333723378142736], [...                9   \n",
       "3197  [[2.9880460561483497], [-4.894965494204392], [...                9   \n",
       "3198  [[1.9855560622439246], [-3.4775971454624637], ...                9   \n",
       "3199  [[2.8477571671820128], [-4.749197002681234], [...                9   \n",
       "\n",
       "                                           feature_flat  \n",
       "0     [6.079045311063784, -7.551458873254243, -20.17...  \n",
       "1     [5.994789910363704, -7.978871468164499, -20.77...  \n",
       "2     [6.010193380499154, -7.7063875553339, -20.4852...  \n",
       "3     [5.8212078257286874, -7.463908156909893, -19.6...  \n",
       "4     [5.974675085061773, -7.945111601415482, -20.66...  \n",
       "...                                                 ...  \n",
       "3195  [1.8941044559494014, -4.088877765699611, -8.69...  \n",
       "3196  [2.3768635651671475, -4.333723378142736, -9.65...  \n",
       "3197  [2.9880460561483497, -4.894965494204392, -11.6...  \n",
       "3198  [1.9855560622439246, -3.4775971454624637, -8.0...  \n",
       "3199  [2.8477571671820128, -4.749197002681234, -11.2...  \n",
       "\n",
       "[3100 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pers_remaining_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fced9b6",
   "metadata": {},
   "source": [
    "## Taking it out of the function to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a856a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840625\n",
      "0.8015625\n",
      "0.834375\n",
      "0.8\n",
      "0.8078125\n"
     ]
    }
   ],
   "source": [
    "#baseline knn\n",
    "\n",
    "#pers_expert_data, pers_remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "#expert_data = pers_expert_data\n",
    "#remaining_data = pers_remaining_data\n",
    "target_column = 'Gesture_Encoded'\n",
    "n_neighbors = 5\n",
    "k = 5\n",
    "\n",
    "# Flatten the features from both datasets\n",
    "#X_expert = flatten_features(expert_data['feature_flat'])\n",
    "#X_remaining = flatten_features(remaining_data['feature_flat'])\n",
    "\n",
    "# Combine the feature sets and their corresponding targets\n",
    "X_combined = userdef['feature_flat']\n",
    "#y_expert = expert_data[target_column].values\n",
    "#y_remaining = remaining_data[target_column].values\n",
    "y_combined = userdef[target_column]\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X_combined):\n",
    "    X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "    y_train, y_test = y_combined[train_index], y_combined[test_index]\n",
    "    \n",
    "    X_train = np.array(X_train.tolist()) if isinstance(X_train, pd.Series) else np.array(X_train)\n",
    "    y_train = np.array(y_train.tolist()) if isinstance(y_train, pd.Series) else np.array(y_train)\n",
    "    X_test = np.array(X_test.tolist()) if isinstance(X_test, pd.Series) else np.array(X_test)\n",
    "    y_test = np.array(y_test.tolist()) if isinstance(y_test, pd.Series) else np.array(y_test)\n",
    "\n",
    "    # Run KNN on the combined dataset\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(accuracy)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "#return accuracies\n",
    "\n",
    "#baseline_accuracies = run_knn_baseline_with_expert(pers_expert_data, pers_remaining_data, 'Gesture_Encoded')\n",
    "#print(f'KNN Accuracy: {baseline_accuracies}')\n",
    "#print(\"userdef\", np.mean(baseline_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf36401",
   "metadata": {},
   "source": [
    "## Turning shuffle off\n",
    "> Idea: Shuffle could be turning this from a cross-subject to an intra-subject scenario. Eg, when shuffled, the model may get to train over ALL users data, and then when it is tested, it is NOT tested on new, novel users (as would be the case in the cross-subject scenario) but rather it is tested on new gesture trials from users (and other gesture trials from the same user from the same gesture) already in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbd1f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1246485473289597\n",
      "0.11996251171508904\n",
      "0.10318949343339587\n"
     ]
    }
   ],
   "source": [
    "#baseline knn\n",
    "\n",
    "#pers_expert_data, pers_remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "#expert_data = pers_expert_data\n",
    "#remaining_data = pers_remaining_data\n",
    "target_column = 'Gesture_Encoded'\n",
    "n_neighbors = 5\n",
    "k = 3\n",
    "\n",
    "# Flatten the features from both datasets\n",
    "#X_expert = flatten_features(expert_data['feature_flat'])\n",
    "#X_remaining = flatten_features(remaining_data['feature_flat'])\n",
    "\n",
    "# Combine the feature sets and their corresponding targets\n",
    "X_combined = userdef['feature_flat']\n",
    "#y_expert = expert_data[target_column].values\n",
    "#y_remaining = remaining_data[target_column].values\n",
    "y_combined = userdef[target_column]\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=False)#True, random_state=42)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X_combined):\n",
    "    X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "    y_train, y_test = y_combined[train_index], y_combined[test_index]\n",
    "    \n",
    "    X_train = np.array(X_train.tolist()) if isinstance(X_train, pd.Series) else np.array(X_train)\n",
    "    y_train = np.array(y_train.tolist()) if isinstance(y_train, pd.Series) else np.array(y_train)\n",
    "    X_test = np.array(X_test.tolist()) if isinstance(X_test, pd.Series) else np.array(X_test)\n",
    "    y_test = np.array(y_test.tolist()) if isinstance(y_test, pd.Series) else np.array(y_test)\n",
    "\n",
    "    # Run KNN on the combined dataset\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(accuracy)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "#return accuracies\n",
    "\n",
    "#baseline_accuracies = run_knn_baseline_with_expert(pers_expert_data, pers_remaining_data, 'Gesture_Encoded')\n",
    "#print(f'KNN Accuracy: {baseline_accuracies}')\n",
    "#print(\"userdef\", np.mean(baseline_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c43d2c",
   "metadata": {},
   "source": [
    "> That appears to be correct! Just turning shuffle off drops the accuracy considerably\n",
    "\n",
    "## Explicitly holding out certain users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aff9e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users1 = ['P125', 'P126', 'P127', 'P128', 'P131', 'P132']\n",
    "test_users2 = ['P004', 'P005', 'P006', 'P008', 'P010', 'P011']  # All users without disabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69f36395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07833333333333334\n"
     ]
    }
   ],
   "source": [
    "#baseline knn\n",
    "\n",
    "test_users = test_users1\n",
    "\n",
    "#pers_expert_data, pers_remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "#expert_data = pers_expert_data\n",
    "#remaining_data = pers_remaining_data\n",
    "target_column = 'Gesture_Encoded'\n",
    "n_neighbors = 5\n",
    "k = 5\n",
    "\n",
    "train_data = userdef[~userdef['Participant'].isin(test_users)]\n",
    "test_data = userdef[userdef['Participant'].isin(test_users)]\n",
    "X_train = train_data['feature_flat']\n",
    "y_train = train_data[target_column]\n",
    "X_test = test_data['feature_flat']\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "X_train = np.array(X_train.tolist()) if isinstance(X_train, pd.Series) else np.array(X_train)\n",
    "y_train = np.array(y_train.tolist()) if isinstance(y_train, pd.Series) else np.array(y_train)\n",
    "X_test = np.array(X_test.tolist()) if isinstance(X_test, pd.Series) else np.array(X_test)\n",
    "y_test = np.array(y_test.tolist()) if isinstance(y_test, pd.Series) else np.array(y_test)\n",
    "\n",
    "# Run KNN on the combined dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "#accuracies.append(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2371257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.165\n"
     ]
    }
   ],
   "source": [
    "#baseline knn\n",
    "\n",
    "test_users = test_users2\n",
    "\n",
    "#pers_expert_data, pers_remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "#expert_data = pers_expert_data\n",
    "#remaining_data = pers_remaining_data\n",
    "target_column = 'Gesture_Encoded'\n",
    "n_neighbors = 5\n",
    "k = 5\n",
    "\n",
    "train_data = userdef[~userdef['Participant'].isin(test_users)]\n",
    "test_data = userdef[userdef['Participant'].isin(test_users)]\n",
    "X_train = train_data['feature_flat']\n",
    "y_train = train_data[target_column]\n",
    "X_test = test_data['feature_flat']\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "X_train = np.array(X_train.tolist()) if isinstance(X_train, pd.Series) else np.array(X_train)\n",
    "y_train = np.array(y_train.tolist()) if isinstance(y_train, pd.Series) else np.array(y_train)\n",
    "X_test = np.array(X_test.tolist()) if isinstance(X_test, pd.Series) else np.array(X_test)\n",
    "y_test = np.array(y_test.tolist()) if isinstance(y_test, pd.Series) else np.array(y_test)\n",
    "\n",
    "# Run KNN on the combined dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "#accuracies.append(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a2a86",
   "metadata": {},
   "source": [
    "Interesting that it improves so much when you test on the users without disabilities... but the accuracy is still pretty poor  \n",
    "\n",
    "> Kai's conclusion: the way the training split is currently conducted (random shuffle on the k folds) results in the testing scenario being more akin to the intra-subject case (eg you have already trained on examples from all users) as opposed to our desired cross-subject case (new users join the network and we havent seen any of their data yet), thus we are testing on data that is well represented in the training data, instead of testing on novel user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b81c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e699e92",
   "metadata": {},
   "source": [
    "## Checking Kaylah's Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba33cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_user = 'P118'\n",
    "test_users = ['P116', 'P132']\n",
    "train_users = ['P104', 'P111', 'P114', 'P119', 'P123', 'P124', 'P126']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29c42313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13\n"
     ]
    }
   ],
   "source": [
    "#baseline knn\n",
    "\n",
    "#pers_expert_data, pers_remaining_data = hold_out_expert(userdef, 'Participant')\n",
    "#expert_data = pers_expert_data\n",
    "#remaining_data = pers_remaining_data\n",
    "target_column = 'Gesture_Encoded'\n",
    "n_neighbors = 5\n",
    "k = 5\n",
    "\n",
    "train_data = userdef[userdef['Participant'].isin(train_users)]\n",
    "test_data = userdef[userdef['Participant'].isin(test_users)]\n",
    "X_train = train_data['feature_flat']\n",
    "y_train = train_data[target_column]\n",
    "X_test = test_data['feature_flat']\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "X_train = np.array(X_train.tolist()) if isinstance(X_train, pd.Series) else np.array(X_train)\n",
    "y_train = np.array(y_train.tolist()) if isinstance(y_train, pd.Series) else np.array(y_train)\n",
    "X_test = np.array(X_test.tolist()) if isinstance(X_test, pd.Series) else np.array(X_test)\n",
    "y_test = np.array(y_test.tolist()) if isinstance(y_test, pd.Series) else np.array(y_test)\n",
    "\n",
    "# Run KNN on the combined dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "#accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88524360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P004', 'P005', 'P006', 'P008', 'P010', 'P011', 'P102', 'P103',\n",
       "       'P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P110', 'P111',\n",
       "       'P112', 'P114', 'P115', 'P116', 'P118', 'P119', 'P121', 'P122',\n",
       "       'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P131', 'P132'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdef['Participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5042f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(userdef['Participant'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7edf1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_disability = ['P102', 'P103',\n",
    "       'P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P110', 'P111',\n",
    "       'P112', 'P114', 'P115', 'P116', 'P118', 'P119', 'P121', 'P122', \n",
    "       'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P131', 'P132']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "46c191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Def Mean accuracies per participant: {'P118': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# CCA\n",
    "\n",
    "userdef_rep = {}\n",
    "# = hold_out_expert(c, 'Participant')\n",
    "expert_data = userdef[userdef['Participant']==expert_user]\n",
    "remaining_data = userdef[userdef['Participant']!=expert_user]\n",
    "cca_knn_accuracy=k_fold_cross_validation(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', k=5, n_neighbors=5, n_components=2)\n",
    "\n",
    "#get mean accuracy for each expert_id\n",
    "participants = expert_data['Participant'].unique()\n",
    "for participant, accuracy in zip(participants, cca_knn_accuracy):\n",
    "    if participant not in userdef_rep:\n",
    "        userdef_rep[participant] = []\n",
    "    userdef_rep[participant].append(accuracy)\n",
    "\n",
    "# Calculate mean accuracy per participant\n",
    "mean_accuracies_per_participant = {participant: np.mean(accuracies) for participant, accuracies in userdef_rep.items()}\n",
    "\n",
    "print(\"User-Def Mean accuracies per participant:\", mean_accuracies_per_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a2930db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47\n",
      "User-Def Mean accuracies per participant: {'P118': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# CCA\n",
    "\n",
    "userdef_rep = {}\n",
    "# = hold_out_expert(c, 'Participant')\n",
    "expert_data = userdef[userdef['Participant']==expert_user]\n",
    "remaining_data = userdef[userdef['Participant']!=expert_user]\n",
    "#cca_knn_accuracy=k_fold_cross_validation(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', k=5, n_neighbors=5, n_components=2)\n",
    "# Implement k-fold cross-validation and refer to each of the other functions to run CCA between expert user and training and expert user and testing\n",
    "#def k_fold_cross_validation(\n",
    "data = remaining_data\n",
    "expert_data = expert_data\n",
    "participant_column = 'Participant'\n",
    "target_column = 'Gesture_Encoded'\n",
    "k=5\n",
    "n_neighbors=5\n",
    "n_components=2\n",
    "\n",
    "#kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "kf = KFold(n_splits=k, shuffle=False)\n",
    "accuracies = []\n",
    "\n",
    "#80/20 train test split. 4 folds in train and 1 in test\n",
    "#for train_index, test_index in kf.split(data[participant_column].unique()):\n",
    "# Split based on participant IDs\n",
    "#train_ids = data[participant_column].unique()[train_index]\n",
    "#test_ids = data[participant_column].unique()[test_index]\n",
    "#train_data = data[data[participant_column].isin(train_ids)]\n",
    "#test_data = data[data[participant_column].isin(test_ids)]\n",
    "train_data = data[data['Participant'].isin(train_users)]\n",
    "test_data = data[data['Participant'].isin(test_users)]\n",
    "\n",
    "# CCA between expert and training users\n",
    "X_train_cca = []\n",
    "y_train = []\n",
    "for pid in train_users: #train_pids\n",
    "    pid_data = train_data[train_data[participant_column] == pid]\n",
    "    #print(\"participant id\", pid)\n",
    "    #print(\"expert data\", expert_data.shape)\n",
    "    #print(\"participant data\", pid_data.shape)\n",
    "    X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "    X_train_cca.append(X_pid_cca)\n",
    "    y_train.append(y_pid)\n",
    "\n",
    "# Combine all CCA-transformed training data\n",
    "X_train_cca = np.vstack(X_train_cca)\n",
    "y_train = np.concatenate(y_train)\n",
    "\n",
    "# CCA between expert and testing users\n",
    "X_test_cca = []\n",
    "y_test = []\n",
    "for pid in test_ids:\n",
    "    pid_data = test_data[test_data[participant_column] == pid]\n",
    "    #change to X_expert, X_other for output     X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "    X_test_cca.append(X_pid_cca)\n",
    "    y_test.append(y_pid)\n",
    "\n",
    "# Combine all CCA-transformed testing data\n",
    "X_test_cca = np.vstack(X_test_cca)\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "# Run KNN on CCA-transformed feature sets\n",
    "accuracy = run_knn(X_train_cca, y_train, X_test_cca, y_test, n_neighbors)\n",
    "accuracies.append(accuracy)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "#get mean accuracy for each expert_id\n",
    "participants = expert_data['Participant'].unique()\n",
    "for participant, accuracy in zip(participants, cca_knn_accuracy):\n",
    "    if participant not in userdef_rep:\n",
    "        userdef_rep[participant] = []\n",
    "    userdef_rep[participant].append(accuracy)\n",
    "\n",
    "# Calculate mean accuracy per participant\n",
    "mean_accuracies_per_participant = {participant: np.mean(accuracies) for participant, accuracies in userdef_rep.items()}\n",
    "\n",
    "print(\"User-Def Mean accuracies per participant:\", mean_accuracies_per_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d40cc205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fb97704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378102b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0970555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afdefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002eb11",
   "metadata": {},
   "source": [
    "## NOT USING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf79514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "(4,)\n",
      "(13,)\n",
      "(3,)\n",
      "(13,)\n",
      "(3,)\n",
      "(13,)\n",
      "(3,)\n",
      "(13,)\n",
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['P104', 'P111', 'P114', 'P116', 'P119', 'P123', 'P126', 'P131', 'P132']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the shapes of all pid and expert_data for fitting\n",
    "def k_fold_cross_validation(data, expert_data, participant_column, target_column, k=5, n_neighbors=5, n_components=2):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(data[participant_column].unique()):\n",
    "        train_ids = data[participant_column].unique()[train_index]\n",
    "        test_ids = data[participant_column].unique()[test_index]\n",
    "        print(train_ids.shape)\n",
    "        print(test_ids.shape)\n",
    "\n",
    "        train_data = data[data[participant_column].isin(train_ids)]\n",
    "        test_data = data[data[participant_column].isin(test_ids)]\n",
    "        diff_shape = []\n",
    "\n",
    "        for pid in train_ids:\n",
    "            pid_data = train_data[train_data[participant_column] == pid]\n",
    "            if expert_data.shape != pid_data.shape:\n",
    "                diff_shape.append(pid)\n",
    "    return diff_shape\n",
    "k_fold_cross_validation(remaining_data, expert_data, 'Participant', 'Gesture_Encoded', k=5, n_neighbors=5, n_components=1)\n",
    "\n",
    "#shape problem with 'P104', 'P111', 'P114', 'P116', 'P119', 'P123', 'P126', 'P131', 'P132'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78f83348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cca_knn_accuracy) #accuracy for each k-fold.... really poor (maybe because n_components is 1??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c307784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: [0.3433962264150943, 0.4666666666666667, 0.38, 0.255, 0.31]\n",
      "Average accuracy: 0.3510125786163522\n"
     ]
    }
   ],
   "source": [
    "#### don't use\n",
    "# Hold out one participant as the expert\n",
    "def hold_out_expert(data, participant_column):\n",
    "    participant_ids = data[participant_column].unique()\n",
    "    expert_user = np.random.choice(participant_ids)  # Randomly select one participant as expert\n",
    "    data_expert = data[data[participant_column] == expert_user]  # Expert data\n",
    "    data_remaining = data[data[participant_column] != expert_user]  # Remaining data\n",
    "    return data_expert, data_remaining\n",
    "\n",
    "def apply_cca_between_expert_and_others(expert_data, participant_data, target_column, n_components):\n",
    "    \"\"\"\n",
    "    Apply CCA between expert data and a participant's data.\n",
    "    \n",
    "    Parameters:\n",
    "        expert_data (DataFrame): Expert user data.\n",
    "        participant_data (DataFrame): Participant data.\n",
    "        target_column (str): Name of the target column.\n",
    "        n_components (int): Number of CCA components.\n",
    "    \n",
    "    Returns:\n",
    "        X_cca (array): CCA-transformed features for the participant data.\n",
    "        y (array): Target values for the participant data.\n",
    "    \"\"\"\n",
    "    # Extract features and target\n",
    "    X_expert = np.vstack(expert_data['feature'].apply(lambda x: np.ravel(x)).values)\n",
    "    X_participant = np.vstack(participant_data['feature'].apply(lambda x: np.ravel(x)).values)\n",
    "    y = participant_data[target_column].values\n",
    "\n",
    "    # Encode target labels for CCA\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y).reshape(-1, 1)\n",
    "\n",
    "    # Apply CCA\n",
    "    cca = CCA(n_components=n_components)\n",
    "    X_cca, _ = cca.fit_transform(X_participant, y_encoded)\n",
    "\n",
    "    return X_cca, y\n",
    "\n",
    "def run_knn(X_train, y_train, X_test, y_test, n_neighbors):\n",
    "    \"\"\"\n",
    "    Run KNN classification on the transformed data and return the accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (array): CCA-transformed training features.\n",
    "        y_train (array): Training labels.\n",
    "        X_test (array): CCA-transformed test features.\n",
    "        y_test (array): Test labels.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): Classification accuracy of KNN.\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "def k_fold_cross_validation(data, expert_data, participant_column, target_column, k=5, n_neighbors=5, n_components=2):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation using CCA between expert user data and participants,\n",
    "    and KNN classification on the transformed data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): Input DataFrame containing participants' data.\n",
    "        expert_data (DataFrame): Expert user data for CCA.\n",
    "        participant_column (str): Name of the participant column.\n",
    "        target_column (str): Name of the target column.\n",
    "        k (int): Number of folds for cross-validation.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        n_components (int): Number of CCA components.\n",
    "    \n",
    "    Returns:\n",
    "        accuracies (list): List of accuracies for each fold.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    \n",
    "    # k-fold cross-validation\n",
    "    for train_index, test_index in kf.split(data[participant_column].unique()):\n",
    "        train_ids = data[participant_column].unique()[train_index]\n",
    "        test_ids = data[participant_column].unique()[test_index]\n",
    "        \n",
    "        train_data = data[data[participant_column].isin(train_ids)]\n",
    "        test_data = data[data[participant_column].isin(test_ids)]\n",
    "        \n",
    "        # Apply CCA between expert and training participants\n",
    "        X_train_cca = []\n",
    "        y_train = []\n",
    "        for pid in train_ids:\n",
    "            pid_data = train_data[train_data[participant_column] == pid]\n",
    "            X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "            X_train_cca.append(X_pid_cca)\n",
    "            y_train.append(y_pid)\n",
    "        \n",
    "        # Combine CCA-transformed training data\n",
    "        X_train_cca = np.vstack(X_train_cca)\n",
    "        y_train = np.concatenate(y_train)\n",
    "        \n",
    "        # Apply CCA between expert and testing participants\n",
    "        X_test_cca = []\n",
    "        y_test = []\n",
    "        for pid in test_ids:\n",
    "            pid_data = test_data[test_data[participant_column] == pid]\n",
    "            X_pid_cca, y_pid = apply_cca_between_expert_and_others(expert_data, pid_data, target_column, n_components)\n",
    "            X_test_cca.append(X_pid_cca)\n",
    "            y_test.append(y_pid)\n",
    "        \n",
    "        # Combine CCA-transformed testing data\n",
    "        X_test_cca = np.vstack(X_test_cca)\n",
    "        y_test = np.concatenate(y_test)\n",
    "        \n",
    "        # Run KNN on CCA-transformed data\n",
    "        accuracy = run_knn(X_train_cca, y_train, X_test_cca, y_test, n_neighbors)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "expert_data, remaining_data = hold_out_expert(result, 'Participant')\n",
    "accuracies = k_fold_cross_validation(remaining_data, expert_data, 'Participant', 'Gesture_ID', k=5, n_neighbors=10, n_components=1)\n",
    "print(\"Cross-validation accuracies:\", accuracies)\n",
    "print(\"Average accuracy:\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f5d5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution with resampling -- didn't use for final run through\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def apply_cca_between_expert_and_others(expert_data, other_data, target_column, n_components=2):\n",
    "    # Separate features (X) and target (y)\n",
    "    X_expert = flatten_features(expert_data['feature'])\n",
    "    X_other = flatten_features(other_data['feature'])\n",
    "\n",
    "    y_expert = expert_data[target_column]\n",
    "    y_other = other_data[target_column]\n",
    "    \n",
    "    # Match the sample size between X_expert and X_other\n",
    "    X_other_resampled, y_other_resampled = resample(X_other, y_other, n_samples=X_expert.shape[0], random_state=42)\n",
    "    \n",
    "    # Apply CCA between expert and other participant\n",
    "    cca = CCA(n_components=n_components)\n",
    "    cca.fit(X_expert, X_other_resampled)\n",
    "    X_other_cca = cca.transform(X_other_resampled)\n",
    "    \n",
    "    return X_other_cca, y_other_resampled#, X_expert, X_other_resampled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to relabel freq_data to time data. and turn that to np.fft.fft to convert to time\n",
    "\n",
    "# Parseval's theorem verification function\n",
    "def parsevals_theorem_verification(df_freq):\n",
    "    time_domain_data = {}\n",
    "    freq_domain_squares_sum = {}\n",
    "    time_domain_squares_sum = {}\n",
    "\n",
    "    for sensor in df_freq.columns:\n",
    "        # Extract the time domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        \n",
    "        # FFT to transform to freq domain\n",
    "        freq_data = np.fft.fft(time_data)\n",
    "        \n",
    "        # Parseval's theorem: Sum of squares in the frequency domain\n",
    "        freq_domain_sum_sq = np.sum(np.abs(freq_data)**2)\n",
    "        \n",
    "        # Parseval's theorem: Sum of squares in the time domain\n",
    "        time_domain_sum_sq = np.sum(np.abs(time_data)**2)\n",
    "        \n",
    "        # Store results in dictionaries\n",
    "        time_domain_data[sensor] = time_data\n",
    "        freq_domain_squares_sum[sensor] = freq_domain_sum_sq\n",
    "        time_domain_squares_sum[sensor] = time_domain_sum_sq\n",
    "        \n",
    "        # Output verification for this sensor\n",
    "        print(f'Sensor: {sensor}')\n",
    "        print(f'Sum of squares (Frequency domain): {freq_domain_sum_sq}')\n",
    "        print(f'Sum of squares (Time domain): {time_domain_sum_sq}')\n",
    "        print(f'Parseval\\'s theorem holds: {np.isclose(freq_domain_sum_sq, time_domain_sum_sq)}\\n')\n",
    "    \n",
    "    return pd.DataFrame(time_domain_data), freq_domain_squares_sum, time_domain_squares_sum\n",
    "\n",
    "# Assuming df_freq is your dataframe with frequency domain EMG data\n",
    "# df_time, freq_squares, time_squares = parsevals_theorem_verification(df_freq)\n",
    "\n",
    "\n",
    "#want to see that freq_domain_sum_sq is the same as time_domain_sum_sq\n",
    "#can try and generalize by gesture trial as opposed to by column\n",
    "#load in metadata and break up huge dataframe by each gesture chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsevals_theorem_verification(EMG_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOT USING\n",
    "#RAN INTO ISSUE WITH TAKING SECOND LOG OF NEG VALUES SINCE LOG(ZERO/DOT) GAVE NEG VALUES\n",
    "# either follow code in cell below to take maximum or maybe do abs value of log(zero/dot)?? these are both diff from original formula though\n",
    "def third_order_moment_log(df_freq, second_order_moments_raw, first_order_moments_raw, zero_order_moments_raw):\n",
    "    # Step 1: Compute the square roots\n",
    "    sqrt_first_diff = np.sqrt(zero_order_moments_raw - first_order_moments_raw)\n",
    "    sqrt_second_diff = np.sqrt(zero_order_moments_raw - second_order_moments_raw)\n",
    "    #print(sqrt_first_diff)\n",
    "    #print(sqrt_second_diff)\n",
    "    \n",
    "    # Step 2: Perform the dot product\n",
    "    dot_product = np.dot(sqrt_first_diff, sqrt_second_diff)\n",
    "    #print(dot_product)\n",
    "    # Step 3: Compute the sparseness formula\n",
    "    sparseness = np.log(np.abs(zero_order_moments_raw / dot_product))\n",
    "    print(zero_order_moments_raw)\n",
    "    print(dot_product)\n",
    "    \n",
    "    print(sparseness)\n",
    "    \n",
    "    # Step 4: Apply the logarithm again for third-order moments\n",
    "    third_order_moments_log = np.log(sparseness)\n",
    "    \n",
    "    return third_order_moments_log\n",
    "third_order_moments_log = third_order_moment_log(EMG_sliced, second_order_moments_raw, first_order_moments_raw, zero_order_moments_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc43cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOT USING\n",
    "def freq_second_order_moment_log(df_freq, zero_order_moments_raw):\n",
    "    second_order_moments_log = []\n",
    "    second_order_moments_raw = []\n",
    "\n",
    "    for i, sensor in enumerate(df_freq.columns):\n",
    "        # Extract the frequency domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        freq_data = np.fft.fft(time_data)\n",
    "        \n",
    "        # Step 1: Take first derivative\n",
    "        first_deriv = np.gradient(freq_data)\n",
    "        \n",
    "        #Step 2: take second derivative\n",
    "        second_deriv = np.gradient(first_deriv)\n",
    "        \n",
    "        # Step 3: Square the signal (power) at each frequency of the first deriv\n",
    "        signal_squared = np.abs(second_deriv) ** 2\n",
    "        \n",
    "        # Step 4: Integrate (sum) the squared values to get the total power\n",
    "        total_power = np.sum(signal_squared)\n",
    "        \n",
    "        # Step 4: Take the logarithm of the total power\n",
    "        log_total_power = np.log(total_power/(zero_order_moments_raw[i]**4))\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        second_order_moments_log.append(log_total_power)\n",
    "        second_order_moments_raw.append(total_power)\n",
    "\n",
    "    \n",
    "    return second_order_moments_log, second_order_moments_raw\n",
    "\n",
    "freq_second_order_moments_log, second_order_moments_raw = freq_first_order_moment_log(EMG_sliced, zero_order_moments_raw)\n",
    "#output = zero_order_moments_log is the zeroeth feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOT USING\n",
    "def freq_first_order_moment_log(df_freq, freq_zero_order_moments_raw):\n",
    "    # Initialize lists to store the results for each sensor\n",
    "    first_order_moments_log = []\n",
    "    first_order_moments_raw = []\n",
    "    \n",
    "    for i,sensor in enumerate(df_freq.columns):\n",
    "        # Extract the frequency domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        freq_data = np.fft.fft(time_data)\n",
    "        \n",
    "        # Step 1: Take first derivative\n",
    "        first_deriv = np.gradient(freq_data)\n",
    "        \n",
    "        # Step 2: Square the signal (power) at each frequency of the first deriv\n",
    "        signal_squared = np.abs(first_deriv) ** 2\n",
    "        \n",
    "        # Step 3: Integrate (sum) the squared values to get the total power\n",
    "        total_power = np.sum(signal_squared)\n",
    "        \n",
    "        # Step 4: Take the logarithm of the total power\n",
    "        log_total_power = np.log(total_power / (zero_order_moments_raw[i]**2))\n",
    "        \n",
    "        # Store the results in the lists\n",
    "        first_order_moments_log.append(log_total_power)\n",
    "        first_order_moments_raw.append(total_power)\n",
    "\n",
    "    # Convert lists to numpy arrays for consistency\n",
    "    first_order_moments_log = np.array(first_order_moments_log)\n",
    "    first_order_moments_raw = np.array(first_order_moments_raw)\n",
    "    \n",
    "    return first_order_moments_log, first_order_moments_raw\n",
    "\n",
    "# Example usage:\n",
    "freq_first_order_moments_log, freq_first_order_moments_raw = freq_first_order_moment_log(EMG_sliced, freq_zero_order_moments_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOT USING\n",
    "def freq_zero_order_moment_log(df_freq):\n",
    "    zero_order_moments_log = []\n",
    "    zero_order_moments_raw = []\n",
    "\n",
    "    for sensor in df_freq.columns:\n",
    "        # Extract the frequency domain data for the sensor (column of the dataframe)\n",
    "        time_data = df_freq[sensor].values\n",
    "        freq_data = np.fft.fft(time_data)\n",
    "\n",
    "        \n",
    "        # Step 1: Square the signal (power) at each frequency\n",
    "        signal_squared = np.abs(freq_data) ** 2\n",
    "        \n",
    "        # Step 2: Integrate (sum) the squared values to get the total power\n",
    "        total_power = np.sum(signal_squared)\n",
    "        \n",
    "        # Step 3: Take the logarithm of the total power\n",
    "        log_total_power = np.log(total_power)\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        zero_order_moments_log.append(log_total_power)\n",
    "        zero_order_moments_raw.append(total_power)\n",
    "        \n",
    "        #convert to np.array for consistency\n",
    "    \n",
    "    return zero_order_moments_log, zero_order_moments_raw\n",
    "\n",
    "freq_zero_order_moments_log, freq_zero_order_moments_raw = freq_zero_order_moment_log(EMG_sliced)\n",
    "#output = zero_order_moments_log is the zeroeth feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a920672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do fft: need to slice out EMG data and only use that\n",
    "EMG_sliced = EMG_stand.iloc[:,3:]\n",
    "EMG_sliced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
