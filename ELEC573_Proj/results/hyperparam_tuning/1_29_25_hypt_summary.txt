ACCURACIES 55.98%, 54.5%, 51.4%

bs: 16 | 16 | 32, 
conv_layers: 32,5,1; 64,3,1; 128,2,1 | 32,5,1; 64,3,1; 128,2,1 | 32,5,1; 64,3,1; 128,2,1
do: 0.3 | 0 | 0.3 --> Wait if dense layer isn't used, does this do anything? Is this applied to LSTM... or dense layer, or output dense? Do I even have output dense? --> Yah do wasn't even implemented for the dense layers
fc: 128,64 | 128,64 | 64,32
ft_bs: 10 | 5 | 1
ft_lr: 0.01 | 0.001 | 0.01
ft_wd: 0.0001 | 0 | 0
lr: 0.001 | 0.001 | 0.001
lstm_do: 0.8 | 0.8 | 0
lstm_hs: 16 | 24 | 8
lstm_layers: 1 | 1 | 1
opt: adam  adam | sgd
pooling: 4F | T3F | 4F
use_dense_cnn_lstm: true | false | false
wd: 0 | 0.0001 | 0.0001

BEST
bs: 16?
conv_layers: 32,5,1; 64,3,1; 128,2,1
do: 0.3? [double check how/where this is applied] --> Wasnt on...
fc: 128, 64?
ft_bs: no consensus: 10?
ft_lr: 0.01?
ft_wd: 0? Best had 0.0001...
lr: 0.001 --> I think this was the lowest I tested, might be worth testing lower ones...
lstm_do: 0.8? --> Was 0.3/0.5 in the running?
lstm_hs: no consensus. Best had 16
lstm_layers: 1
opt: adam?
pooling: 4F? --> It's not just memorizing is it...
use_dense_between_cnn_lstm: true? 2 falses but best had true...
wd: 0? 2 0.0001 but best had 0...
