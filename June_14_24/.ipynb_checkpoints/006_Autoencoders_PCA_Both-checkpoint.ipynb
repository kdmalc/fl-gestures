{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ef1e63-343c-4d55-ad97-cf3cabb3fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ae_torch_classes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347da32c-36cc-49e0-80cd-e5b456561a30",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533b39ce-387c-4e40-b5bd-58dd97d233f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pID 101 because it doesn't exist\n",
    "# remove pID 131 because it  doesnt have enough user defined gestures\n",
    "# each participant has 100 experimenter defined files and 50 user defined files\n",
    "# 10 experimenter defined gestures and 5 user defined gestures\n",
    "\n",
    "file_types = [\"IMU_extract\", \"movavg_files\"]\n",
    "expt_types = [\"experimenter-defined\"]\n",
    "\n",
    "#remove participant 131 because they are missing gestures \n",
    "pIDs_impaired = ['P102','P103','P104','P105','P106','P107','P108','P109','P110','P111',\n",
    "       'P112','P114','P115','P116','P118','P119','P121','P122','P123','P124','P125',\n",
    "       'P126','P127','P128', 'P132']\n",
    "# remove participants P001 and P003 because they dont have duplicate or open gestures\n",
    "pIDs_unimpaired = ['P004','P005','P006','P008','P010','P011']\n",
    "\n",
    "pIDs_both = pIDs_impaired + pIDs_unimpaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647a91a5-4d27-485f-8ed0-db92218ccb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n"
     ]
    }
   ],
   "source": [
    "# Kai's laptop\n",
    "#data_path = \"C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\PCA_40D\\\\\"\n",
    "#metadata_cols_df = pd.read_pickle('C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Data\\\\$M\\\\metadata_cols_df.pkl')\n",
    "# BRC Desktop\n",
    "data_path = \"D:\\\\Kai_MetaGestureClustering_24\\\\saved_datasets\\\\PCA_40D\\\\\"\n",
    "metadata_cols_df = pd.read_pickle('D:\\\\Kai_MetaGestureClustering_24\\\\saved_datasets\\\\metadata_cols_df.pkl')\n",
    "\n",
    "print(\"Loading\")\n",
    "\n",
    "metadata_cols = ['Participant', 'Gesture_ID', 'Gesture_Num']\n",
    "\n",
    "PCA_df = pd.read_pickle(data_path+'PCA_ms_IMUEMG_df.pkl')\n",
    "\n",
    "# Dropping the metadata when we read it in!\n",
    "training_u_df = pd.read_pickle(data_path+'training_u_df.pkl').drop(metadata_cols, axis=1)\n",
    "test_users_df = pd.read_pickle(data_path+'test_users_df.pkl').drop(metadata_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0430ab-dd10-48ee-b823-b5a91d36b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327168, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027903</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>-0.019699</td>\n",
       "      <td>0.027333</td>\n",
       "      <td>-0.031254</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>0.066484</td>\n",
       "      <td>0.108729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019453</td>\n",
       "      <td>0.062983</td>\n",
       "      <td>-0.025869</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.037645</td>\n",
       "      <td>-0.186270</td>\n",
       "      <td>-0.046251</td>\n",
       "      <td>-0.104630</td>\n",
       "      <td>-0.002939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038982</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>0.031336</td>\n",
       "      <td>-0.007901</td>\n",
       "      <td>-0.027368</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>0.035053</td>\n",
       "      <td>-0.056843</td>\n",
       "      <td>-0.008895</td>\n",
       "      <td>-0.022542</td>\n",
       "      <td>-0.022563</td>\n",
       "      <td>-0.160826</td>\n",
       "      <td>-0.048161</td>\n",
       "      <td>-0.073771</td>\n",
       "      <td>0.043268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116782</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>-0.014612</td>\n",
       "      <td>-0.093325</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>-0.013155</td>\n",
       "      <td>-0.046150</td>\n",
       "      <td>0.036385</td>\n",
       "      <td>0.052746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014298</td>\n",
       "      <td>0.072109</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.018695</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>-0.041831</td>\n",
       "      <td>-0.109653</td>\n",
       "      <td>0.027043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.030245</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.022540</td>\n",
       "      <td>-0.048905</td>\n",
       "      <td>-0.029129</td>\n",
       "      <td>0.090026</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>-0.064307</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>0.053055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010992</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>-0.097073</td>\n",
       "      <td>-0.056870</td>\n",
       "      <td>-0.001038</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>-0.165858</td>\n",
       "      <td>-0.049424</td>\n",
       "      <td>-0.108671</td>\n",
       "      <td>0.069886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.112950</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>-0.063254</td>\n",
       "      <td>-0.108892</td>\n",
       "      <td>0.198729</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>-0.124893</td>\n",
       "      <td>0.114817</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>-0.093678</td>\n",
       "      <td>-0.131263</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.157963</td>\n",
       "      <td>-0.041911</td>\n",
       "      <td>-0.145308</td>\n",
       "      <td>0.063311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \n",
       "0 -0.027903  0.001411 -0.019509  0.013428 -0.019699  0.027333 -0.031254  \\\n",
       "1 -0.038982  0.006470 -0.000111  0.010904 -0.015323  0.031336 -0.007901   \n",
       "2 -0.116782  0.003824  0.011550 -0.014612 -0.093325  0.081718 -0.013155   \n",
       "3 -0.030245 -0.017409  0.022540 -0.048905 -0.029129  0.090026 -0.024645   \n",
       "4 -0.112950  0.026262  0.004837 -0.063254 -0.108892  0.198729 -0.010583   \n",
       "\n",
       "         7         8         9   ...        30        31        32        33   \n",
       "0 -0.022910  0.066484  0.108729  ... -0.019453  0.062983 -0.025869  0.014303  \\\n",
       "1 -0.027368  0.060370  0.074712  ...  0.041438  0.035053 -0.056843 -0.008895   \n",
       "2 -0.046150  0.036385  0.052746  ... -0.014298  0.072109 -0.026536 -0.034365   \n",
       "3 -0.064307  0.074589  0.053055  ... -0.010992  0.059990 -0.097073 -0.056870   \n",
       "4 -0.124893  0.114817  0.038628  ...  0.035735  0.050880 -0.093678 -0.131263   \n",
       "\n",
       "         34        35        36        37        38        39  \n",
       "0 -0.013387 -0.037645 -0.186270 -0.046251 -0.104630 -0.002939  \n",
       "1 -0.022542 -0.022563 -0.160826 -0.048161 -0.073771  0.043268  \n",
       "2  0.018695 -0.011940 -0.160580 -0.041831 -0.109653  0.027043  \n",
       "3 -0.001038 -0.008015 -0.165858 -0.049424 -0.108671  0.069886  \n",
       "4  0.018035  0.056185 -0.157963 -0.041911 -0.145308  0.063311  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_u_df.shape)\n",
    "training_u_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc04167-5ab8-4ac3-ae19-80b8e1988242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE TRAINING SET\n",
    "num_rows_per_gesture = 64 # From the interp\n",
    "num_gestures = len(training_u_df) // num_rows_per_gesture\n",
    "num_features = training_u_df.shape[1]\n",
    "\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(training_u_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns)\n",
    "X_3D_PCA40 = training_u_df.to_numpy().reshape(num_gestures, num_rows_per_gesture, num_features)\n",
    "#flattened_PCA = PCA_np.reshape(num_gestures, -1)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_3DTensor_PCA40 = torch.tensor(X_3D_PCA40, dtype=torch.float32)\n",
    "\n",
    "# Dummy dataset\n",
    "#data = torch.randn(num_gestures, timesteps, num_features)\n",
    "#dataset = TensorDataset(data)\n",
    "#data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the dataset\n",
    "u_training_dataset = GestureDatasetAE(X_3DTensor_PCA40)\n",
    "\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "train_loader = DataLoader(u_training_dataset, batch_size=batch_size, shuffle=True) # Should shuffle be False? \n",
    "## It's shuffling the gesture order I think so that should be fine...\n",
    "\n",
    "# CREATE THE TEST SET\n",
    "num_test_gestures = len(test_users_df) // num_rows_per_gesture\n",
    "# Ensure the data can be evenly divided into gestures\n",
    "assert len(test_users_df) % num_rows_per_gesture == 0, \"The total number of rows is not a multiple of the number of rows per gesture.\"\n",
    "\n",
    "# Reshape into (batch_dim, time_step, n_features) AKA (n_gestures, n_rows_per_gesture, n_columns) and convert to torch tensor\n",
    "## Theres probably an easier way to just create it as a torch tensor lol\n",
    "Xtest_3DTensor_PCA40 = torch.tensor(test_users_df.to_numpy().reshape(num_test_gestures, num_rows_per_gesture, num_features), dtype=torch.float32)\n",
    "\n",
    "# Create the dataset\n",
    "u_testing_dataset = GestureDatasetAE(Xtest_3DTensor_PCA40)\n",
    "test_loader = DataLoader(u_testing_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9b852-4054-443f-a4e9-bdb280c5cc84",
   "metadata": {},
   "source": [
    "# RNN Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb18329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dim = 40\n",
    "#num_features = 40\n",
    "seq_len = 64\n",
    "#timesteps = 64\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a200a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Hyperparameters and dataset setup\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "hidden_dim = 128\n",
    "\n",
    "# With progressive halving\n",
    "model = RNNAutoencoder(input_dim, hidden_dim, num_layers, seq_len, progressive=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0de53807-9950-47b1-a331-3e7ff7a5d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Epoch [1/10], Train Loss: 0.0654, Test Loss: 0.0227\n",
      "Epoch [2/10], Train Loss: 0.0208, Test Loss: 0.0140\n",
      "Epoch [3/10], Train Loss: 0.0131, Test Loss: 0.0098\n",
      "Epoch [4/10], Train Loss: 0.0094, Test Loss: 0.0076\n",
      "Epoch [5/10], Train Loss: 0.0070, Test Loss: 0.0059\n",
      "Epoch [6/10], Train Loss: 0.0051, Test Loss: 0.0044\n",
      "Epoch [7/10], Train Loss: 0.0040, Test Loss: 0.0036\n",
      "Epoch [8/10], Train Loss: 0.0032, Test Loss: 0.0030\n",
      "Epoch [9/10], Train Loss: 0.0026, Test Loss: 0.0024\n",
      "Epoch [10/10], Train Loss: 0.0022, Test Loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in train_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        #print(batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #batch = batch[0]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_epoch_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023c0bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Hyperparameters and dataset setu\n",
    "num_layers = None\n",
    "lr = 0.001\n",
    "hidden_dim_list = [128, 64, 32, 4]\n",
    "\n",
    "# With specific hidden dimensions\n",
    "model = RNNAutoencoder(input_dim, hidden_dim_list, num_layers, seq_len, mirror=True, progressive=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09bf065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Epoch [1/10], Train Loss: 0.1678, Test Loss: 0.1128\n",
      "Epoch [2/10], Train Loss: 0.1271, Test Loss: 0.0997\n",
      "Epoch [3/10], Train Loss: 0.1110, Test Loss: 0.0852\n",
      "Epoch [4/10], Train Loss: 0.0992, Test Loss: 0.0819\n",
      "Epoch [5/10], Train Loss: 0.0958, Test Loss: 0.0806\n",
      "Epoch [6/10], Train Loss: 0.0914, Test Loss: 0.0796\n",
      "Epoch [7/10], Train Loss: 0.0881, Test Loss: 0.0783\n",
      "Epoch [8/10], Train Loss: 0.0856, Test Loss: 0.0771\n",
      "Epoch [9/10], Train Loss: 0.0831, Test Loss: 0.0756\n",
      "Epoch [10/10], Train Loss: 0.0815, Test Loss: 0.0745\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in train_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #batch = batch[0]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_epoch_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb888647",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d599e5-f764-4636-b190-b11c196ebd5a",
   "metadata": {},
   "source": [
    "## Old Grid Search\n",
    "> Trying a ton of different architectures to find what's best for this dataset!\n",
    "- Progressive should only be True, only False if nodes are being entered manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f018b6-25b1-47c3-a99c-fedab6561717",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40  # Number of features\n",
    "seq_len = 64  # Number of timesteps\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_dim': [32, 64, 128],\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'progressive': [True]  # Flag to halve the hidden units for each successive layer\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "best_model, best_params = grid_search_rnn_autoencoder(train_loader, test_loader, input_dim, seq_len, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bc7df-43fe-4f75-bb71-3d9a516e5f78",
   "metadata": {},
   "source": [
    "- Trial: 0;: {'hidden_dim': 32, 'num_layers': 2, 'progressive': True}, Validation Loss: 0.02083\n",
    "- 4- \r\n",
    "Trial: 1s: {'hidden_dim': 32, 'num_layers': 2, 'progressive': False}, Validation Loss: 0.016\n",
    "- 3- 35\r\n",
    "Trial:ams: {'hidden_dim': 32, 'num_layers': 3, 'progressive': True}, Validation Loss: 0.0\n",
    "- 7- 5635\r\n",
    "Triaarams: {'hidden_dim': 32, 'num_layers': 3, 'progressive': False}, Validation Loss: 0\n",
    "- 5- 940434\r\n",
    "Tr\n",
    "Params: {'hidden_dim': 32, 'num_layers': 4, 'progressive': True}, Validation Loss:\n",
    "- 7- 31617157\r",
    " e\r\n",
    "Params: {'hidden_dim': 32, 'num_layers': 4, 'progressive': False}, Validation Los\n",
    "- 0- 6267585271rue\r\n",
    "Params: {'hidden_dim': 64, 'num_layers': 2, 'progressive': True}, Validation Lo\n",
    "- 8- 4156883282False\r\n",
    "Params: {'hidden_dim': 64, 'num_layers': 2, 'progressive': False}, Validation \n",
    "- 5- 4547723424e: True\r\n",
    "Params: {'hidden_dim': 64, 'num_layers': 3, 'progressive': True}, Validati\n",
    "- 2- 2054176014ve: False\r\n",
    "Params: {'hidden_dim': 64, 'num_layers': 3, 'progressive': False}, Valida\n",
    "- .- 01182716042ssive: True\r\n",
    "Params: {'hidden_dim': 64, 'num_layers': 4, 'progressive': True}, Vali\n",
    "- :-  0.04446715essive: False\r\n",
    "Params: {'hidden_dim': 64, 'num_layers': 4, 'progressive': False}, Va\n",
    "- s- : 0.0171945ogressive: True\r\n",
    "Params: {'hidden_dim': 128, 'num_layers': 2, 'progressive': True}, \n",
    "- s- s: 0.002466rogressive: False\r\n",
    "Params: {'hidden_dim': 128, 'num_layers': 2, 'progressive': False}\n",
    "- L- oss: 0.0017; progressive: True\r\n",
    "Params: {'hidden_dim': 128, 'num_layers': 3, 'progressive': Tru\n",
    "- o- n Loss: 0.03; progressive: False\r\n",
    "Params: {'hidden_dim': 128, 'num_layers': 3, 'progressive': Fa\n",
    "- t- ion Loss: 0s: 4; progressive: True\r\n",
    "Params: {'hidden_dim': 128, 'num_layers': 4, 'progressive'5\n",
    "- d- ation Loss:rs: 4; progressive: False\r\n",
    "Params: {'hidden_dim': 128, 'num_layers': 4, 'progressive\n",
    "- l- idation Loss: 0.010927938756399922\r\n",
    "Best Params: {'hidden_dim': 128, 'num_layers': 2, 'progressive': Fallidation Loss: 0.0017131721086079037\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0d17c-17d2-4ff5-80f5-cdc69ee49a48",
   "metadata": {},
   "source": [
    "__Best model from above:__ <br>\n",
    "Trial: 13; hidden_dim: 128; num_layers: 2; progressive: False <br>\n",
    "Params: {'hidden_dim': 128, 'num_layers': 2, 'progressive': False}, Validation Loss: 0.0017131721086079037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7053f-db18-4762-8413-4a04afc4328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'C:\\\\Users\\\\YamagamiLab\\\\Desktop\\\\Dev\\\\fl-gestures\\\\models\\\\RNNAE_latent128_progFalse_numlayers2_trial13_vallossp001_BothPCA40.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd6cb8-470b-41c6-9912-2d3feba26bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c4759-9e9b-43ca-8486-8262c4592a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0979debf-2edc-4c90-b449-2219ec3e61b1",
   "metadata": {},
   "source": [
    "Original Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fa199-a3f4-4b72-9484-3481f9bf79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Hyperparameters and dataset setup\n",
    "timesteps = 64\n",
    "num_features = 40\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = RNNAutoencoder(num_features, hidden_dim, num_layers, seq_len=timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83f2b9-7412-4213-99ec-85971a591a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started\")\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in train_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #batch = batch[0]\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_epoch_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcb47c-ed50-4031-8d73-87c2a9cb4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so I don't have to retrain later\n",
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Repos\\\\fl-gestures\\\\models\\\\RNNAE_Default_BothPCA40.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a04a6c-3aee-4228-ad38-0e1969ceafe3",
   "metadata": {},
   "source": [
    "Trying lower hidden dims..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c255bdc-ea0e-4bd5-9bd2-ac10f544fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and dataset setup\n",
    "timesteps = 64\n",
    "num_features = 40\n",
    "hidden_dim = 3\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "vanilla_RNN_AE_latent3 = RNNAutoencoder(num_features, hidden_dim, num_layers, seq_len=timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(vanilla_RNN_AE_latent3.parameters(), lr=lr)\n",
    "\n",
    "print(\"Started\")\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in train_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        optimizer.zero_grad()\n",
    "        output = vanilla_RNN_AE_latent3(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #batch = batch[0]\n",
    "            output = vanilla_RNN_AE_latent3(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_epoch_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec825dc4-69af-4c5f-b352-04c0cf7fc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so I don't have to retrain later\n",
    "torch.save(vanilla_RNN_AE_latent3.state_dict(), 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Repos\\\\fl-gestures\\\\models\\\\RNNAE_latent3_BothPCA40.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598251d-5139-49af-bf5b-f626aabe466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and dataset setup\n",
    "timesteps = 64\n",
    "num_features = 40\n",
    "hidden_dim = 12\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "vanilla_RNN_AE_latent12 = RNNAutoencoder(num_features, hidden_dim, num_layers, seq_len=timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(vanilla_RNN_AE_latent12.parameters(), lr=lr)\n",
    "\n",
    "print(\"Started\")\n",
    "\n",
    "# Training\n",
    "average_train_loss = []  # To store average loss (eg across all batches) per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # To store loss for each batch within an epoch\n",
    "    for batch in train_loader:\n",
    "        #print(type(batch))\n",
    "        #print(len(batch))\n",
    "        #batch = batch[0]  # batch is a list for some reason idk\n",
    "        optimizer.zero_grad()\n",
    "        output = vanilla_RNN_AE_latent12(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate and log the average loss for the epoch\n",
    "    average_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average_train_loss.append(average_epoch_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #batch = batch[0]\n",
    "            output = vanilla_RNN_AE_latent12(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            test_losses.append(loss.item())\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_epoch_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c15dac-d41c-4c5b-bd42-462c3d7c5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so I don't have to retrain later\n",
    "torch.save(vanilla_RNN_AE_latent12.state_dict(), 'C:\\\\Users\\\\kdmen\\\\Desktop\\\\Research\\\\Repos\\\\fl-gestures\\\\models\\\\RNNAE_latent12_BothPCA40.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b20a2-eb0a-492c-883f-e2603f295521",
   "metadata": {},
   "source": [
    "# Temporal Convolution Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85022701-bada-40a8-99d2-21938d17978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is different but not sure what the effect is...\n",
    "## Observed a higher starting loss, only sort of converged, loss only somewhat decreased\n",
    "#class TCNEncoder(nn.Module):\n",
    "#    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "#        super(TCNEncoder, self).__init__()\n",
    "#        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2, stride=2)  # Adjusted stride\n",
    "#        \n",
    "#    def forward(self, x):\n",
    "#        x = x.permute(0, 2, 1)\n",
    "#        x = torch.relu(self.conv1(x))\n",
    "#        return x\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, kernel_size):\n",
    "        super(TCNDecoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size, stride=2, padding=kernel_size//2, output_padding=1)  # Adjusted to maintain dimensions\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, output_dim, kernel_size=1)  # Convolution to adjust channels if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))  # Apply activation after the final convolution\n",
    "        x = x.permute(0, 2, 1)  # Permute dimensions to match the desired output size\n",
    "        return x\n",
    "\n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
    "        super(TCNAutoencoder, self).__init__()\n",
    "        self.encoder = TCNEncoder(input_dim, hidden_dim, kernel_size)\n",
    "        self.decoder = TCNDecoder(hidden_dim, input_dim, kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09094d09-9f33-4f45-b28e-31795d511dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 40\n",
    "hidden_dim = 64\n",
    "kernel_size = 5\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "temp_conv_ae_model = TCNAutoencoder(input_dim, hidden_dim, kernel_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(temp_conv_ae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = temp_conv_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbb1d9-29ff-4661-b669-9ff300336246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc824e7-03e0-4eae-b880-6be6118a56fb",
   "metadata": {},
   "source": [
    "# Varitational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b356ebb-4b72-46d1-a1a0-8fd21c163c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, use_xavier_init=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim*2)  # mean and logvar\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        if use_xavier_init:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138cb2d-7d58-4f83-8dcd-0074f0544c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting\")\n",
    "\n",
    "# Hyperparameters\n",
    "#input_dim = data.size(1) * data.size(2)\n",
    "input_dim = 64 * 40\n",
    "hidden_dim = 128\n",
    "latent_dim = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "vae_model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(batch)\n",
    "        loss = vae_model.loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68375495-db2c-4b19-83af-8d69eaa5b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae_model.state_dict(), 'C:\\\\Users\\\\YamagamiLab\\\\Desktop\\\\Dev\\\\fl-gestures\\\\models\\\\BrokenVAE_hidden128_latent32_NegativeLossesLol.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e51c3f-89f8-41de-a46d-51eb348e3e42",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d43eff-c569-4a7c-b3cf-ef428d97196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE LAYER SparseAE\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.relu(self.encoder(x))\n",
    "        decoded = self.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579078d4-5d9c-45ec-b412-ad9997fba508",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 12\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "sparse_ae_model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "optimizer = optim.Adam(sparse_ae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = sparse_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa228928-b580-453e-b570-c52ebfb10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sparse_ae_model.state_dict(), 'C:\\\\Users\\\\YamagamiLab\\\\Desktop\\\\Dev\\\\fl-gestures\\\\models\\\\SparseAE_latent12_onelayer_conv_trainlossp08.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556637d9-d576-416d-bfe2-ef4a27dae1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N LAYER SparseAE\n",
    "## NOT WORKING YET\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        if type(hidden_dims) is int:\n",
    "            hidden_dims = list(hidden_dims)\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            encoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        for i in range(len(hidden_dims) - 1, 0, -1):\n",
    "            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i-1]))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        output = self.output_layer(decoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8137596-45af-4972-b837-65dfb791fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim_lst = [3]\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "sparse_ae_model = SparseAutoencoder(input_dim, hidden_dim_lst)\n",
    "optimizer = optim.Adam(sparse_ae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = sparse_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780becf-e1c6-496a-95e7-1f9f1fbca123",
   "metadata": {},
   "source": [
    "# Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4717f-35d1-4b24-8df8-e869e0f1a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE LAYER DenoisingAE\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.relu(self.encoder(x))\n",
    "        decoded = self.sigmoid(self.decoder(encoded))\n",
    "        return decoded\n",
    "\n",
    "    def add_noise(self, x, noise_level=0.1):\n",
    "        # Add Gaussian noise to the input\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8b68c-2401-4b99-8c23-5dea55abad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 12\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "denoising_ae_model = DenoisingAutoencoder(input_dim, hidden_dim)\n",
    "optimizer = optim.Adam(denoising_ae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = denoising_ae_model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd45f2-6881-490a-803f-eae28d4573e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(denoising_ae_model.state_dict(), 'C:\\\\Users\\\\YamagamiLab\\\\Desktop\\\\Dev\\\\fl-gestures\\\\models\\\\denoisingAE_flatconv_onelayer_latent12.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ba039-02a7-40bd-98dc-6e6e78b26368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
